"""
çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ - FastAPI åç«¯æœåŠ¡
æä¾›RESTful APIæ¥å£æ¥æ“ä½œç°æœ‰çš„çˆ¬è™«åŠŸèƒ½
"""

import os
import sys
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
from contextlib import asynccontextmanager
import json
import requests

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response
from pydantic import BaseModel, Field
import uvicorn
import mimetypes
import random
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼ˆç°åœ¨main.pyå°±åœ¨æ ¹ç›®å½•ï¼‰
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

# å¯¼å…¥ç°æœ‰çš„ä¸šåŠ¡é€»è¾‘æ¨¡å—
from zsxq_interactive_crawler import ZSXQInteractiveCrawler, load_config
from zsxq_database import ZSXQDatabase
from zsxq_file_database import ZSXQFileDatabase
from db_path_manager import get_db_path_manager
from image_cache_manager import get_image_cache_manager
# ä½¿ç”¨SQLè´¦å·ç®¡ç†å™¨
from accounts_sql_manager import get_accounts_sql_manager
from account_info_db import get_account_info_db
from zsxq_columns_database import ZSXQColumnsDatabase
from logger_config import log_info, log_warning, log_error, log_exception, log_debug, ensure_configured
from wecom_webhook import WeComWebhook  # âœ… æ–°å¢ï¼šå¯¼å…¥ä¼ä¸šå¾®ä¿¡Webhookç±»

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
ensure_configured()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šå¯åŠ¨æ—¶æ‰«ææœ¬åœ°ç¾¤"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    try:
        await asyncio.to_thread(scan_local_groups)
    except Exception as e:
        print(f"âš ï¸ å¯åŠ¨æ‰«ææœ¬åœ°ç¾¤å¤±è´¥: {e}")
    yield
    # å…³é—­æ—¶æ‰§è¡Œï¼ˆå¦‚éœ€è¦å¯æ·»åŠ æ¸…ç†é€»è¾‘ï¼‰


app = FastAPI(
    title="çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ API",
    description="ä¸ºçŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨æä¾›RESTful APIæ¥å£",
    version="1.0.0",
    lifespan=lifespan
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å‰ç«¯åœ°å€
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡å­˜å‚¨çˆ¬è™«å®ä¾‹å’Œä»»åŠ¡çŠ¶æ€
crawler_instance: Optional[ZSXQInteractiveCrawler] = None
current_tasks: Dict[str, Dict[str, Any]] = {}
task_counter = 0
task_logs: Dict[str, List[str]] = {}  # å­˜å‚¨ä»»åŠ¡æ—¥å¿—
sse_connections: Dict[str, List] = {}  # å­˜å‚¨SSEè¿æ¥
task_stop_flags: Dict[str, bool] = {}  # ä»»åŠ¡åœæ­¢æ ‡å¿—
file_downloader_instances: Dict[str, Any] = {}  # å­˜å‚¨æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹

# =========================
# æœ¬åœ°ç¾¤æ‰«æï¼ˆoutput ç›®å½•ï¼‰
# =========================

# å¯é…ç½®ï¼šé»˜è®¤ ./outputï¼›å¯é€šè¿‡ç¯å¢ƒå˜é‡ OUTPUT_DIR è¦†ç›–
LOCAL_OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "output")
# å¤„ç†ä¸Šé™ä¿æŠ¤ï¼Œé»˜è®¤ 10000ï¼›å¯é€šè¿‡ LOCAL_GROUPS_SCAN_LIMIT è¦†ç›–
try:
    LOCAL_SCAN_LIMIT = int(os.environ.get("LOCAL_GROUPS_SCAN_LIMIT", "10000"))
except Exception:
    LOCAL_SCAN_LIMIT = 10000

# æœ¬åœ°ç¾¤ç¼“å­˜
_local_groups_cache = {
    "ids": set(),     # set[int]
    "scanned_at": 0.0 # epoch ç§’
}


def _safe_listdir(path: str):
    """å®‰å…¨åˆ—ç›®å½•ï¼Œå¼‚å¸¸ä¸æŠ›å‡ºï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶å‘Šè­¦"""
    try:
        return os.listdir(path)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–ç›®å½• {path}: {e}")
        return []


def _collect_numeric_dirs(base: str, limit: int) -> set:
    """
    æ‰«æ base çš„ä¸€çº§å­ç›®å½•ï¼Œæ”¶é›†çº¯æ•°å­—ç›®å½•åï¼ˆ^\d+$ï¼‰ä½œä¸ºç¾¤IDã€‚
    å¿½ç•¥ï¼šéç›®å½•ã€è½¯é“¾æ¥ã€éšè—ç›®å½•ï¼ˆä»¥ . å¼€å¤´ï¼‰ã€‚
    """
    ids = set()
    if not base:
        return ids

    base_abs = os.path.abspath(base)
    if not (os.path.exists(base_abs) and os.path.isdir(base_abs)):
        # è§†ä¸ºç©ºé›†åˆï¼Œä¸æŠ¥é”™
        print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨æˆ–ä¸å¯è¯»: {base_abs}ï¼Œè§†ä¸ºç©ºé›†åˆ")
        return ids

    processed = 0
    for name in _safe_listdir(base_abs):
        # éšè—ç›®å½•
        if not name or name.startswith('.'):
            continue

        path = os.path.join(base_abs, name)
        try:
            # è½¯é“¾æ¥/éç›®å½•å¿½ç•¥
            if os.path.islink(path) or not os.path.isdir(path):
                continue

            # ä»…çº¯æ•°å­—ç›®å½•å
            if name.isdigit():
                ids.add(int(name))
                processed += 1
                if processed >= limit:
                    print(f"âš ï¸ å­ç›®å½•æ•°é‡è¶…è¿‡ä¸Šé™ {limit}ï¼Œå·²æˆªæ–­")
                    break
        except Exception:
            # å•é¡¹å¤±è´¥å®‰å…¨é™çº§
            continue

    return ids


def scan_local_groups(output_dir: str = None, limit: int = None) -> set:
    """
    æ‰«ææœ¬åœ° output çš„ä¸€çº§å­ç›®å½•ï¼Œè·å–ç¾¤IDé›†åˆã€‚
    åŒæ—¶å…¼å®¹ output/databases ç»“æ„ï¼ˆå¦‚å­˜åœ¨ï¼‰ã€‚
    åŒæ­¥æ‰§è¡Œï¼ˆç”¨äºæ‰‹åŠ¨åˆ·æ–°æˆ–å¼ºåˆ¶åˆ·æ–°ï¼‰ï¼Œå¼‚å¸¸å®‰å…¨é™çº§ã€‚
    """
    try:
        odir = output_dir or LOCAL_OUTPUT_DIR
        lim = int(limit or LOCAL_SCAN_LIMIT)

        # ä¸»è·¯å¾„ï¼šä»…æ‰«æ output çš„ä¸€çº§å­ç›®å½•
        ids_primary = _collect_numeric_dirs(odir, lim)

        # å…¼å®¹è·¯å¾„ï¼šoutput/databases çš„ä¸€çº§å­ç›®å½•ï¼ˆè‹¥å­˜åœ¨ï¼‰
        ids_secondary = _collect_numeric_dirs(os.path.join(odir, "databases"), lim)

        ids = set(ids_primary) | set(ids_secondary)

        # æ›´æ–°ç¼“å­˜
        _local_groups_cache["ids"] = ids
        _local_groups_cache["scanned_at"] = time.time()

        return ids
    except Exception as e:
        print(f"âš ï¸ æœ¬åœ°ç¾¤æ‰«æå¼‚å¸¸: {e}")
        # å®‰å…¨é™çº§ä¸ºæ—§ç¼“å­˜
        return _local_groups_cache.get("ids", set())


def get_cached_local_group_ids(force_refresh: bool = False) -> set:
    """
    è·å–ç¼“å­˜ä¸­çš„æœ¬åœ°ç¾¤IDé›†åˆï¼›å¯é€‰å¼ºåˆ¶åˆ·æ–°ã€‚
    æœªæ‰«æè¿‡æˆ–è¦æ±‚å¼ºæ›´æ—¶è§¦å‘åŒæ­¥æ‰«æã€‚
    """
    if force_refresh or not _local_groups_cache.get("ids"):
        return scan_local_groups()
    return _local_groups_cache.get("ids", set())


# Pydanticæ¨¡å‹å®šä¹‰
class ConfigModel(BaseModel):
    cookie: str = Field(..., description="çŸ¥è¯†æ˜ŸçƒCookie")

class CrawlHistoricalRequest(BaseModel):
    pages: int = Field(default=10, ge=1, le=1000, description="çˆ¬å–é¡µæ•°")
    per_page: int = Field(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
    crawlIntervalMin: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    pagesPerBatch: Optional[int] = Field(default=None, ge=5, le=50, description="æ¯æ‰¹æ¬¡é¡µé¢æ•°")

class CrawlSettingsRequest(BaseModel):
    crawlIntervalMin: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    pagesPerBatch: Optional[int] = Field(default=None, ge=5, le=50, description="æ¯æ‰¹æ¬¡é¡µé¢æ•°")

class FileDownloadRequest(BaseModel):
    max_files: Optional[int] = Field(default=None, description="æœ€å¤§ä¸‹è½½æ–‡ä»¶æ•°")
    sort_by: str = Field(default="download_count", description="æ’åºæ–¹å¼: download_count æˆ– time")
    download_interval: float = Field(default=1.0, ge=0.1, le=300.0, description="å•æ¬¡ä¸‹è½½é—´éš”ï¼ˆç§’ï¼‰")
    long_sleep_interval: float = Field(default=60.0, ge=10.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”ï¼ˆç§’ï¼‰")
    files_per_batch: int = Field(default=10, ge=1, le=100, description="ä¸‹è½½å¤šå°‘æ–‡ä»¶åè§¦å‘é•¿ä¼‘çœ ")
    # éšæœºé—´éš”èŒƒå›´å‚æ•°ï¼ˆå¯é€‰ï¼‰
    download_interval_min: Optional[float] = Field(default=None, ge=1.0, le=300.0, description="éšæœºä¸‹è½½é—´éš”æœ€å°å€¼ï¼ˆç§’ï¼‰")
    download_interval_max: Optional[float] = Field(default=None, ge=1.0, le=300.0, description="éšæœºä¸‹è½½é—´éš”æœ€å¤§å€¼ï¼ˆç§’ï¼‰")
    long_sleep_interval_min: Optional[float] = Field(default=None, ge=10.0, le=3600.0, description="éšæœºé•¿ä¼‘çœ é—´éš”æœ€å°å€¼ï¼ˆç§’ï¼‰")
    long_sleep_interval_max: Optional[float] = Field(default=None, ge=10.0, le=3600.0, description="éšæœºé•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼ï¼ˆç§’ï¼‰")

class ColumnsSettingsRequest(BaseModel):
    """ä¸“æ é‡‡é›†è®¾ç½®è¯·æ±‚"""
    crawlIntervalMin: Optional[float] = Field(default=2.0, ge=1.0, le=60.0, description="é‡‡é›†é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=5.0, ge=1.0, le=60.0, description="é‡‡é›†é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=30.0, ge=10.0, le=600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=60.0, ge=10.0, le=600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    itemsPerBatch: Optional[int] = Field(default=10, ge=3, le=50, description="æ¯æ‰¹æ¬¡å¤„ç†æ•°é‡")
    downloadFiles: Optional[bool] = Field(default=True, description="æ˜¯å¦ä¸‹è½½æ–‡ä»¶")
    downloadVideos: Optional[bool] = Field(default=True, description="æ˜¯å¦ä¸‹è½½è§†é¢‘(éœ€è¦ffmpeg)")
    cacheImages: Optional[bool] = Field(default=True, description="æ˜¯å¦ç¼“å­˜å›¾ç‰‡")
    incrementalMode: Optional[bool] = Field(default=False, description="å¢é‡æ¨¡å¼ï¼šè·³è¿‡å·²å­˜åœ¨çš„æ–‡ç« è¯¦æƒ…")

class AccountCreateRequest(BaseModel):
    cookie: str = Field(..., description="è´¦å·Cookie")
    name: Optional[str] = Field(default=None, description="è´¦å·åç§°")

class AssignGroupAccountRequest(BaseModel):
    account_id: str = Field(..., description="è´¦å·ID")

class GroupInfo(BaseModel):
    group_id: int
    name: str
    type: str
    background_url: Optional[str] = None
    owner: Optional[dict] = None
    statistics: Optional[dict] = None

class TaskResponse(BaseModel):
    task_id: str
    status: str  # pending, running, completed, failed
    message: str
    result: Optional[Dict[str, Any]] = None
    created_at: datetime
    updated_at: datetime

# è¾…åŠ©å‡½æ•°
def get_crawler(log_callback=None) -> ZSXQInteractiveCrawler:
    """è·å–çˆ¬è™«å®ä¾‹"""
    global crawler_instance
    if crawler_instance is None:
        config = load_config()
        if not config:
            raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")

        auth_config = config.get('auth', {})

        cookie = auth_config.get('cookie', '')
        group_id = auth_config.get('group_id', '')

        if cookie == "your_cookie_here" or group_id == "your_group_id_here" or not cookie or not group_id:
            raise HTTPException(status_code=400, detail="è¯·å…ˆåœ¨config.tomlä¸­é…ç½®Cookieå’Œç¾¤ç»„ID")

        # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler_instance = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)

    return crawler_instance

def get_crawler_for_group(group_id: str, log_callback=None) -> ZSXQInteractiveCrawler:
    """ä¸ºæŒ‡å®šç¾¤ç»„è·å–çˆ¬è™«å®ä¾‹"""
    config = load_config()
    if not config:
        raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")

    # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
    cookie = get_cookie_for_group(group_id)

    if not cookie or cookie == "your_cookie_here":
        raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆåœ¨è´¦å·ç®¡ç†æˆ–config.tomlä¸­é…ç½®")

    # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æŒ‡å®šç¾¤ç»„çš„æ•°æ®åº“è·¯å¾„
    path_manager = get_db_path_manager()
    db_path = path_manager.get_topics_db_path(group_id)

    return ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)

def get_crawler_safe() -> Optional[ZSXQInteractiveCrawler]:
    """å®‰å…¨è·å–çˆ¬è™«å®ä¾‹ï¼Œé…ç½®æœªè®¾ç½®æ—¶è¿”å›None"""
    try:
        return get_crawler()
    except HTTPException:
        return None

def get_primary_cookie() -> Optional[str]:
    """
    è·å–å½“å‰ä¼˜å…ˆä½¿ç”¨çš„Cookieï¼š
    1. è‹¥è´¦å·ç®¡ç†ä¸­å­˜åœ¨è´¦å·ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè´¦å·çš„Cookie
    2. å¦åˆ™å›é€€åˆ° config.toml ä¸­çš„ Cookieï¼ˆè‹¥å·²é…ç½®ï¼‰
    """
    # 1. ç¬¬ä¸€ä¸ªè´¦å·
    try:
        sql_mgr = get_accounts_sql_manager()
        first_acc = sql_mgr.get_first_account(mask_cookie=False)
        if first_acc:
            cookie = (first_acc.get("cookie") or "").strip()
            if cookie:
                return cookie
    except Exception:
        pass

    # 2. config.toml ä¸­çš„ Cookie
    try:
        config = load_config()
        if not config:
            return None
        auth_config = config.get("auth", {}) or {}
        cookie = (auth_config.get("cookie") or "").strip()
        if cookie and cookie != "your_cookie_here":
            return cookie
    except Exception:
        return None

    return None


def is_configured() -> bool:
    """æ£€æŸ¥æ˜¯å¦å·²é…ç½®è‡³å°‘ä¸€ä¸ªå¯ç”¨çš„è®¤è¯Cookieï¼ˆè´¦å·ç®¡ç†æˆ–config.toml å‡å¯ï¼‰"""
    return get_primary_cookie() is not None

def create_task(task_type: str, description: str) -> str:
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    global task_counter
    task_counter += 1
    task_id = f"task_{task_counter}_{int(datetime.now().timestamp())}"
    
    current_tasks[task_id] = {
        "task_id": task_id,
        "type": task_type,
        "status": "pending",
        "message": description,
        "result": None,
        "created_at": datetime.now(),
        "updated_at": datetime.now()
    }

    # åˆå§‹åŒ–ä»»åŠ¡æ—¥å¿—å’Œåœæ­¢æ ‡å¿—
    task_logs[task_id] = []
    task_stop_flags[task_id] = False
    add_task_log(task_id, f"ä»»åŠ¡åˆ›å»º: {description}")

    return task_id

def add_task_log(task_id: str, log_message: str):
    """æ·»åŠ ä»»åŠ¡æ—¥å¿—"""
    if task_id not in task_logs:
        task_logs[task_id] = []

    timestamp = datetime.now().strftime("%H:%M:%S")
    formatted_log = f"[{timestamp}] {log_message}"
    task_logs[task_id].append(formatted_log)

    # å¹¿æ’­æ—¥å¿—åˆ°æ‰€æœ‰SSEè¿æ¥
    broadcast_log(task_id, formatted_log)

def broadcast_log(task_id: str, log_message: str):
    """å¹¿æ’­æ—¥å¿—åˆ°SSEè¿æ¥"""
    # è¿™ä¸ªå‡½æ•°ç°åœ¨ä¸»è¦ç”¨äºå­˜å‚¨æ—¥å¿—ï¼Œå®é™…çš„SSEå¹¿æ’­åœ¨streamç«¯ç‚¹ä¸­å®ç°
    pass

def build_stealth_headers(cookie: str) -> Dict[str, str]:
    """æ„é€ æ›´æ¥è¿‘å®˜ç½‘çš„è¯·æ±‚å¤´ï¼Œæå‡æˆåŠŸç‡"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    ]
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7",
        "Cache-Control": "no-cache",
        "Cookie": cookie,
        "Origin": "https://wx.zsxq.com",
        "Pragma": "no-cache",
        "Priority": "u=1, i",
        "Referer": "https://wx.zsxq.com/",
        "Sec-Ch-Ua": "\"Google Chrome\";v=\"137\", \"Chromium\";v=\"137\", \"Not/A)Brand\";v=\"24\"",
        "Sec-Ch-Ua-Mobile": "?0",
        "Sec-Ch-Ua-Platform": "\"Windows\"",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-site",
        "User-Agent": random.choice(user_agents),
        "X-Aduid": "a3be07cd6-dd67-3912-0093-862d844e7fe",
        "X-Request-Id": f"dcc5cb6ab-1bc3-8273-cc26-{random.randint(100000000000, 999999999999)}",
        "X-Signature": "733fd672ddf6d4e367730d9622cdd1e28a4b6203",
        "X-Timestamp": str(int(time.time())),
        "X-Version": "2.77.0",
    }
    return headers

def update_task(task_id: str, status: str, message: str, result: Optional[Dict[str, Any]] = None):
    """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
    if task_id in current_tasks:
        current_tasks[task_id].update({
            "status": status,
            "message": message,
            "result": result,
            "updated_at": datetime.now()
        })

        # æ·»åŠ çŠ¶æ€å˜æ›´æ—¥å¿—
        add_task_log(task_id, f"çŠ¶æ€æ›´æ–°: {message}")

def stop_task(task_id: str) -> bool:
    """åœæ­¢ä»»åŠ¡"""
    if task_id not in current_tasks:
        return False

    task = current_tasks[task_id]

    if task["status"] not in ["pending", "running"]:
        return False

    # è®¾ç½®åœæ­¢æ ‡å¿—
    task_stop_flags[task_id] = True
    add_task_log(task_id, "ğŸ›‘ æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œæ­£åœ¨åœæ­¢ä»»åŠ¡...")

    # å¦‚æœæœ‰çˆ¬è™«å®ä¾‹ï¼Œä¹Ÿè®¾ç½®çˆ¬è™«çš„åœæ­¢æ ‡å¿—
    global crawler_instance, file_downloader_instances
    if crawler_instance:
        crawler_instance.set_stop_flag()

    # å¦‚æœæœ‰æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹ï¼Œä¹Ÿè®¾ç½®åœæ­¢æ ‡å¿—
    if task_id in file_downloader_instances:
        downloader = file_downloader_instances[task_id]
        downloader.set_stop_flag()

    update_task(task_id, "cancelled", "ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")

    return True

def is_task_stopped(task_id: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢"""
    stopped = task_stop_flags.get(task_id, False)
    return stopped

# APIè·¯ç”±å®šä¹‰
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨ API æœåŠ¡", "version": "1.0.0"}

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now()}

@app.get("/api/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    try:
        config = load_config()
        auth_config = (config or {}).get('auth', {}) if config else {}
        cookie = auth_config.get('cookie', '') if auth_config else ''

        configured = is_configured()

        # éšè—æ•æ„Ÿä¿¡æ¯ï¼Œä»…è¿”å›é…ç½®çŠ¶æ€å’Œä¸‹è½½ç›¸å…³é…ç½®
        return {
            "configured": configured,
            "auth": {
                "cookie": "***" if cookie and cookie != "your_cookie_here" else "æœªé…ç½®",
            },
            "database": config.get('database', {}) if config else {},
            "download": config.get('download', {}) if config else {}
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é…ç½®å¤±è´¥: {str(e)}")

@app.post("/api/config")
async def update_config(config: ConfigModel):
    """æ›´æ–°é…ç½®"""
    try:
        # åˆ›å»ºé…ç½®å†…å®¹
        config_content = f"""# çŸ¥è¯†æ˜Ÿçƒæ•°æ®é‡‡é›†å™¨é…ç½®æ–‡ä»¶
# é€šè¿‡Webç•Œé¢è‡ªåŠ¨ç”Ÿæˆ

[auth]
# çŸ¥è¯†æ˜Ÿçƒç™»å½•Cookie
cookie = "{config.cookie}"

[download]
# ä¸‹è½½ç›®å½•
dir = "downloads"
"""

        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = "config.toml"
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)

        # é‡ç½®çˆ¬è™«å®ä¾‹ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®
        global crawler_instance
        crawler_instance = None

        return {"message": "é…ç½®æ›´æ–°æˆåŠŸ", "success": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°é…ç½®å¤±è´¥: {str(e)}")

# è´¦å·ç®¡ç† API
@app.get("/api/accounts")
async def list_accounts():
    """è·å–æ‰€æœ‰è´¦å·åˆ—è¡¨"""
    try:
        sql_mgr = get_accounts_sql_manager()
        accounts = sql_mgr.get_accounts(mask_cookie=True)
        return {"accounts": accounts}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve account list: {str(e)}")

@app.post("/api/accounts")
async def create_account(request: AccountCreateRequest):
    """åˆ›å»ºæ–°è´¦å·"""
    try:
        sql_mgr = get_accounts_sql_manager()
        acc = sql_mgr.add_account(request.cookie, request.name)
        safe_acc = sql_mgr.get_account_by_id(acc.get("id"), mask_cookie=True)
        # æ¸…é™¤è´¦å·ç¾¤ç»„æ£€æµ‹ç¼“å­˜ï¼Œä½¿æ–°è´¦å·çš„ç¾¤ç»„ç«‹å³å¯è§
        clear_account_detect_cache()
        return {"account": safe_acc}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create account: {str(e)}")

@app.delete("/api/accounts/{account_id}")
async def remove_account(account_id: str):
    """åˆ é™¤è´¦å·"""
    try:
        sql_mgr = get_accounts_sql_manager()
        ok = sql_mgr.delete_account(account_id)
        if not ok:
            raise HTTPException(status_code=404, detail="Account does not exist")
        # æ¸…é™¤è´¦å·ç¾¤ç»„æ£€æµ‹ç¼“å­˜
        clear_account_detect_cache()
        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete account: {str(e)}")

@app.post("/api/groups/{group_id}/assign-account")
async def assign_account_to_group(group_id: str, request: AssignGroupAccountRequest):
    """åˆ†é…ç¾¤ç»„åˆ°æŒ‡å®šè´¦å·"""
    try:
        sql_mgr = get_accounts_sql_manager()
        ok, msg = sql_mgr.assign_group_account(group_id, request.account_id)
        if not ok:
            raise HTTPException(status_code=400, detail=msg)
        return {"success": True, "message": msg}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to assign account: {str(e)}")

@app.get("/api/groups/{group_id}/account")
async def get_group_account(group_id: str):
    try:
        summary = get_account_summary_for_group_auto(group_id)
        return {"account": summary}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è´¦å·å¤±è´¥: {str(e)}")

# è´¦å·â€œè‡ªæˆ‘ä¿¡æ¯â€æŒä¹…åŒ– (/v3/users/self)
@app.get("/api/accounts/{account_id}/self")
async def get_account_self(account_id: str):
    """è·å–å¹¶è¿”å›æŒ‡å®šè´¦å·çš„å·²æŒä¹…åŒ–è‡ªæˆ‘ä¿¡æ¯ï¼›è‹¥æ— åˆ™å°è¯•æŠ“å–å¹¶ä¿å­˜"""
    try:
        db = get_account_info_db()
        info = db.get_self_info(account_id)
        if info:
            return {"self": info}

        # è‹¥æ•°æ®åº“æ— è®°å½•åˆ™æŠ“å–
        sql_mgr = get_accounts_sql_manager()
        acc = sql_mgr.get_account_by_id(account_id, mask_cookie=False)
        if not acc:
            raise HTTPException(status_code=404, detail="Account does not exist")

        cookie = acc.get("cookie", "")
        if not cookie:
            raise HTTPException(status_code=400, detail="Account has no configured Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="API returned failure")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"Network request failed: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve account info: {str(e)}")

@app.post("/api/accounts/{account_id}/self/refresh")
async def refresh_account_self(account_id: str):
    """å¼ºåˆ¶æŠ“å– /v3/users/self å¹¶æ›´æ–°æŒä¹…åŒ–"""
    try:
        sql_mgr = get_accounts_sql_manager()
        acc = sql_mgr.get_account_by_id(account_id, mask_cookie=False)
        if not acc:
            raise HTTPException(status_code=404, detail="Account does not exist")

        cookie = acc.get("cookie", "")
        if not cookie:
            raise HTTPException(status_code=400, detail="Account has no configured Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="API returned failure")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db = get_account_info_db()
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"Network request failed: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to refresh account info: {str(e)}")

@app.get("/api/groups/{group_id}/self")
async def get_group_account_self(group_id: str):
    """è·å–ç¾¤ç»„å½“å‰ä½¿ç”¨è´¦å·çš„è‡ªæˆ‘ä¿¡æ¯ï¼ˆè‹¥æ— åˆ™å°è¯•æŠ“å–å¹¶ä¿å­˜ï¼‰"""
    try:
        summary = get_account_summary_for_group_auto(group_id)
        cookie = get_cookie_for_group(group_id)
        account_id = (summary or {}).get('id', 'default')

        if not cookie:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·æˆ–é»˜è®¤Cookie")

        db = get_account_info_db()
        info = db.get_self_info(account_id)
        if info:
            return {"self": info}

        # æŠ“å–å¹¶å†™å…¥
        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.post("/api/groups/{group_id}/self/refresh")
async def refresh_group_account_self(group_id: str):
    """å¼ºåˆ¶æŠ“å–ç¾¤ç»„å½“å‰ä½¿ç”¨è´¦å·çš„è‡ªæˆ‘ä¿¡æ¯å¹¶æŒä¹…åŒ–"""
    try:
        summary = get_account_summary_for_group_auto(group_id)
        cookie = get_cookie_for_group(group_id)
        account_id = (summary or {}).get('id', 'default')

        if not cookie:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·æˆ–é»˜è®¤Cookie")

        headers = build_stealth_headers(cookie)
        resp = requests.get('https://api.zsxq.com/v3/users/self', headers=headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        if not data.get('succeeded'):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        rd = data.get('resp_data', {}) or {}
        user = rd.get('user', {}) or {}
        wechat = (rd.get('accounts', {}) or {}).get('wechat', {}) or {}

        self_info = {
            "uid": user.get("uid"),
            "name": user.get("name") or wechat.get("name"),
            "avatar_url": user.get("avatar_url") or wechat.get("avatar_url"),
            "location": user.get("location"),
            "user_sid": user.get("user_sid"),
            "grade": user.get("grade"),
        }
        db = get_account_info_db()
        db.upsert_self_info(account_id, self_info, raw_json=data)
        return {"self": db.get_self_info(account_id)}
    except HTTPException:
        raise
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ·æ–°ç¾¤ç»„è´¦å·ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.get("/api/database/stats")
async def get_database_stats():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        configured = is_configured()
        if not configured:
            return {
                "configured": False,
                "topic_database": {
                    "stats": {},
                    "timestamp_info": {
                        "total_topics": 0,
                        "oldest_timestamp": "",
                        "newest_timestamp": "",
                        "has_data": False,
                    },
                },
                "file_database": {
                    "stats": {},
                },
            }

        # èšåˆæ‰€æœ‰æœ¬åœ°ç¾¤ç»„çš„æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        path_manager = get_db_path_manager()
        groups_info = path_manager.list_all_groups()

        if not groups_info:
            # å·²é…ç½®ä½†å°šæœªäº§ç”Ÿæœ¬åœ°æ•°æ®
            return {
                "configured": True,
                "topic_database": {
                    "stats": {},
                    "timestamp_info": {
                        "total_topics": 0,
                        "oldest_timestamp": "",
                        "newest_timestamp": "",
                        "has_data": False,
                    },
                },
                "file_database": {
                    "stats": {},
                },
            }

        aggregated_topic_stats: Dict[str, int] = {}
        aggregated_file_stats: Dict[str, int] = {}

        oldest_ts: Optional[str] = None
        newest_ts: Optional[str] = None
        total_topics = 0
        has_data = False

        for gi in groups_info:
            group_id = gi.get("group_id")
            topics_db_path = gi.get("topics_db")
            if not topics_db_path:
                continue

            # è¯é¢˜æ•°æ®åº“ç»Ÿè®¡
            db = ZSXQDatabase(topics_db_path)
            try:
                topic_stats = db.get_database_stats()
                ts_info = db.get_timestamp_range_info()
            finally:
                db.close()

            for table, count in (topic_stats or {}).items():
                aggregated_topic_stats[table] = aggregated_topic_stats.get(table, 0) + int(count or 0)

            if ts_info.get("has_data"):
                has_data = True
                ot = ts_info.get("oldest_timestamp")
                nt = ts_info.get("newest_timestamp")
                if ot:
                    if oldest_ts is None or ot < oldest_ts:
                        oldest_ts = ot
                if nt:
                    if newest_ts is None or nt > newest_ts:
                        newest_ts = nt
                total_topics += int(ts_info.get("total_topics") or 0)

            # æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡ï¼ˆå¦‚å­˜åœ¨ï¼‰
            db_paths = path_manager.list_group_databases(str(group_id))
            files_db_path = db_paths.get("files")
            if files_db_path:
                fdb = ZSXQFileDatabase(files_db_path)
                try:
                    file_stats = fdb.get_database_stats()
                finally:
                    fdb.close()

                for table, count in (file_stats or {}).items():
                    aggregated_file_stats[table] = aggregated_file_stats.get(table, 0) + int(count or 0)

        timestamp_info = {
            "total_topics": total_topics,
            "oldest_timestamp": oldest_ts or "",
            "newest_timestamp": newest_ts or "",
            "has_data": has_data,
        }

        return {
            "configured": True,
            "topic_database": {
                "stats": aggregated_topic_stats,
                "timestamp_info": timestamp_info,
            },
            "file_database": {
                "stats": aggregated_file_stats,
            },
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.get("/api/tasks")
async def get_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    return list(current_tasks.values())

@app.get("/api/tasks/{task_id}")
async def get_task(task_id: str):
    """è·å–ç‰¹å®šä»»åŠ¡çŠ¶æ€"""
    if task_id not in current_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return current_tasks[task_id]

@app.post("/api/tasks/{task_id}/stop")
async def stop_task_api(task_id: str):
    """åœæ­¢ä»»åŠ¡"""
    if stop_task(task_id):
        return {"message": "ä»»åŠ¡åœæ­¢è¯·æ±‚å·²å‘é€", "task_id": task_id}
    else:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•åœæ­¢")

# åå°ä»»åŠ¡æ‰§è¡Œå‡½æ•°
def run_crawl_historical_task(task_id: str, group_id: str, pages: int, per_page: int, crawl_settings: CrawlHistoricalRequest = None):
    """åå°æ‰§è¡Œå†å²æ•°æ®çˆ¬å–ä»»åŠ¡"""
    try:
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        update_task(task_id, "running", f"å¼€å§‹çˆ¬å–å†å²æ•°æ® {pages} é¡µ...")
        add_task_log(task_id, f"ğŸš€ å¼€å§‹è·å–å†å²æ•°æ®ï¼Œ{pages} é¡µï¼Œæ¯é¡µ {per_page} æ¡")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        # è®¾ç½®æ—¥å¿—å›è°ƒå‡½æ•°
        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)
        # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        crawler.stop_check_func = stop_check

        # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
        if crawl_settings:
            crawler.set_custom_intervals(
                crawl_interval_min=crawl_settings.crawlIntervalMin,
                crawl_interval_max=crawl_settings.crawlIntervalMax,
                long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                pages_per_batch=crawl_settings.pagesPerBatch
            )

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
        add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        result = crawler.crawl_incremental(pages, per_page)

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
        if result and result.get('expired'):
            add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {result.get('message', 'æˆå‘˜ä½“éªŒå·²åˆ°æœŸ')}")
            update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", {"expired": True, "code": result.get('code'), "message": result.get('message')})
            return

        add_task_log(task_id, f"âœ… è·å–å®Œæˆï¼æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}")
        update_task(task_id, "completed", "å†å²æ•°æ®çˆ¬å–å®Œæˆ", result)
    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ è·å–å¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"çˆ¬å–å¤±è´¥: {str(e)}")

def run_file_download_task(task_id: str, group_id: str, max_files: Optional[int], sort_by: str,
                          download_interval: float = 1.0, long_sleep_interval: float = 60.0,
                          files_per_batch: int = 10, download_interval_min: Optional[float] = None,
                          download_interval_max: Optional[float] = None,
                          long_sleep_interval_min: Optional[float] = None,
                          long_sleep_interval_max: Optional[float] = None):
    """åå°æ‰§è¡Œæ–‡ä»¶ä¸‹è½½ä»»åŠ¡"""
    try:
        update_task(task_id, "running", "å¼€å§‹æ–‡ä»¶ä¸‹è½½...")

        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
        from zsxq_file_downloader import ZSXQFileDownloader
        from db_path_manager import get_db_path_manager

        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        # è·å–wecomé…ç½®
        config = load_config()
        wecom_webhook_url = None
        wecom_enabled = True
        if config:
            wecom_config = config.get('wecom_webhook', {})
            if isinstance(wecom_config, dict):
                wecom_webhook_url = wecom_config.get('webhook_url')
                wecom_enabled = wecom_config.get('enabled', True)

        # åˆ›å»ºwecom_webhookå®ä¾‹
        wecom_webhook_instance = None
        if wecom_webhook_url:
            try:
                # âœ… ä¿®å¤ï¼šåªä¼ é€’æ”¯æŒçš„å‚æ•°
                wecom_webhook_instance = WeComWebhook(wecom_webhook_url, enabled=wecom_enabled)
                add_task_log(task_id, "ğŸ“± ä¼ä¸šå¾®ä¿¡Webhookå·²å¯ç”¨")
            except Exception as e:
                add_task_log(task_id, f"âš ï¸ åˆ›å»ºwecom_webhookå®ä¾‹å¤±è´¥: {e}")
        else:
            add_task_log(task_id, "âš ï¸ æœªé…ç½®ä¼ä¸šå¾®ä¿¡Webhook URL")

        # ä¿®æ”¹ä¸‹è½½å™¨åˆ›å»ºä»£ç ï¼Œæ·»åŠ wecom_webhookå‚æ•°
        downloader = ZSXQFileDownloader(
            cookie=cookie,
            group_id=group_id,
            db_path=db_path,
            download_interval=download_interval,
            long_sleep_interval=long_sleep_interval,
            files_per_batch=files_per_batch,
            download_interval_min=download_interval_min,
            download_interval_max=download_interval_max,
            long_sleep_interval_min=long_sleep_interval_min,
            long_sleep_interval_max=long_sleep_interval_max,
            wecom_webhook=wecom_webhook_instance,
            log_callback=log_callback
        )
        # è®¾ç½®æ—¥å¿—å›è°ƒå’Œåœæ­¢æ£€æŸ¥å‡½æ•°
        downloader.log_callback = log_callback
        downloader.stop_check_func = stop_check

        add_task_log(task_id, f"âš™ï¸ ä¸‹è½½é…ç½®:")
        add_task_log(task_id, f"   â±ï¸ å•æ¬¡ä¸‹è½½é—´éš”: {download_interval}ç§’")
        add_task_log(task_id, f"   ğŸ˜´ é•¿ä¼‘çœ é—´éš”: {long_sleep_interval}ç§’")
        add_task_log(task_id, f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {files_per_batch}ä¸ªæ–‡ä»¶")

        # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
        global file_downloader_instances
        file_downloader_instances[task_id] = downloader

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
        add_task_log(task_id, "ğŸ” å¼€å§‹æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")

        # å…ˆæ”¶é›†æ–‡ä»¶åˆ—è¡¨
        collect_result = downloader.collect_incremental_files()

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        add_task_log(task_id, f"ğŸ“Š æ–‡ä»¶æ”¶é›†å®Œæˆ: {collect_result}")
        add_task_log(task_id, "ğŸš€ å¼€å§‹ä¸‹è½½æ–‡ä»¶...")

        # æ ¹æ®æ’åºæ–¹å¼ä¸‹è½½æ–‡ä»¶
        if sort_by == "download_count":
            result = downloader.download_files_from_database(max_files=max_files, status_filter='pending')
        else:
            result = downloader.download_files_from_database(max_files=max_files, status_filter='pending')

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
        if is_task_stopped(task_id):
            return

        add_task_log(task_id, f"âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆï¼")
        update_task(task_id, "completed", "æ–‡ä»¶ä¸‹è½½å®Œæˆ", {"downloaded_files": result})
    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
        if task_id in file_downloader_instances:
            del file_downloader_instances[task_id]

def run_single_file_download_task(task_id: str, group_id: str, file_id: int):
    """è¿è¡Œå•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡"""
    try:
        update_task(task_id, "running", f"å¼€å§‹ä¸‹è½½æ–‡ä»¶ (ID: {file_id})...")

        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # åˆ›å»ºæ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        from zsxq_file_downloader import ZSXQFileDownloader
        from db_path_manager import get_db_path_manager

        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        downloader = ZSXQFileDownloader(
            cookie=cookie,
            group_id=group_id,
            db_path=db_path
        )
        # è®¾ç½®æ—¥å¿—å›è°ƒå’Œåœæ­¢æ£€æŸ¥å‡½æ•°
        downloader.log_callback = log_callback
        downloader.stop_check_func = stop_check

        # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
        global file_downloader_instances
        file_downloader_instances[task_id] = downloader

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        # å°è¯•ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯
        downloader.file_db.cursor.execute('''
            SELECT file_id, name, size, download_count
            FROM files
            WHERE file_id = ?
        ''', (file_id,))

        result = downloader.file_db.cursor.fetchone()

        if result:
            # å¦‚æœæ•°æ®åº“ä¸­æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨æ•°æ®åº“ä¿¡æ¯
            file_id_db, file_name, file_size, download_count = result
            add_task_log(task_id, f"ğŸ“„ ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯: {file_name} ({file_size} bytes)")

            # æ„é€ æ–‡ä»¶ä¿¡æ¯ç»“æ„
            file_info = {
                'file': {
                    'id': file_id,
                    'name': file_name,
                    'size': file_size,
                    'download_count': download_count
                }
            }
        else:
            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ–‡ä»¶ä¿¡æ¯ï¼Œç›´æ¥å°è¯•ä¸‹è½½
            add_task_log(task_id, f"ğŸ“„ æ•°æ®åº“ä¸­æ— æ–‡ä»¶ä¿¡æ¯ï¼Œå°è¯•ç›´æ¥ä¸‹è½½æ–‡ä»¶ ID: {file_id}")

            # æ„é€ æœ€å°æ–‡ä»¶ä¿¡æ¯ç»“æ„
            file_info = {
                'file': {
                    'id': file_id,
                    'name': f'file_{file_id}',  # ä½¿ç”¨é»˜è®¤æ–‡ä»¶å
                    'size': 0,  # æœªçŸ¥å¤§å°
                    'download_count': 0
                }
            }

        # ä¸‹è½½æ–‡ä»¶
        result = downloader.download_file(file_info)

        if result == "skipped":
            add_task_log(task_id, "âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            update_task(task_id, "completed", "æ–‡ä»¶å·²å­˜åœ¨")
        elif result:
            add_task_log(task_id, "âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ")

            # è·å–å®é™…ä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯
            actual_file_info = file_info['file']
            actual_file_name = actual_file_info.get('name', f'file_{file_id}')
            actual_file_size = actual_file_info.get('size', 0)

            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶è·å–å®é™…å¤§å°
            import os
            safe_filename = "".join(c for c in actual_file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
            if not safe_filename:
                safe_filename = f"file_{file_id}"
            local_path = os.path.join(downloader.download_dir, safe_filename)

            if os.path.exists(local_path):
                actual_file_size = os.path.getsize(local_path)

            # æ›´æ–°æˆ–æ’å…¥æ–‡ä»¶çŠ¶æ€
            downloader.file_db.cursor.execute('''
                INSERT OR REPLACE INTO files
                (file_id, name, size, download_status, local_path, download_time, download_count)
                VALUES (?, ?, ?, 'downloaded', ?, CURRENT_TIMESTAMP, ?)
            ''', (file_id, actual_file_name, actual_file_size, local_path,
                  actual_file_info.get('download_count', 0)))
            downloader.file_db.conn.commit()

            update_task(task_id, "completed", "ä¸‹è½½æˆåŠŸ")
        else:
            add_task_log(task_id, "âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            update_task(task_id, "failed", "ä¸‹è½½å¤±è´¥")

    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"ä»»åŠ¡å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
        if task_id in file_downloader_instances:
            del file_downloader_instances[task_id]

def run_single_file_download_task_with_info(task_id: str, group_id: str, file_id: int,
                                           file_name: Optional[str] = None, file_size: Optional[int] = None):
    """è¿è¡Œå•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡ï¼ˆå¸¦æ–‡ä»¶ä¿¡æ¯ï¼‰"""
    try:
        update_task(task_id, "running", f"å¼€å§‹ä¸‹è½½æ–‡ä»¶ (ID: {file_id})...")

        def log_callback(message: str):
            add_task_log(task_id, message)

        # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
        def stop_check():
            return is_task_stopped(task_id)

        # åˆ›å»ºæ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        from zsxq_file_downloader import ZSXQFileDownloader
        from db_path_manager import get_db_path_manager

        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        downloader = ZSXQFileDownloader(
            cookie=cookie,
            group_id=group_id,
            db_path=db_path
        )
        # è®¾ç½®æ—¥å¿—å›è°ƒå’Œåœæ­¢æ£€æŸ¥å‡½æ•°
        downloader.log_callback = log_callback
        downloader.stop_check_func = stop_check

        # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
        global file_downloader_instances
        file_downloader_instances[task_id] = downloader

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
        if is_task_stopped(task_id):
            add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
            return

        # æ„é€ æ–‡ä»¶ä¿¡æ¯ç»“æ„
        if file_name and file_size:
            add_task_log(task_id, f"ğŸ“„ ä½¿ç”¨æä¾›çš„æ–‡ä»¶ä¿¡æ¯: {file_name} ({file_size} bytes)")
            file_info = {
                'file': {
                    'id': file_id,
                    'name': file_name,
                    'size': file_size,
                    'download_count': 0
                }
            }
        else:
            # å°è¯•ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯
            downloader.file_db.cursor.execute('''
                SELECT file_id, name, size, download_count
                FROM files
                WHERE file_id = ?
            ''', (file_id,))

            result = downloader.file_db.cursor.fetchone()

            if result:
                file_id_db, db_file_name, db_file_size, download_count = result
                add_task_log(task_id, f"ğŸ“„ ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯: {db_file_name} ({db_file_size} bytes)")
                file_info = {
                    'file': {
                        'id': file_id,
                        'name': db_file_name,
                        'size': db_file_size,
                        'download_count': download_count
                    }
                }
            else:
                add_task_log(task_id, f"ğŸ“„ ç›´æ¥ä¸‹è½½æ–‡ä»¶ ID: {file_id}")
                file_info = {
                    'file': {
                        'id': file_id,
                        'name': f'file_{file_id}',
                        'size': 0,
                        'download_count': 0
                    }
                }

        # ä¸‹è½½æ–‡ä»¶
        result = downloader.download_file(file_info)

        if result == "skipped":
            add_task_log(task_id, "âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            update_task(task_id, "completed", "æ–‡ä»¶å·²å­˜åœ¨")
        elif result:
            add_task_log(task_id, "âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ")

            # è·å–å®é™…ä¸‹è½½çš„æ–‡ä»¶ä¿¡æ¯
            actual_file_info = file_info['file']
            actual_file_name = actual_file_info.get('name', f'file_{file_id}')
            actual_file_size = actual_file_info.get('size', 0)

            # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶è·å–å®é™…å¤§å°
            import os
            safe_filename = "".join(c for c in actual_file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
            if not safe_filename:
                safe_filename = f"file_{file_id}"
            local_path = os.path.join(downloader.download_dir, safe_filename)

            if os.path.exists(local_path):
                actual_file_size = os.path.getsize(local_path)

            # æ›´æ–°æˆ–æ’å…¥æ–‡ä»¶çŠ¶æ€
            downloader.file_db.cursor.execute('''
                INSERT OR REPLACE INTO files
                (file_id, name, size, download_status, local_path, download_time, download_count)
                VALUES (?, ?, ?, 'downloaded', ?, CURRENT_TIMESTAMP, ?)
            ''', (file_id, actual_file_name, actual_file_size, local_path,
                  actual_file_info.get('download_count', 0)))
            downloader.file_db.conn.commit()

            update_task(task_id, "completed", "ä¸‹è½½æˆåŠŸ")
        else:
            add_task_log(task_id, "âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            update_task(task_id, "failed", "ä¸‹è½½å¤±è´¥")

    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"ä»»åŠ¡å¤±è´¥: {str(e)}")
    finally:
        # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
        if task_id in file_downloader_instances:
            del file_downloader_instances[task_id]

# ç¾¤ç»„ç›¸å…³è¾…åŠ©å‡½æ•°
def fetch_groups_from_api(cookie: str) -> List[dict]:
    """ä»çŸ¥è¯†æ˜ŸçƒAPIè·å–ç¾¤ç»„åˆ—è¡¨"""
    import requests

    # å¦‚æœæ˜¯æµ‹è¯•Cookieï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
    if cookie == "test_cookie":
        return [
            {
                "group_id": 123456,
                "name": "æµ‹è¯•çŸ¥è¯†æ˜Ÿçƒç¾¤ç»„",
                "type": "public",
                "background_url": "https://via.placeholder.com/400x200/4f46e5/ffffff?text=Test+Group",
                "description": "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•çš„çŸ¥è¯†æ˜Ÿçƒç¾¤ç»„ï¼ŒåŒ…å«å„ç§æŠ€æœ¯è®¨è®ºå’Œå­¦ä¹ èµ„æºåˆ†äº«ã€‚",
                "create_time": "2023-01-15T10:30:00+08:00",
                "subscription_time": "2024-01-01T00:00:00+08:00",
                "expiry_time": "2024-12-31T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1001,
                    "name": "æµ‹è¯•ç¾¤ä¸»",
                    "avatar_url": "https://via.placeholder.com/64x64/10b981/ffffff?text=Owner"
                },
                "statistics": {
                    "members_count": 1250,
                    "topics_count": 89,
                    "files_count": 156
                }
            },
            {
                "group_id": 789012,
                "name": "æŠ€æœ¯äº¤æµç¾¤",
                "type": "private",
                "background_url": "https://via.placeholder.com/400x200/059669/ffffff?text=Tech+Group",
                "description": "ä¸“æ³¨äºå‰ç«¯ã€åç«¯ã€ç§»åŠ¨å¼€å‘ç­‰æŠ€æœ¯é¢†åŸŸçš„æ·±åº¦äº¤æµä¸å®è·µåˆ†äº«ã€‚",
                "create_time": "2023-03-20T14:15:00+08:00",
                "subscription_time": "2024-02-15T00:00:00+08:00",
                "expiry_time": "2025-02-14T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1002,
                    "name": "æŠ€æœ¯ä¸“å®¶",
                    "avatar_url": "https://via.placeholder.com/64x64/dc2626/ffffff?text=Tech"
                },
                "statistics": {
                    "members_count": 856,
                    "topics_count": 234,
                    "files_count": 67
                }
            },
            {
                "group_id": 345678,
                "name": "äº§å“è®¾è®¡è®¨è®º",
                "type": "public",
                "background_url": "https://via.placeholder.com/400x200/7c3aed/ffffff?text=Design+Group",
                "description": "UI/UXè®¾è®¡ã€äº§å“æ€ç»´ã€ç”¨æˆ·ä½“éªŒç­‰è®¾è®¡ç›¸å…³è¯é¢˜çš„ä¸“ä¸šè®¨è®ºç¤¾åŒºã€‚",
                "create_time": "2023-06-10T09:45:00+08:00",
                "subscription_time": "2024-03-01T00:00:00+08:00",
                "expiry_time": "2024-08-31T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1003,
                    "name": "è®¾è®¡å¸ˆ",
                    "avatar_url": "https://via.placeholder.com/64x64/ea580c/ffffff?text=Design"
                },
                "statistics": {
                    "members_count": 432,
                    "topics_count": 156,
                    "files_count": 89
                }
            },
            {
                "group_id": 456789,
                "name": "åˆ›ä¸šæŠ•èµ„åœˆ",
                "type": "private",
                "background_url": "https://via.placeholder.com/400x200/dc2626/ffffff?text=Startup",
                "description": "åˆ›ä¸šè€…ã€æŠ•èµ„äººã€è¡Œä¸šä¸“å®¶çš„äº¤æµå¹³å°ï¼Œåˆ†äº«åˆ›ä¸šç»éªŒå’ŒæŠ•èµ„è§è§£ã€‚",
                "create_time": "2023-08-05T16:20:00+08:00",
                "subscription_time": "2024-01-10T00:00:00+08:00",
                "expiry_time": "2024-07-09T23:59:59+08:00",
                "status": "expiring_soon",
                "owner": {
                    "user_id": 1004,
                    "name": "æŠ•èµ„äºº",
                    "avatar_url": "https://via.placeholder.com/64x64/f59e0b/ffffff?text=VC"
                },
                "statistics": {
                    "members_count": 298,
                    "topics_count": 78,
                    "files_count": 45
                }
            },
            {
                "group_id": 567890,
                "name": "AIäººå·¥æ™ºèƒ½ç ”ç©¶",
                "type": "public",
                "background_url": "https://via.placeholder.com/400x200/06b6d4/ffffff?text=AI+Research",
                "description": "äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰å‰æ²¿æŠ€æœ¯çš„ç ”ç©¶ä¸åº”ç”¨è®¨è®ºã€‚",
                "create_time": "2023-09-12T11:30:00+08:00",
                "subscription_time": "2024-04-01T00:00:00+08:00",
                "expiry_time": "2025-03-31T23:59:59+08:00",
                "status": "active",
                "owner": {
                    "user_id": 1005,
                    "name": "AIç ”ç©¶å‘˜",
                    "avatar_url": "https://via.placeholder.com/64x64/8b5cf6/ffffff?text=AI"
                },
                "statistics": {
                    "members_count": 1876,
                    "topics_count": 345,
                    "files_count": 234
                }
            }
        ]

    headers = build_stealth_headers(cookie)

    try:
        response = requests.get('https://api.zsxq.com/v2/groups', headers=headers, timeout=30)
        response.raise_for_status()

        data = response.json()
        if data.get('succeeded'):
            return data.get('resp_data', {}).get('groups', [])
        else:
            raise Exception(f"APIè¿”å›å¤±è´¥: {data.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
    except requests.RequestException as e:
        raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise Exception(f"è·å–ç¾¤ç»„åˆ—è¡¨å¤±è´¥: {str(e)}")

# çˆ¬å–ç›¸å…³APIè·¯ç”±
@app.post("/api/crawl/historical/{group_id}")
async def crawl_historical(group_id: str, request: CrawlHistoricalRequest, background_tasks: BackgroundTasks):
    """çˆ¬å–å†å²æ•°æ®"""
    try:
        task_id = create_task("crawl_historical", f"çˆ¬å–å†å²æ•°æ® {request.pages} é¡µ (ç¾¤ç»„: {group_id})")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_historical_task, task_id, group_id, request.pages, request.per_page, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºçˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/crawl/all/{group_id}")
async def crawl_all(group_id: str, request: CrawlSettingsRequest, background_tasks: BackgroundTasks):
    """å…¨é‡çˆ¬å–æ‰€æœ‰å†å²æ•°æ®"""
    try:
        task_id = create_task("crawl_all", f"å…¨é‡çˆ¬å–æ‰€æœ‰å†å²æ•°æ® (ç¾¤ç»„: {group_id})")

        def run_crawl_all_task(task_id: str, group_id: str, crawl_settings: CrawlSettingsRequest = None):
            try:
                update_task(task_id, "running", "å¼€å§‹å…¨é‡çˆ¬å–...")
                add_task_log(task_id, "ğŸš€ å¼€å§‹å…¨é‡çˆ¬å–...")
                add_task_log(task_id, "âš ï¸ è­¦å‘Šï¼šæ­¤æ¨¡å¼å°†æŒç»­çˆ¬å–ç›´åˆ°æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´")

                # åˆ›å»ºæ—¥å¿—å›è°ƒå‡½æ•°
                def log_callback(message):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºè¿™ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹ï¼ˆå¸¦æ—¥å¿—å›è°ƒï¼‰ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
                cookie = get_cookie_for_group(group_id)
                # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
                path_manager = get_db_path_manager()
                db_path = path_manager.get_topics_db_path(group_id)

                crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                crawler.stop_check_func = stop_check

                # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
                if crawl_settings:
                    crawler.set_custom_intervals(
                        crawl_interval_min=crawl_settings.crawlIntervalMin,
                        crawl_interval_max=crawl_settings.crawlIntervalMax,
                        long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                        long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                        pages_per_batch=crawl_settings.pagesPerBatch
                    )

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                # è·å–æ•°æ®åº“çŠ¶æ€
                db_stats = crawler.db.get_database_stats()
                add_task_log(task_id, f"ğŸ“Š å½“å‰æ•°æ®åº“çŠ¶æ€: è¯é¢˜: {db_stats.get('topics', 0)}, ç”¨æˆ·: {db_stats.get('users', 0)}")

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                add_task_log(task_id, "ğŸŒŠ å¼€å§‹æ— é™å†å²çˆ¬å–...")
                result = crawler.crawl_all_historical(per_page=20, auto_confirm=True)

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                if result and result.get('expired'):
                    add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {result.get('message', 'æˆå‘˜ä½“éªŒå·²åˆ°æœŸ')}")
                    update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", {"expired": True, "code": result.get('code'), "message": result.get('message')})
                    return

                add_task_log(task_id, f"ğŸ‰ å…¨é‡çˆ¬å–å®Œæˆï¼")
                add_task_log(task_id, f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}, æ€»é¡µæ•°: {result.get('pages', 0)}")
                update_task(task_id, "completed", "å…¨é‡çˆ¬å–å®Œæˆ", result)
            except Exception as e:
                add_task_log(task_id, f"âŒ å…¨é‡çˆ¬å–å¤±è´¥: {str(e)}")
                update_task(task_id, "failed", f"å…¨é‡çˆ¬å–å¤±è´¥: {str(e)}")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_all_task, task_id, group_id, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå…¨é‡çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/crawl/incremental/{group_id}")
async def crawl_incremental(group_id: str, request: CrawlHistoricalRequest, background_tasks: BackgroundTasks):
    """å¢é‡çˆ¬å–å†å²æ•°æ®"""
    try:
        task_id = create_task("crawl_incremental", f"å¢é‡çˆ¬å–å†å²æ•°æ® {request.pages} é¡µ (ç¾¤ç»„: {group_id})")

        def run_crawl_incremental_task(task_id: str, group_id: str, pages: int, per_page: int, crawl_settings: CrawlHistoricalRequest = None):
            try:
                update_task(task_id, "running", "å¼€å§‹å¢é‡çˆ¬å–...")

                def log_callback(message: str):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹
                cookie = get_cookie_for_group(group_id)
                # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
                path_manager = get_db_path_manager()
                db_path = path_manager.get_topics_db_path(group_id)

                crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                crawler.stop_check_func = stop_check

                # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
                if crawl_settings:
                    crawler.set_custom_intervals(
                        crawl_interval_min=crawl_settings.crawlIntervalMin,
                        crawl_interval_max=crawl_settings.crawlIntervalMax,
                        long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                        long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                        pages_per_batch=crawl_settings.pagesPerBatch
                    )

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

                result = crawler.crawl_incremental(pages, per_page)

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                add_task_log(task_id, f"âœ… å¢é‡çˆ¬å–å®Œæˆï¼æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}")
                update_task(task_id, "completed", "å¢é‡çˆ¬å–å®Œæˆ", result)
            except Exception as e:
                if not is_task_stopped(task_id):
                    add_task_log(task_id, f"âŒ å¢é‡çˆ¬å–å¤±è´¥: {str(e)}")
                    update_task(task_id, "failed", f"å¢é‡çˆ¬å–å¤±è´¥: {str(e)}")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_incremental_task, task_id, group_id, request.pages, request.per_page, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¢é‡çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/crawl/latest-until-complete/{group_id}")
async def crawl_latest_until_complete(group_id: str, request: CrawlSettingsRequest, background_tasks: BackgroundTasks):
    """è·å–æœ€æ–°è®°å½•ï¼šæ™ºèƒ½å¢é‡æ›´æ–°"""
    try:
        task_id = create_task("crawl_latest_until_complete", f"è·å–æœ€æ–°è®°å½• (ç¾¤ç»„: {group_id})")

        def run_crawl_latest_task(task_id: str, group_id: str, crawl_settings: CrawlSettingsRequest = None):
            try:
                update_task(task_id, "running", "å¼€å§‹è·å–æœ€æ–°è®°å½•...")

                def log_callback(message: str):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹ï¼Œä½¿ç”¨ä¼ å…¥çš„group_id
                cookie = get_cookie_for_group(group_id)
                # ä½¿ç”¨ä¼ å…¥çš„group_idè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
                path_manager = get_db_path_manager()
                db_path = path_manager.get_topics_db_path(group_id)
                
                # âœ… æ·»åŠ ï¼šè¯»å–ä¼ä¸šå¾®ä¿¡webhooké…ç½®
                config = load_config()
                wecom_webhook_url = None
                wecom_enabled = True
                if config:
                    wecom_config = config.get('wecom_webhook', {})
                    if isinstance(wecom_config, dict):
                        wecom_webhook_url = wecom_config.get('webhook_url')
                        wecom_enabled = wecom_config.get('enabled', True)
                
                # âœ… ä¿®æ”¹ï¼šä¼ é€’webhookå‚æ•°
                crawler = ZSXQInteractiveCrawler(
                    cookie, 
                    group_id, 
                    db_path, 
                    log_callback,
                    wecom_webhook_url=wecom_webhook_url,
                    wecom_enabled=wecom_enabled
                )
        
                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                crawler.stop_check_func = stop_check

                # è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°
                if crawl_settings:
                    crawler.set_custom_intervals(
                        crawl_interval_min=crawl_settings.crawlIntervalMin,
                        crawl_interval_max=crawl_settings.crawlIntervalMax,
                        long_sleep_interval_min=crawl_settings.longSleepIntervalMin,
                        long_sleep_interval_max=crawl_settings.longSleepIntervalMax,
                        pages_per_batch=crawl_settings.pagesPerBatch
                    )

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                add_task_log(task_id, "ğŸ” æ£€æŸ¥æ•°æ®åº“çŠ¶æ€...")

                result = crawler.crawl_latest_until_complete()

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                if result and result.get('expired'):
                    add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {result.get('message', 'æˆå‘˜ä½“éªŒå·²åˆ°æœŸ')}")
                    update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", {"expired": True, "code": result.get('code'), "message": result.get('message')})
                    return

                add_task_log(task_id, f"âœ… è·å–æœ€æ–°è®°å½•å®Œæˆï¼æ–°å¢è¯é¢˜: {result.get('new_topics', 0)}, æ›´æ–°è¯é¢˜: {result.get('updated_topics', 0)}")
                update_task(task_id, "completed", "è·å–æœ€æ–°è®°å½•å®Œæˆ", result)
            except Exception as e:
                if not is_task_stopped(task_id):
                    add_task_log(task_id, f"âŒ è·å–æœ€æ–°è®°å½•å¤±è´¥: {str(e)}")
                    update_task(task_id, "failed", f"è·å–æœ€æ–°è®°å½•å¤±è´¥: {str(e)}")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawl_latest_task, task_id, group_id, request)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºè·å–æœ€æ–°è®°å½•ä»»åŠ¡å¤±è´¥: {str(e)}")

# æ–‡ä»¶ç›¸å…³APIè·¯ç”±
@app.post("/api/files/collect/{group_id}")
async def collect_files(group_id: str, background_tasks: BackgroundTasks):
    """æ”¶é›†æ–‡ä»¶åˆ—è¡¨"""
    try:
        task_id = create_task("collect_files", "æ”¶é›†æ–‡ä»¶åˆ—è¡¨")

        def run_collect_files_task(task_id: str, group_id: str):
            try:
                update_task(task_id, "running", "å¼€å§‹æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")

                def log_callback(message: str):
                    add_task_log(task_id, message)

                # è®¾ç½®åœæ­¢æ£€æŸ¥å‡½æ•°
                def stop_check():
                    return is_task_stopped(task_id)

                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æ–‡ä»¶ä¸‹è½½å™¨å®ä¾‹
                cookie = get_cookie_for_group(group_id)

                from zsxq_file_downloader import ZSXQFileDownloader
                from db_path_manager import get_db_path_manager

                path_manager = get_db_path_manager()
                db_path = path_manager.get_files_db_path(group_id)

                downloader = ZSXQFileDownloader(cookie, group_id, db_path)
                downloader.log_callback = log_callback
                downloader.stop_check_func = stop_check

                # å°†ä¸‹è½½å™¨å®ä¾‹å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
                global file_downloader_instances
                file_downloader_instances[task_id] = downloader

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è®¾ç½®è¿‡ç¨‹ä¸­è¢«åœæ­¢
                if is_task_stopped(task_id):
                    add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­è¢«åœæ­¢")
                    return

                add_task_log(task_id, "ğŸ“¡ è¿æ¥åˆ°çŸ¥è¯†æ˜ŸçƒAPI...")
                result = downloader.collect_incremental_files()

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«åœæ­¢
                if is_task_stopped(task_id):
                    return

                add_task_log(task_id, f"âœ… æ–‡ä»¶åˆ—è¡¨æ”¶é›†å®Œæˆï¼")
                update_task(task_id, "completed", "æ–‡ä»¶åˆ—è¡¨æ”¶é›†å®Œæˆ", result)
            except Exception as e:
                if not is_task_stopped(task_id):
                    add_task_log(task_id, f"âŒ æ–‡ä»¶åˆ—è¡¨æ”¶é›†å¤±è´¥: {str(e)}")
                    update_task(task_id, "failed", f"æ–‡ä»¶åˆ—è¡¨æ”¶é›†å¤±è´¥: {str(e)}")
            finally:
                # æ¸…ç†ä¸‹è½½å™¨å®ä¾‹
                if task_id in file_downloader_instances:
                    del file_downloader_instances[task_id]

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_collect_files_task, task_id, group_id)

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ–‡ä»¶æ”¶é›†ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/files/download/{group_id}")
async def download_files(group_id: str, request: FileDownloadRequest, background_tasks: BackgroundTasks):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        task_id = create_task("download_files", f"ä¸‹è½½æ–‡ä»¶ (æ’åº: {request.sort_by})")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            run_file_download_task,
            task_id,
            group_id,
            request.max_files,
            request.sort_by,
            request.download_interval,
            request.long_sleep_interval,
            request.files_per_batch,
            request.download_interval_min,
            request.download_interval_max,
            request.long_sleep_interval_min,
            request.long_sleep_interval_max
        )

        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ–‡ä»¶ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/files/download-single/{group_id}/{file_id}")
async def download_single_file(group_id: str, file_id: int, background_tasks: BackgroundTasks,
                              file_name: Optional[str] = None, file_size: Optional[int] = None):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        task_id = create_task("download_single_file", f"ä¸‹è½½å•ä¸ªæ–‡ä»¶ (ID: {file_id})")

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            run_single_file_download_task_with_info,
            task_id,
            group_id,
            file_id,
            file_name,
            file_size
        )

        return {"task_id": task_id, "message": "å•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡å·²åˆ›å»º"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå•ä¸ªæ–‡ä»¶ä¸‹è½½ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.get("/api/files/status/{group_id}/{file_id}")
async def get_file_status(group_id: str, file_id: int):
    """è·å–æ–‡ä»¶ä¸‹è½½çŠ¶æ€"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        # æŸ¥è¯¢æ–‡ä»¶ä¿¡æ¯
        downloader.file_db.cursor.execute('''
            SELECT name, size, download_status
            FROM files
            WHERE file_id = ?
        ''', (file_id,))

        result = downloader.file_db.cursor.fetchone()

        if not result:
            # æ–‡ä»¶ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŒåæ–‡ä»¶åœ¨ä¸‹è½½ç›®å½•
            import os
            download_dir = downloader.download_dir

            # å°è¯•ä»è¯é¢˜è¯¦æƒ…ä¸­è·å–æ–‡ä»¶åï¼ˆè¿™é‡Œéœ€è¦é¢å¤–çš„é€»è¾‘ï¼‰
            # æš‚æ—¶è¿”å›æ–‡ä»¶ä¸å­˜åœ¨çš„çŠ¶æ€
            return {
                "file_id": file_id,
                "name": f"file_{file_id}",
                "size": 0,
                "download_status": "not_collected",
                "local_exists": False,
                "local_size": 0,
                "local_path": None,
                "is_complete": False,
                "message": "æ–‡ä»¶ä¿¡æ¯æœªæ”¶é›†ï¼Œè¯·å…ˆè¿è¡Œæ–‡ä»¶æ”¶é›†ä»»åŠ¡"
            }

        file_name, file_size, download_status = result

        # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        safe_filename = "".join(c for c in file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
        if not safe_filename:
            safe_filename = f"file_{file_id}"

        download_dir = downloader.download_dir
        file_path = os.path.join(download_dir, safe_filename)

        local_exists = os.path.exists(file_path)
        local_size = os.path.getsize(file_path) if local_exists else 0

        return {
            "file_id": file_id,
            "name": file_name,
            "size": file_size,
            "download_status": download_status or "pending",
            "local_exists": local_exists,
            "local_size": local_size,
            "local_path": file_path if local_exists else None,
            "is_complete": local_exists and local_size == file_size
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶çŠ¶æ€å¤±è´¥: {str(e)}")

@app.get("/api/files/check-local/{group_id}")
async def check_local_file_status(group_id: str, file_name: str, file_size: int):
    """æ£€æŸ¥æœ¬åœ°æ–‡ä»¶çŠ¶æ€ï¼ˆä¸ä¾èµ–æ•°æ®åº“ï¼‰"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        # æ¸…ç†æ–‡ä»¶å
        import os
        safe_filename = "".join(c for c in file_name if c.isalnum() or c in '._-ï¼ˆï¼‰()[]{}')
        if not safe_filename:
            safe_filename = file_name

        download_dir = downloader.download_dir
        file_path = os.path.join(download_dir, safe_filename)

        local_exists = os.path.exists(file_path)
        local_size = os.path.getsize(file_path) if local_exists else 0

        return {
            "file_name": file_name,
            "safe_filename": safe_filename,
            "expected_size": file_size,
            "local_exists": local_exists,
            "local_size": local_size,
            "local_path": file_path if local_exists else None,
            "is_complete": local_exists and (file_size == 0 or local_size == file_size),
            "download_dir": download_dir
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/api/files/stats/{group_id}")
async def get_file_stats(group_id: str):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    crawler = None
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        # è·å–æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡
        stats = downloader.file_db.get_database_stats()

        # è·å–ä¸‹è½½çŠ¶æ€ç»Ÿè®¡
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰download_statusåˆ—
        downloader.file_db.cursor.execute("PRAGMA table_info(files)")
        columns = [col[1] for col in downloader.file_db.cursor.fetchall()]

        if 'download_status' in columns:
            # æ–°ç‰ˆæœ¬æ•°æ®åº“ï¼Œæœ‰download_statusåˆ—
            downloader.file_db.cursor.execute("""
                SELECT
                    COUNT(*) as total_files,
                    COUNT(CASE WHEN download_status = 'completed' THEN 1 END) as downloaded,
                    COUNT(CASE WHEN download_status = 'pending' THEN 1 END) as pending,
                    COUNT(CASE WHEN download_status = 'failed' THEN 1 END) as failed
                FROM files
            """)
            download_stats = downloader.file_db.cursor.fetchone()
        else:
            # æ—§ç‰ˆæœ¬æ•°æ®åº“ï¼Œæ²¡æœ‰download_statusåˆ—ï¼Œåªç»Ÿè®¡æ€»æ•°
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
            total_files = downloader.file_db.cursor.fetchone()[0]
            download_stats = (total_files, 0, 0, 0)  # æ€»æ•°, å·²ä¸‹è½½, å¾…ä¸‹è½½, å¤±è´¥

        result = {
            "database_stats": stats,
            "download_stats": {
                "total_files": download_stats[0] if download_stats else 0,
                "downloaded": download_stats[1] if download_stats else 0,
                "pending": download_stats[2] if download_stats else 0,
                "failed": download_stats[3] if download_stats else 0
            }
        }

        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {str(e)}")
    finally:
        # ç¡®ä¿å…³é—­æ•°æ®åº“è¿æ¥
        if crawler:
            try:
                if hasattr(crawler, 'file_downloader') and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, 'file_db') and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                if hasattr(crawler, 'db') and crawler.db:
                    crawler.db.close()
                print(f"ğŸ”’ å·²å…³é—­ç¾¤ç»„ {group_id} çš„æ•°æ®åº“è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

@app.post("/api/files/clear/{group_id}")
async def clear_file_database(group_id: str):
    """åˆ é™¤æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶æ•°æ®åº“æ–‡ä»¶"""
    try:
        path_manager = get_db_path_manager()
        db_path = path_manager.get_files_db_path(group_id)

        print(f"ğŸ—‘ï¸ å°è¯•åˆ é™¤æ–‡ä»¶æ•°æ®åº“: {db_path}")

        if os.path.exists(db_path):
            # å¼ºåˆ¶å…³é—­æ‰€æœ‰å¯èƒ½çš„æ•°æ®åº“è¿æ¥
            import gc
            import sqlite3

            # å°è¯•å¤šç§æ–¹å¼å…³é—­è¿æ¥
            try:
                # æ–¹å¼1ï¼šé€šè¿‡çˆ¬è™«å®ä¾‹å…³é—­
                crawler = get_crawler_for_group(group_id)
                downloader = crawler.get_file_downloader()
                if hasattr(downloader, 'file_db') and downloader.file_db:
                    downloader.file_db.close()
                if hasattr(crawler, 'db') and crawler.db:
                    crawler.db.close()
                print(f"âœ… å·²å…³é—­çˆ¬è™«å®ä¾‹çš„æ•°æ®åº“è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ å…³é—­çˆ¬è™«æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

            # æ–¹å¼2ï¼šå¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # æ–¹å¼3ï¼šç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿æ¥é‡Šæ”¾
            import time
            time.sleep(0.5)

            # åˆ é™¤æ•°æ®åº“æ–‡ä»¶
            try:
                os.remove(db_path)
                print(f"âœ… æ–‡ä»¶æ•°æ®åº“å·²åˆ é™¤: {db_path}")

                # åŒæ—¶åˆ é™¤è¯¥ç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜
                try:
                    from image_cache_manager import get_image_cache_manager, clear_group_cache_manager
                    cache_manager = get_image_cache_manager(group_id)
                    success, message = cache_manager.clear_cache()
                    if success:
                        print(f"âœ… å›¾ç‰‡ç¼“å­˜å·²æ¸…ç©º: {message}")
                    else:
                        print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜å¤±è´¥: {message}")
                    # æ¸…é™¤ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
                    clear_group_cache_manager(group_id)
                except Exception as cache_error:
                    print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜æ—¶å‡ºé”™: {cache_error}")

                return {"message": f"ç¾¤ç»„ {group_id} çš„æ–‡ä»¶æ•°æ®åº“å’Œå›¾ç‰‡ç¼“å­˜å·²åˆ é™¤"}
            except PermissionError as pe:
                print(f"âŒ æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤æ•°æ®åº“æ–‡ä»¶ã€‚è¯·ç¨åé‡è¯•ã€‚")
        else:
            print(f"â„¹ï¸ æ–‡ä»¶æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
            return {"message": f"ç¾¤ç»„ {group_id} çš„æ–‡ä»¶æ•°æ®åº“ä¸å­˜åœ¨"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {str(e)}")

@app.post("/api/topics/clear/{group_id}")
async def clear_topic_database(group_id: str):
    """åˆ é™¤æŒ‡å®šç¾¤ç»„çš„è¯é¢˜æ•°æ®åº“æ–‡ä»¶"""
    try:
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        print(f"ğŸ—‘ï¸ å°è¯•åˆ é™¤è¯é¢˜æ•°æ®åº“: {db_path}")

        if os.path.exists(db_path):
            # å¼ºåˆ¶å…³é—­æ‰€æœ‰å¯èƒ½çš„æ•°æ®åº“è¿æ¥
            import gc
            import time

            # å°è¯•å¤šç§æ–¹å¼å…³é—­è¿æ¥
            try:
                # æ–¹å¼1ï¼šé€šè¿‡çˆ¬è™«å®ä¾‹å…³é—­
                crawler = get_crawler_for_group(group_id)
                if hasattr(crawler, 'db') and crawler.db:
                    crawler.db.close()
                if hasattr(crawler, 'file_downloader') and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, 'file_db') and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                print(f"âœ… å·²å…³é—­çˆ¬è™«å®ä¾‹çš„æ•°æ®åº“è¿æ¥")
            except Exception as e:
                print(f"âš ï¸ å…³é—­çˆ¬è™«æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

            # æ–¹å¼2ï¼šå¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

            # æ–¹å¼3ï¼šç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿æ¥é‡Šæ”¾
            time.sleep(0.5)

            # åˆ é™¤æ•°æ®åº“æ–‡ä»¶
            try:
                os.remove(db_path)
                print(f"âœ… è¯é¢˜æ•°æ®åº“å·²åˆ é™¤: {db_path}")

                # åŒæ—¶åˆ é™¤è¯¥ç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜
                try:
                    from image_cache_manager import get_image_cache_manager, clear_group_cache_manager
                    cache_manager = get_image_cache_manager(group_id)
                    success, message = cache_manager.clear_cache()
                    if success:
                        print(f"âœ… å›¾ç‰‡ç¼“å­˜å·²æ¸…ç©º: {message}")
                    else:
                        print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜å¤±è´¥: {message}")
                    # æ¸…é™¤ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
                    clear_group_cache_manager(group_id)
                except Exception as cache_error:
                    print(f"âš ï¸ æ¸…ç©ºå›¾ç‰‡ç¼“å­˜æ—¶å‡ºé”™: {cache_error}")

                return {"message": f"ç¾¤ç»„ {group_id} çš„è¯é¢˜æ•°æ®åº“å’Œå›¾ç‰‡ç¼“å­˜å·²åˆ é™¤"}
            except PermissionError as pe:
                print(f"âŒ æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
                raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤æ•°æ®åº“æ–‡ä»¶ã€‚è¯·ç¨åé‡è¯•ã€‚")
        else:
            print(f"â„¹ï¸ è¯é¢˜æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
            return {"message": f"ç¾¤ç»„ {group_id} çš„è¯é¢˜æ•°æ®åº“ä¸å­˜åœ¨"}
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {str(e)}")

# æ•°æ®æŸ¥è¯¢APIè·¯ç”±
@app.get("/api/topics")
async def get_topics(page: int = 1, per_page: int = 20, search: Optional[str] = None):
    """è·å–è¯é¢˜åˆ—è¡¨"""
    try:
        crawler = get_crawler()

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL
        if search:
            query = """
                SELECT topic_id, title, create_time, likes_count, comments_count, reading_count
                FROM topics
                WHERE title LIKE ?
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (f"%{search}%", per_page, offset)
        else:
            query = """
                SELECT topic_id, title, create_time, likes_count, comments_count, reading_count
                FROM topics
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (per_page, offset)

        crawler.db.cursor.execute(query, params)
        topics = crawler.db.cursor.fetchall()

        # è·å–æ€»æ•°
        if search:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics WHERE title LIKE ?", (f"%{search}%",))
        else:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics")
        total = crawler.db.cursor.fetchone()[0]

        return {
            "topics": [
                {
                    "topic_id": topic[0],
                    "title": topic[1],
                    "create_time": topic[2],
                    "likes_count": topic[3],
                    "comments_count": topic[4],
                    "reading_count": topic[5]
                }
                for topic in topics
            ],
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è¯é¢˜åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/files/{group_id}")
async def get_files(group_id: str, page: int = 1, per_page: int = 20, status: Optional[str] = None):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ–‡ä»¶åˆ—è¡¨"""
    try:
        crawler = get_crawler_for_group(group_id)
        downloader = crawler.get_file_downloader()

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL
        if status:
            query = """
                SELECT file_id, name, size, download_count, create_time, download_status
                FROM files
                WHERE download_status = ?
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (status, per_page, offset)
        else:
            query = """
                SELECT file_id, name, size, download_count, create_time, download_status
                FROM files
                ORDER BY create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (per_page, offset)

        downloader.file_db.cursor.execute(query, params)
        files = downloader.file_db.cursor.fetchall()

        # è·å–æ€»æ•°
        if status:
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files WHERE download_status = ?", (status,))
        else:
            downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
        total = downloader.file_db.cursor.fetchone()[0]

        return {
            "files": [
                {
                    "file_id": file[0],
                    "name": file[1],
                    "size": file[2],
                    "download_count": file[3],
                    "create_time": file[4],
                    "download_status": file[5] if len(file) > 5 else "unknown"
                }
                for file in files
            ],
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

# ç¾¤ç»„ç›¸å…³APIç«¯ç‚¹
@app.post("/api/local-groups/refresh")
async def refresh_local_groups():
    """
    æ‰‹åŠ¨åˆ·æ–°æœ¬åœ°ç¾¤ï¼ˆoutputï¼‰æ‰«æç¼“å­˜ï¼›ä¸æŠ›é”™ï¼Œå¼‚å¸¸æ—¶è¿”å›æ—§ç¼“å­˜ã€‚
    """
    try:
        ids = await asyncio.to_thread(scan_local_groups)
        return {"success": True, "count": len(ids), "groups": sorted(list(ids))}
    except Exception as e:
        cached = get_cached_local_group_ids(force_refresh=False) or set()
        # ä¸æŠ¥é”™ï¼Œè¿”å›é™çº§ç»“æœ
        return {"success": False, "count": len(cached), "groups": sorted(list(cached)), "error": str(e)}

def _persist_group_meta_local(group_id: int, info: Dict[str, Any]):
    """
    å°†ç¾¤ç»„çš„å°é¢ã€åç§°ã€ç¾¤ä¸»ä¸æ—¶é—´ç­‰å…ƒä¿¡æ¯æŒä¹…åŒ–åˆ°æœ¬åœ°ç›®å½•ã€‚
    è¿™æ ·å³ä½¿åç»­è´¦å· Cookie å¤±æ•ˆï¼Œä»…ä¿ç•™æœ¬åœ°æ•°æ®æ—¶ï¼Œä¹Ÿèƒ½å±•ç¤ºå®Œæ•´ä¿¡æ¯ã€‚
    """
    try:
        from pathlib import Path

        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_data_dir(str(group_id))
        meta_path = Path(group_dir) / "group_meta.json"

        meta = {
            "group_id": group_id,
            "name": info.get("name") or f"æœ¬åœ°ç¾¤ï¼ˆ{group_id}ï¼‰",
            "type": info.get("type", ""),
            "background_url": info.get("background_url", ""),
            "owner": info.get("owner", {}) or {},
            "statistics": info.get("statistics", {}) or {},
            "create_time": info.get("create_time"),
            "subscription_time": info.get("subscription_time"),
            "expiry_time": info.get("expiry_time"),
            "join_time": info.get("join_time"),
            "last_active_time": info.get("last_active_time"),
            "description": info.get("description", ""),
            "is_trial": info.get("is_trial", False),
            "trial_end_time": info.get("trial_end_time"),
            "membership_end_time": info.get("membership_end_time"),
        }

        with meta_path.open("w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ å†™å…¥æœ¬åœ°ç¾¤ç»„å…ƒæ•°æ®å¤±è´¥: {e}")


@app.get("/api/groups")
async def get_groups():
    """è·å–ç¾¤ç»„åˆ—è¡¨ï¼šè´¦å·ç¾¤ âˆª æœ¬åœ°ç›®å½•ç¾¤ï¼ˆå»é‡åˆå¹¶ï¼‰"""
    try:
        # è‡ªåŠ¨æ„å»ºç¾¤ç»„â†’è´¦å·æ˜ å°„ï¼ˆå¤šè´¦å·æ”¯æŒï¼‰
        group_account_map = build_account_group_detection()
        local_ids = get_cached_local_group_ids(force_refresh=False)

        # è·å–â€œå½“å‰è´¦å·â€çš„ç¾¤åˆ—è¡¨ï¼ˆä¼˜å…ˆè´¦å·é»˜è®¤è´¦å·ï¼Œå…¶æ¬¡config.tomlï¼›è‹¥æœªé…ç½®åˆ™è§†ä¸ºç©ºé›†åˆï¼‰
        groups_data: List[dict] = []
        try:
            primary_cookie = get_primary_cookie()
            if primary_cookie:
                groups_data = fetch_groups_from_api(primary_cookie)
        except Exception as e:
            # ä¸é˜»æ–­ï¼Œè®°å½•å‘Šè­¦
            print(f"âš ï¸ è·å–è´¦å·ç¾¤å¤±è´¥ï¼Œé™çº§ä¸ºæœ¬åœ°é›†åˆ: {e}")
            groups_data = []

        # ç»„è£…è´¦å·ä¾§ç¾¤ä¸ºå­—å…¸ï¼ˆid -> infoï¼‰
        by_id: Dict[int, dict] = {}

        for group in groups_data or []:
            # æå–ç”¨æˆ·ç‰¹å®šä¿¡æ¯
            user_specific = group.get('user_specific', {}) or {}
            validity = user_specific.get('validity', {}) or {}
            trial = user_specific.get('trial', {}) or {}

            # è¿‡æœŸä¿¡æ¯ä¸çŠ¶æ€
            actual_expiry_time = trial.get('end_time') or validity.get('end_time')
            is_trial = bool(trial.get('end_time'))

            status = None
            if actual_expiry_time:
                from datetime import datetime, timezone
                try:
                    end_time = datetime.fromisoformat(actual_expiry_time.replace('Z', '+00:00'))
                    now = datetime.now(timezone.utc)
                    days_until_expiry = (end_time - now).days
                    if days_until_expiry < 0:
                        status = 'expired'
                    elif days_until_expiry <= 7:
                        status = 'expiring_soon'
                    else:
                        status = 'active'
                except Exception:
                    pass

            gid = group.get('group_id')
            try:
                gid = int(gid)
            except Exception:
                continue

            info = {
                "group_id": gid,
                "name": group.get('name', ''),
                "type": group.get('type', ''),
                "background_url": group.get('background_url', ''),
                "owner": group.get('owner', {}) or {},
                "statistics": group.get('statistics', {}) or {},
                "status": status,
                "create_time": group.get('create_time'),
                "subscription_time": validity.get('begin_time'),
                "expiry_time": actual_expiry_time,
                "join_time": user_specific.get('join_time'),
                "last_active_time": user_specific.get('last_active_time'),
                "description": group.get('description', ''),
                "is_trial": is_trial,
                "trial_end_time": trial.get('end_time'),
                "membership_end_time": validity.get('end_time'),
                "account": group_account_map.get(str(gid)),
                "source": "account"
            }
            by_id[gid] = info

        # åˆå¹¶æœ¬åœ°ç›®å½•ç¾¤
        for gid in local_ids or []:
            try:
                gid_int = int(gid)
            except Exception:
                continue
            if gid_int in by_id:
                # æ ‡æ³¨æ¥æºä¸º account|localï¼Œå¹¶æŒä¹…åŒ–ä¸€ä»½å…ƒä¿¡æ¯åˆ°æœ¬åœ°
                src = by_id[gid_int].get("source", "account")
                if "local" not in src:
                    by_id[gid_int]["source"] = "account|local"
                _persist_group_meta_local(gid_int, by_id[gid_int])
            else:
                # ä»…å­˜åœ¨äºæœ¬åœ°ï¼šä¼˜å…ˆä» group_meta.json è¯»å–å…ƒä¿¡æ¯ï¼Œå…¶æ¬¡ä»æœ¬åœ°æ•°æ®åº“è¡¥å…¨
                local_name = f"æœ¬åœ°ç¾¤ï¼ˆ{gid_int}ï¼‰"
                local_type = "local"
                local_bg = ""
                owner: Dict[str, Any] = {}
                join_time = None
                expiry_time = None
                last_active_time = None
                description = ""
                statistics: Dict[str, Any] = {}

                # 1. ä¼˜å…ˆè¯»å–æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœä¹‹å‰æœ‰è´¦å·+æœ¬åœ°æ—¶å·²ç»è½ç›˜ï¼‰
                try:
                    from pathlib import Path

                    path_manager = get_db_path_manager()
                    group_dir = path_manager.get_group_data_dir(str(gid_int))
                    meta_path = Path(group_dir) / "group_meta.json"
                    if meta_path.exists():
                        with meta_path.open("r", encoding="utf-8") as f:
                            meta = json.load(f)
                        local_name = meta.get("name", local_name)
                        local_type = meta.get("type", local_type)
                        local_bg = meta.get("background_url", local_bg)
                        owner = meta.get("owner", {}) or owner
                        statistics = meta.get("statistics", {}) or statistics
                        join_time = meta.get("join_time", join_time)
                        expiry_time = meta.get("expiry_time", expiry_time)
                        last_active_time = meta.get("last_active_time", last_active_time)
                        description = meta.get("description", description)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æœ¬åœ°ç¾¤ç»„ {gid_int} å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")

                # 2. è‹¥å…ƒæ•°æ®æ–‡ä»¶ä¸­ä»ç¼ºå°‘ä¿¡æ¯ï¼Œå†ä»æœ¬åœ°æ•°æ®åº“è¡¥å……
                try:
                    path_manager = get_db_path_manager()
                    db_paths = path_manager.list_group_databases(str(gid_int))
                    topics_db = db_paths.get("topics")
                    if topics_db and os.path.exists(topics_db):
                        db = ZSXQDatabase(topics_db)
                        try:
                            cur = db.cursor
                            # ç¾¤ç»„åŸºç¡€ä¿¡æ¯
                            if not local_bg or local_name.startswith("æœ¬åœ°ç¾¤ï¼ˆ"):
                                cur.execute(
                                    "SELECT name, type, background_url FROM groups WHERE group_id = ? LIMIT 1",
                                    (gid_int,),
                                )
                                row = cur.fetchone()
                                if row:
                                    if row[0]:
                                        local_name = row[0]
                                    if row[1]:
                                        local_type = row[1]
                                    if row[2]:
                                        local_bg = row[2]

                            # æœ¬åœ°æ•°æ®æ—¶é—´èŒƒå›´ï¼ˆä»¥è¯é¢˜æ—¶é—´æ›¿ä»£â€œåŠ å…¥/è¿‡æœŸæ—¶é—´â€çš„è¿‘ä¼¼ï¼‰
                            if not join_time or not expiry_time:
                                cur.execute(
                                    """
                                    SELECT MIN(create_time), MAX(create_time)
                                    FROM topics
                                    WHERE group_id = ? AND create_time IS NOT NULL AND create_time != ''
                                    """,
                                    (gid_int,),
                                )
                                trow = cur.fetchone()
                                if trow:
                                    if not join_time:
                                        join_time = trow[0]
                                    if not expiry_time:
                                        expiry_time = trow[1]
                                    if not last_active_time:
                                        last_active_time = trow[1]

                            # ç®€å•ç»Ÿè®¡ï¼šè¯é¢˜æ•°é‡
                            if not statistics:
                                cur.execute(
                                    "SELECT COUNT(*) FROM topics WHERE group_id = ?",
                                    (gid_int,),
                                )
                                topics_count = cur.fetchone()[0] or 0
                                statistics = {
                                    "topics": {
                                        "topics_count": topics_count,
                                        "answers_count": 0,
                                        "digests_count": 0,
                                    }
                                }
                        finally:
                            db.close()
                except Exception as e:
                    # å‡ºé”™æ—¶é™çº§ä¸ºå ä½ä¿¡æ¯ï¼Œä¸ä¸­æ–­æ•´ä¸ªæ¥å£
                    print(f"âš ï¸ è¯»å–æœ¬åœ°ç¾¤ç»„ {gid_int} å…ƒæ•°æ®å¤±è´¥: {e}")

                by_id[gid_int] = {
                    "group_id": gid_int,
                    "name": local_name,
                    "type": local_type,
                    "background_url": local_bg,
                    "owner": owner,
                    "statistics": statistics,
                    "status": None,
                    "create_time": join_time,
                    "subscription_time": None,
                    "expiry_time": expiry_time,
                    "join_time": join_time,
                    "last_active_time": last_active_time,
                    "description": description,
                    "is_trial": False,
                    "trial_end_time": None,
                    "membership_end_time": None,
                    "account": None,
                    "source": "local",
                }

        # æ’åºï¼šæŒ‰ç¾¤IDå‡åºï¼›å¦‚éœ€äºŒçº§æ’åºå†æŒ‰æ¥æºï¼ˆè´¦å·ä¼˜å…ˆï¼‰
        merged = [by_id[k] for k in sorted(by_id.keys())]

        return {
            "groups": merged,
            "total": len(merged)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/topics/{topic_id}/{group_id}")
async def get_topic_detail(topic_id: int, group_id: str):
    """è·å–è¯é¢˜è¯¦æƒ…ï¼ˆä»…ä»æœ¬åœ°æ•°æ®åº“è¯»å–ï¼Œä¸ä¸»åŠ¨çˆ¬å–ï¼‰

    æ³¨æ„ï¼š
    - å¦‚æœæœ¬åœ° topics è¡¨ä¸­ä¸å­˜åœ¨è¯¥ topic_idï¼Œä¼šè¿”å› 404ï¼›
    - ä¸ä¼šè°ƒç”¨çŸ¥è¯†æ˜Ÿçƒå®˜æ–¹ API æ‹‰å–æœ€æ–°æ•°æ®ï¼Œå¦‚éœ€è¡¥é‡‡è¯·è°ƒç”¨
      POST /api/topics/fetch-single/{group_id}/{topic_id}ã€‚
    """
    try:
        crawler = get_crawler_for_group(group_id)
        topic_detail = crawler.db.get_topic_detail(topic_id)

        if not topic_detail:
            # ä¸šåŠ¡ä¸Šè¿™æ˜¯ä¸€ä¸ªâ€œæ­£å¸¸â€çš„ä¸å­˜åœ¨åœºæ™¯ï¼Œç›´æ¥å‘å¤–æŠ› 404ï¼Œ
            # é¿å…è¢«ä¸‹é¢çš„é€šç”¨å¼‚å¸¸åŒ…è£…æˆ 500ã€‚
            raise HTTPException(status_code=404, detail="è¯é¢˜ä¸å­˜åœ¨")

        return topic_detail
    except HTTPException:
        # ä¿ç•™åŸæœ‰çš„çŠ¶æ€ç ï¼ˆä¾‹å¦‚ä¸Šé¢çš„ 404ï¼‰
        raise
    except Exception as e:
        # åªæœ‰çœŸæ­£çš„é HTTPException å¼‚å¸¸æ‰åŒ…è£…ä¸º 500
        raise HTTPException(status_code=500, detail=f"è·å–è¯é¢˜è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.post("/api/topics/{topic_id}/{group_id}/refresh")
async def refresh_topic(topic_id: int, group_id: str):
    """å®æ—¶æ›´æ–°å•ä¸ªè¯é¢˜ä¿¡æ¯"""
    try:
        crawler = get_crawler_for_group(group_id)

        # ä½¿ç”¨çŸ¥è¯†æ˜ŸçƒAPIè·å–æœ€æ–°è¯é¢˜ä¿¡æ¯
        url = f"https://api.zsxq.com/v2/topics/{topic_id}/info"
        headers = crawler.get_stealth_headers()

        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code == 200:
            data = response.json()
            if data.get('succeeded') and data.get('resp_data'):
                topic_data = data['resp_data']['topic']

                # åªæ›´æ–°è¯é¢˜çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…åˆ›å»ºé‡å¤è®°å½•
                success = crawler.db.update_topic_stats(topic_data)

                if not success:
                    return {"success": False, "message": "è¯é¢˜ä¸å­˜åœ¨æˆ–æ›´æ–°å¤±è´¥"}

                crawler.db.conn.commit()

                return {
                    "success": True,
                    "message": "è¯é¢˜ä¿¡æ¯å·²æ›´æ–°",
                    "updated_data": {
                        "likes_count": topic_data.get('likes_count', 0),
                        "comments_count": topic_data.get('comments_count', 0),
                        "reading_count": topic_data.get('reading_count', 0),
                        "readers_count": topic_data.get('readers_count', 0)
                    }
                }
            else:
                return {"success": False, "message": "APIè¿”å›æ•°æ®æ ¼å¼é”™è¯¯"}
        else:
            return {"success": False, "message": f"APIè¯·æ±‚å¤±è´¥: {response.status_code}"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°è¯é¢˜å¤±è´¥: {str(e)}")

@app.post("/api/topics/{topic_id}/{group_id}/fetch-comments")
async def fetch_more_comments(topic_id: int, group_id: str):
    """æ‰‹åŠ¨è·å–è¯é¢˜çš„æ›´å¤šè¯„è®ºï¼ˆåœ¨å·²å­˜åœ¨æœ¬åœ°è¯é¢˜è®°å½•çš„å‰æä¸‹ï¼‰"""
    try:
        crawler = get_crawler_for_group(group_id)

        # å…ˆè·å–è¯é¢˜åŸºæœ¬ä¿¡æ¯ï¼ˆä»…æŸ¥æœ¬åœ°ï¼‰
        topic_detail = crawler.db.get_topic_detail(topic_id)
        if not topic_detail:
            # åŒæ ·è¿™å±äºä¸šåŠ¡å±‚é¢çš„â€œè¯é¢˜æœªé‡‡é›†â€ï¼Œç›´æ¥è¿”å› 404
            raise HTTPException(status_code=404, detail="è¯é¢˜ä¸å­˜åœ¨")

        comments_count = topic_detail.get('comments_count', 0)
        if comments_count <= 8:
            return {
                "success": True,
                "message": f"è¯é¢˜åªæœ‰ {comments_count} æ¡è¯„è®ºï¼Œæ— éœ€è·å–æ›´å¤š",
                "comments_fetched": 0
            }

        # è·å–æ›´å¤šè¯„è®º
        try:
            additional_comments = crawler.fetch_all_comments(topic_id, comments_count)
            if additional_comments:
                crawler.db.import_additional_comments(topic_id, additional_comments)
                crawler.db.conn.commit()

                return {
                    "success": True,
                    "message": f"æˆåŠŸè·å–å¹¶å¯¼å…¥ {len(additional_comments)} æ¡è¯„è®º",
                    "comments_fetched": len(additional_comments)
                }
            else:
                return {
                    "success": False,
                    "message": "è·å–è¯„è®ºå¤±è´¥ï¼Œå¯èƒ½æ˜¯æƒé™é™åˆ¶æˆ–ç½‘ç»œé—®é¢˜",
                    "comments_fetched": 0
                }
        except Exception as e:
            return {
                "success": False,
                "message": f"è·å–è¯„è®ºæ—¶å‡ºé”™: {str(e)}",
                "comments_fetched": 0
            }

    except HTTPException:
        # ä¿ç•™æ˜¾å¼æŠ›å‡ºçš„ä¸šåŠ¡é”™è¯¯ï¼ˆä¾‹å¦‚ 404ï¼‰
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ›´å¤šè¯„è®ºå¤±è´¥: {str(e)}")

@app.delete("/api/topics/{topic_id}/{group_id}")
async def delete_single_topic(topic_id: int, group_id: int):
    """åˆ é™¤å•ä¸ªè¯é¢˜åŠå…¶æ‰€æœ‰å…³è”æ•°æ®"""
    crawler = None
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹ï¼Œä»¥ä¾¿å¤ç”¨å…¶æ•°æ®åº“è¿æ¥
        crawler = get_crawler_for_group(str(group_id))

        # æ£€æŸ¥è¯é¢˜æ˜¯å¦å­˜åœ¨ä¸”å±äºè¯¥ç¾¤ç»„
        crawler.db.cursor.execute('SELECT COUNT(*) FROM topics WHERE topic_id = ? AND group_id = ?', (topic_id, group_id))
        exists = crawler.db.cursor.fetchone()[0] > 0
        if not exists:
            return {"success": False, "message": "è¯é¢˜ä¸å­˜åœ¨"}

        # ä¾èµ–é¡ºåºåˆ é™¤å…³è”æ•°æ®
        tables_to_clean = [
            'user_liked_emojis',
            'like_emojis',
            'likes',
            'images',
            'comments',
            'answers',
            'questions',
            'articles',
            'talks',
            'topic_files',
            'topic_tags'
        ]

        for table in tables_to_clean:
            crawler.db.cursor.execute(f'DELETE FROM {table} WHERE topic_id = ?', (topic_id,))

        # æœ€ååˆ é™¤è¯é¢˜æœ¬èº«ï¼ˆé™å®šç¾¤ç»„ï¼‰
        crawler.db.cursor.execute('DELETE FROM topics WHERE topic_id = ? AND group_id = ?', (topic_id, group_id))

        deleted = crawler.db.cursor.rowcount
        crawler.db.conn.commit()

        return {"success": True, "deleted_topic_id": topic_id, "deleted": deleted > 0}
    except Exception as e:
        try:
            if crawler and hasattr(crawler, 'db') and crawler.db:
                crawler.db.conn.rollback()
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¯é¢˜å¤±è´¥: {str(e)}")

# å•ä¸ªè¯é¢˜é‡‡é›† API
@app.post("/api/topics/fetch-single/{group_id}/{topic_id}")
async def fetch_single_topic(group_id: str, topic_id: int, fetch_comments: bool = True):
    """çˆ¬å–å¹¶å¯¼å…¥å•ä¸ªè¯é¢˜ï¼ˆç”¨äºç‰¹æ®Šè¯é¢˜æµ‹è¯•ï¼‰ï¼Œå¯é€‰æ‹‰å–å®Œæ•´è¯„è®º"""
    try:
        # ä½¿ç”¨è¯¥ç¾¤çš„è‡ªåŠ¨åŒ¹é…è´¦å·
        crawler = get_crawler_for_group(str(group_id))

        # æ‹‰å–è¯é¢˜è¯¦ç»†ä¿¡æ¯
        url = f"https://api.zsxq.com/v2/topics/{topic_id}/info"
        headers = crawler.get_stealth_headers()
        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail="APIè¯·æ±‚å¤±è´¥")

        data = response.json()
        if not data.get("succeeded") or not data.get("resp_data"):
            raise HTTPException(status_code=400, detail="APIè¿”å›å¤±è´¥")

        topic = (data.get("resp_data", {}) or {}).get("topic", {}) or {}

        if not topic:
            raise HTTPException(status_code=404, detail="æœªè·å–åˆ°æœ‰æ•ˆè¯é¢˜æ•°æ®")

        # æ ¡éªŒè¯é¢˜æ‰€å±ç¾¤ç»„ä¸€è‡´æ€§
        topic_group_id = str((topic.get("group") or {}).get("group_id", ""))
        if topic_group_id and topic_group_id != str(group_id):
            raise HTTPException(status_code=400, detail="è¯¥è¯é¢˜ä¸å±äºå½“å‰ç¾¤ç»„")

        # åˆ¤æ–­è¯é¢˜æ˜¯å¦å·²å­˜åœ¨
        crawler.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
        existed = crawler.db.cursor.fetchone() is not None

        # å¯¼å…¥è¯é¢˜å®Œæ•´æ•°æ®
        crawler.db.import_topic_data(topic)
        crawler.db.conn.commit()

        # å¯é€‰ï¼šè·å–å®Œæ•´è¯„è®º
        comments_fetched = 0
        if fetch_comments:
            comments_count = topic.get("comments_count", 0) or 0
            if comments_count > 0:
                try:
                    additional_comments = crawler.fetch_all_comments(topic_id, comments_count)
                    if additional_comments:
                        crawler.db.import_additional_comments(topic_id, additional_comments)
                        crawler.db.conn.commit()
                        comments_fetched = len(additional_comments)
                except Exception as e:
                    # ä¸é˜»å¡ä¸»æµç¨‹
                    print(f"âš ï¸ å•è¯é¢˜è¯„è®ºè·å–å¤±è´¥: {e}")

        return {
            "success": True,
            "topic_id": topic_id,
            "group_id": int(group_id),
            "imported": "updated" if existed else "created",
            "comments_fetched": comments_fetched
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å•ä¸ªè¯é¢˜é‡‡é›†å¤±è´¥: {str(e)}")

# æ ‡ç­¾ç›¸å…³APIç«¯ç‚¹
@app.get("/api/groups/{group_id}/tags")
async def get_group_tags(group_id: str):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰æ ‡ç­¾"""
    try:
        crawler = get_crawler_for_group(group_id)
        tags = crawler.db.get_tags_by_group(int(group_id))
        
        return {
            "tags": tags,
            "total": len(tags)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ ‡ç­¾åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/tags/{tag_id}/topics")
async def get_topics_by_tag(group_id: int, tag_id: int, page: int = 1, per_page: int = 20):
    """æ ¹æ®æ ‡ç­¾è·å–æŒ‡å®šç¾¤ç»„çš„è¯é¢˜åˆ—è¡¨"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦å­˜åœ¨äºè¯¥ç¾¤ç»„ä¸­
        crawler.db.cursor.execute('SELECT COUNT(*) FROM tags WHERE tag_id = ? AND group_id = ?', (tag_id, group_id))
        tag_count = crawler.db.cursor.fetchone()[0]
        
        if tag_count == 0:
            raise HTTPException(status_code=404, detail="æ ‡ç­¾åœ¨è¯¥ç¾¤ç»„ä¸­ä¸å­˜åœ¨")
            
        result = crawler.db.get_topics_by_tag(tag_id, page, per_page)
        
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ ¹æ®æ ‡ç­¾è·å–è¯é¢˜å¤±è´¥: {str(e)}")

@app.get("/api/proxy-image")
async def proxy_image(url: str, group_id: str = None):
    """ä»£ç†å›¾ç‰‡è¯·æ±‚ï¼Œæ”¯æŒæœ¬åœ°ç¼“å­˜"""
    try:
        cache_manager = get_image_cache_manager(group_id)

        # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
        if cache_manager.is_cached(url):
            cached_path = cache_manager.get_cached_path(url)
            if cached_path and cached_path.exists():
                # è¿”å›ç¼“å­˜çš„å›¾ç‰‡
                content_type = mimetypes.guess_type(str(cached_path))[0] or 'image/jpeg'

                with open(cached_path, 'rb') as f:
                    content = f.read()

                return Response(
                    content=content,
                    media_type=content_type,
                    headers={
                        'Cache-Control': 'public, max-age=86400',  # ç¼“å­˜24å°æ—¶
                        'Access-Control-Allow-Origin': '*',
                        'X-Cache-Status': 'HIT'
                    }
                )

        # ä¸‹è½½å¹¶ç¼“å­˜å›¾ç‰‡
        success, cached_path, error = cache_manager.download_and_cache(url)

        if success and cached_path and cached_path.exists():
            content_type = mimetypes.guess_type(str(cached_path))[0] or 'image/jpeg'

            with open(cached_path, 'rb') as f:
                content = f.read()

            return Response(
                content=content,
                media_type=content_type,
                headers={
                    'Cache-Control': 'public, max-age=86400',
                    'Access-Control-Allow-Origin': '*',
                    'X-Cache-Status': 'MISS'
                }
            )
        else:
            raise HTTPException(status_code=404, detail=f"å›¾ç‰‡åŠ è½½å¤±è´¥: {error}")

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»£ç†å›¾ç‰‡å¤±è´¥: {str(e)}")


@app.get("/api/cache/images/info/{group_id}")
async def get_image_cache_info(group_id: str):
    """è·å–æŒ‡å®šç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        cache_manager = get_image_cache_manager(group_id)
        return cache_manager.get_cache_info()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.delete("/api/cache/images/{group_id}")
async def clear_image_cache(group_id: str):
    """æ¸…ç©ºæŒ‡å®šç¾¤ç»„çš„å›¾ç‰‡ç¼“å­˜"""
    try:
        cache_manager = get_image_cache_manager(group_id)
        success, message = cache_manager.clear_cache()

        if success:
            return {"success": True, "message": message}
        else:
            raise HTTPException(status_code=500, detail=message)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/images/{image_path:path}")
async def get_local_image(group_id: str, image_path: str):
    """è·å–ç¾¤ç»„æœ¬åœ°ç¼“å­˜çš„å›¾ç‰‡"""
    from pathlib import Path
    
    try:
        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_data_dir(group_id)
        images_dir = Path(group_dir) / "images"
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨å›¾ç‰‡ç›®å½•å†…
        image_file = (images_dir / image_path).resolve()
        if not str(image_file).startswith(str(images_dir.resolve())):
            raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®è¯¥è·¯å¾„")
        
        if not image_file.exists():
            raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")
        
        # è·å– MIME ç±»å‹
        content_type = mimetypes.guess_type(str(image_file))[0] or 'application/octet-stream'
        
        # è¯»å–å¹¶è¿”å›å›¾ç‰‡
        with open(image_file, 'rb') as f:
            content = f.read()
        
        return Response(content=content, media_type=content_type)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/videos/{video_path:path}")
async def get_local_video(group_id: str, video_path: str):
    """è·å–ç¾¤ç»„æœ¬åœ°ç¼“å­˜çš„è§†é¢‘ï¼ˆæ”¯æŒèŒƒå›´è¯·æ±‚ï¼Œç”¨äºè§†é¢‘æµæ’­æ”¾ï¼‰"""
    from pathlib import Path
    from fastapi.responses import FileResponse
    from fastapi import Request
    
    try:
        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_dir(group_id)
        videos_dir = Path(group_dir) / "column_videos"
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨è§†é¢‘ç›®å½•å†…
        video_file = (videos_dir / video_path).resolve()
        if not str(video_file).startswith(str(videos_dir.resolve())):
            raise HTTPException(status_code=403, detail="ç¦æ­¢è®¿é—®è¯¥è·¯å¾„")
        
        if not video_file.exists():
            raise HTTPException(status_code=404, detail="è§†é¢‘ä¸å­˜åœ¨")
        
        # è·å– MIME ç±»å‹
        content_type = mimetypes.guess_type(str(video_file))[0] or 'video/mp4'
        
        # ä½¿ç”¨ FileResponse æ”¯æŒèŒƒå›´è¯·æ±‚ï¼ˆè§†é¢‘æ‹–åŠ¨è¿›åº¦æ¡ï¼‰
        return FileResponse(
            path=str(video_file),
            media_type=content_type,
            filename=video_file.name
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–è§†é¢‘å¤±è´¥: {str(e)}")


@app.get("/api/settings/crawl")
async def get_crawl_settings():
    """è·å–è¯é¢˜çˆ¬å–è®¾ç½®"""
    try:
        # è¿”å›é»˜è®¤è®¾ç½®
        return {
            "crawl_interval_min": 2.0,
            "crawl_interval_max": 5.0,
            "long_sleep_interval_min": 180.0,
            "long_sleep_interval_max": 300.0,
            "pages_per_batch": 15
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çˆ¬å–è®¾ç½®å¤±è´¥: {str(e)}")


@app.post("/api/settings/crawl")
async def update_crawl_settings(settings: dict):
    """æ›´æ–°è¯é¢˜çˆ¬å–è®¾ç½®"""
    try:
        # è¿™é‡Œå¯ä»¥å°†è®¾ç½®ä¿å­˜åˆ°é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“
        # ç›®å‰åªæ˜¯è¿”å›æˆåŠŸï¼Œå®é™…è®¾ç½®é€šè¿‡APIå‚æ•°ä¼ é€’
        return {"success": True, "message": "çˆ¬å–è®¾ç½®å·²æ›´æ–°"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°çˆ¬å–è®¾ç½®å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/columns/summary")
async def get_group_columns_summary(group_id: str):
    """è·å–ç¾¤ç»„ä¸“æ æ‘˜è¦ä¿¡æ¯ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸“æ å†…å®¹"""
    try:
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)
        
        if not cookie:
            return {
                "has_columns": False,
                "title": None,
                "error": "æœªæ‰¾åˆ°å¯ç”¨Cookie"
            }
        
        headers = build_stealth_headers(cookie)
        url = f"https://api.zsxq.com/v2/groups/{group_id}/columns/summary"
        
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('succeeded'):
                resp_data = data.get('resp_data', {})
                return {
                    "has_columns": resp_data.get('has_columns', False),
                    "title": resp_data.get('title', None)
                }
            else:
                return {
                    "has_columns": False,
                    "title": None,
                    "error": data.get('error_message', 'APIè¿”å›å¤±è´¥')
                }
        else:
            return {
                "has_columns": False,
                "title": None,
                "error": f"HTTP {response.status_code}"
            }
    except requests.RequestException as e:
        return {
            "has_columns": False,
            "title": None,
            "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
        }
    except Exception as e:
        return {
            "has_columns": False,
            "title": None,
            "error": f"è·å–ä¸“æ ä¿¡æ¯å¤±è´¥: {str(e)}"
        }


@app.get("/api/groups/{group_id}/info")
async def get_group_info(group_id: str):
    """è·å–ç¾¤ç»„ä¿¡æ¯ï¼ˆå¸¦æœ¬åœ°å›é€€ï¼Œé¿å…401/500å¯¼è‡´å‰ç«¯æŠ¥é”™ï¼‰"""
    try:
        # è‡ªåŠ¨åŒ¹é…è¯¥ç¾¤ç»„æ‰€å±è´¦å·ï¼Œè·å–å¯¹åº”Cookie
        cookie = get_cookie_for_group(group_id)

        # æœ¬åœ°å›é€€æ•°æ®æ„é€ ï¼ˆä¸è®¿é—®å®˜æ–¹APIï¼‰
        def build_fallback(source: str = "fallback", note: str = None) -> dict:
            files_count = 0
            try:
                crawler = get_crawler_for_group(group_id)
                downloader = crawler.get_file_downloader()
                try:
                    downloader.file_db.cursor.execute("SELECT COUNT(*) FROM files")
                    row = downloader.file_db.cursor.fetchone()
                    files_count = (row[0] or 0) if row else 0
                except Exception:
                    files_count = 0
            except Exception:
                files_count = 0

            try:
                gid = int(group_id)
            except Exception:
                gid = group_id

            result = {
                "group_id": gid,
                "name": f"ç¾¤ç»„ {group_id}",
                "description": "",
                "statistics": {"files": {"count": files_count}},
                "background_url": None,
                "account": get_account_summary_for_group_auto(group_id),
                "source": source,
            }
            if note:
                result["note"] = note
            return result

        # è‹¥æ²¡æœ‰å¯ç”¨ Cookieï¼Œç›´æ¥è¿”å›æœ¬åœ°å›é€€ï¼Œé¿å…æŠ› 400/500
        if not cookie:
            return build_fallback(note="no_cookie")

        # è°ƒç”¨å®˜æ–¹æ¥å£
        url = f"https://api.zsxq.com/v2/groups/{group_id}"
        headers = {
            'Cookie': cookie,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=30)

        if response.status_code == 200:
            data = response.json()
            if data.get('succeeded'):
                group_data = data.get('resp_data', {}).get('group', {})
                return {
                    "group_id": group_data.get('group_id'),
                    "name": group_data.get('name'),
                    "description": group_data.get('description'),
                    "statistics": group_data.get('statistics', {}),
                    "background_url": group_data.get('background_url'),
                    "account": get_account_summary_for_group_auto(group_id),
                    "source": "remote"
                }
            # å®˜æ–¹è¿”å›é succeededï¼Œä¹Ÿèµ°å›é€€
            return build_fallback(note="remote_response_failed")
        else:
            # æˆæƒå¤±è´¥/æƒé™ä¸è¶³ â†’ ä½¿ç”¨æœ¬åœ°å›é€€ï¼ˆ200è¿”å›ï¼Œå‡å°‘å‰ç«¯å‘Šè­¦ï¼‰
            if response.status_code in (401, 403):
                return build_fallback(note=f"remote_api_{response.status_code}")
            # å…¶ä»–çŠ¶æ€ç ä¹Ÿå›é€€
            return build_fallback(note=f"remote_api_{response.status_code}")

    except Exception:
        # ä»»ä½•å¼‚å¸¸éƒ½å›é€€ä¸ºæœ¬åœ°ä¿¡æ¯ï¼Œé¿å… 500
        return build_fallback(note="exception_fallback")

@app.get("/api/groups/{group_id}/topics")
async def get_group_topics(group_id: int, page: int = 1, per_page: int = 20, search: Optional[str] = None):
    """è·å–æŒ‡å®šç¾¤ç»„çš„è¯é¢˜åˆ—è¡¨"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))

        # ğŸ§ª è°ƒè¯•ï¼šæ‰“å°å½“å‰ä½¿ç”¨çš„æ•°æ®åº“è·¯å¾„
        try:
            db_path = getattr(getattr(crawler, "db", None), "db_path", None)
            print(f"[DEBUG get_group_topics] group_id={group_id}, db_path={db_path}, page={page}, per_page={per_page}")
        except Exception as e:
            print(f"[DEBUG get_group_topics] failed to print db_path: {e}")

        offset = (page - 1) * per_page

        # æ„å»ºæŸ¥è¯¢SQL - åŒ…å«æ‰€æœ‰å†…å®¹ç±»å‹
        if search:
            query = """
                SELECT
                    t.topic_id, t.title, t.create_time, t.likes_count, t.comments_count,
                    t.reading_count, t.type, t.digested, t.sticky,
                    q.text as question_text,
                    a.text as answer_text,
                    tk.text as talk_text,
                    u.user_id, u.name, u.avatar_url, t.imported_at
                FROM topics t
                LEFT JOIN questions q ON t.topic_id = q.topic_id
                LEFT JOIN answers a ON t.topic_id = a.topic_id
                LEFT JOIN talks tk ON t.topic_id = tk.topic_id
                LEFT JOIN users u ON tk.owner_user_id = u.user_id
                WHERE t.group_id = ? AND (t.title LIKE ? OR q.text LIKE ? OR tk.text LIKE ?)
                ORDER BY t.create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (group_id, f"%{search}%", f"%{search}%", f"%{search}%", per_page, offset)
        else:
            query = """
                SELECT
                    t.topic_id, t.title, t.create_time, t.likes_count, t.comments_count,
                    t.reading_count, t.type, t.digested, t.sticky,
                    q.text as question_text,
                    a.text as answer_text,
                    tk.text as talk_text,
                    u.user_id, u.name, u.avatar_url, t.imported_at
                FROM topics t
                LEFT JOIN questions q ON t.topic_id = q.topic_id
                LEFT JOIN answers a ON t.topic_id = a.topic_id
                LEFT JOIN talks tk ON t.topic_id = tk.topic_id
                LEFT JOIN users u ON tk.owner_user_id = u.user_id
                WHERE t.group_id = ?
                ORDER BY t.create_time DESC
                LIMIT ? OFFSET ?
            """
            params = (group_id, per_page, offset)

        crawler.db.cursor.execute(query, params)
        topics = crawler.db.cursor.fetchall()

        # ğŸ§ª è°ƒè¯•ï¼šæ‰“å°å‰è‹¥å¹²æ¡è¯é¢˜çš„ topic_id å’Œæ ‡é¢˜
        try:
            debug_rows = topics[:10]
            debug_list = [(row[0], row[1]) for row in debug_rows]
            print(f"[DEBUG get_group_topics] first topics from DB (topic_id, title): {debug_list}")

            # ç‰¹åˆ«æ‰“å°â€œOfferé€‰æ‹©â€è¿™æ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for row in debug_rows:
                title = row[1] or ""
                if isinstance(title, str) and title.startswith("Offeré€‰æ‹©"):
                    print(f"[DEBUG get_group_topics] Offer topic row from DB: topic_id={row[0]}, title={title}")
        except Exception as e:
            print(f"[DEBUG get_group_topics] failed to debug topics: {e}")

        # è·å–æ€»æ•°
        if search:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics WHERE group_id = ? AND title LIKE ?", (group_id, f"%{search}%"))
        else:
            crawler.db.cursor.execute("SELECT COUNT(*) FROM topics WHERE group_id = ?", (group_id,))
        total = crawler.db.cursor.fetchone()[0]

        # å¤„ç†è¯é¢˜æ•°æ®
        topics_list = []
        for topic in topics:
            # æ³¨æ„ï¼štopic_id å¯èƒ½è¶…è¿‡ JavaScript çš„å®‰å…¨æ•´æ•°èŒƒå›´ï¼ˆ2^53-1ï¼‰ï¼Œ
            # å¦‚æœä»¥æ•°å­—å½¢å¼ä¼ é€’åˆ°å‰ç«¯ä¼šå‘ç”Ÿç²¾åº¦ä¸¢å¤±ï¼ˆä¾‹å¦‚ 82811852151825212 å˜æˆ 82811852151825220ï¼‰ã€‚
            # å› æ­¤è¿™é‡Œç»Ÿä¸€å°† topic_id åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œå‰ç«¯ä¹Ÿåº”æŒ‰å­—ç¬¦ä¸²å¤„ç†ã€‚
            topic_data = {
                "topic_id": str(topic[0]) if topic[0] is not None else None,
                "title": topic[1],
                "create_time": topic[2],
                "likes_count": topic[3],
                "comments_count": topic[4],
                "reading_count": topic[5],
                "type": topic[6],
                "digested": bool(topic[7]) if topic[7] is not None else False,
                "sticky": bool(topic[8]) if topic[8] is not None else False,
                "imported_at": topic[15] if len(topic) > 15 else None  # è·å–æ—¶é—´
            }

            # æ·»åŠ å†…å®¹æ–‡æœ¬
            if topic[6] == 'q&a':
                # é—®ç­”ç±»å‹è¯é¢˜
                topic_data['question_text'] = topic[9] if topic[9] else ''
                topic_data['answer_text'] = topic[10] if topic[10] else ''
            else:
                # å…¶ä»–ç±»å‹è¯é¢˜ï¼ˆtalkã€articleç­‰ï¼‰
                topic_data['talk_text'] = topic[11] if topic[11] else ''
                if topic[12]:  # æœ‰ä½œè€…ä¿¡æ¯
                    topic_data['author'] = {
                        'user_id': topic[12],
                        'name': topic[13],
                        'avatar_url': topic[14]
                    }

            topics_list.append(topic_data)

        return {
            "topics": topics_list,
            "pagination": {
                "page": page,
                "per_page": per_page,
                "total": total,
                "pages": (total + per_page - 1) // per_page
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„è¯é¢˜å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/stats")
async def get_group_stats(group_id: int):
    """è·å–æŒ‡å®šç¾¤ç»„çš„ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))
        cursor = crawler.db.cursor

        # è·å–è¯é¢˜ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM topics WHERE group_id = ?", (group_id,))
        topics_count = cursor.fetchone()[0]

        # è·å–ç”¨æˆ·ç»Ÿè®¡ - ä»talksè¡¨è·å–ï¼Œå› ä¸ºtopicsè¡¨æ²¡æœ‰user_idå­—æ®µ
        cursor.execute("""
            SELECT COUNT(DISTINCT t.owner_user_id)
            FROM talks t
            JOIN topics tp ON t.topic_id = tp.topic_id
            WHERE tp.group_id = ?
        """, (group_id,))
        users_count = cursor.fetchone()[0]

        # è·å–æœ€æ–°è¯é¢˜æ—¶é—´
        cursor.execute("SELECT MAX(create_time) FROM topics WHERE group_id = ?", (group_id,))
        latest_topic_time = cursor.fetchone()[0]

        # è·å–æœ€æ—©è¯é¢˜æ—¶é—´
        cursor.execute("SELECT MIN(create_time) FROM topics WHERE group_id = ?", (group_id,))
        earliest_topic_time = cursor.fetchone()[0]

        # è·å–æ€»ç‚¹èµæ•°
        cursor.execute("SELECT SUM(likes_count) FROM topics WHERE group_id = ?", (group_id,))
        total_likes = cursor.fetchone()[0] or 0

        # è·å–æ€»è¯„è®ºæ•°
        cursor.execute("SELECT SUM(comments_count) FROM topics WHERE group_id = ?", (group_id,))
        total_comments = cursor.fetchone()[0] or 0

        # è·å–æ€»é˜…è¯»æ•°
        cursor.execute("SELECT SUM(reading_count) FROM topics WHERE group_id = ?", (group_id,))
        total_readings = cursor.fetchone()[0] or 0

        return {
            "group_id": group_id,
            "topics_count": topics_count,
            "users_count": users_count,
            "latest_topic_time": latest_topic_time,
            "earliest_topic_time": earliest_topic_time,
            "total_likes": total_likes,
            "total_comments": total_comments,
            "total_readings": total_readings
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç¾¤ç»„ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.get("/api/groups/{group_id}/database-info")
async def get_group_database_info(group_id: int):
    """è·å–æŒ‡å®šç¾¤ç»„çš„æ•°æ®åº“ä¿¡æ¯"""
    try:
        path_manager = get_db_path_manager()
        db_info = path_manager.get_database_info(str(group_id))

        return {
            "group_id": group_id,
            "database_info": db_info
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.delete("/api/groups/{group_id}/topics")
async def delete_group_topics(group_id: int):
    """åˆ é™¤æŒ‡å®šç¾¤ç»„çš„æ‰€æœ‰è¯é¢˜æ•°æ®"""
    try:
        # ä½¿ç”¨æŒ‡å®šç¾¤ç»„çš„çˆ¬è™«å®ä¾‹
        crawler = get_crawler_for_group(str(group_id))

        # è·å–åˆ é™¤å‰çš„ç»Ÿè®¡ä¿¡æ¯
        crawler.db.cursor.execute('SELECT COUNT(*) FROM topics WHERE group_id = ?', (group_id,))
        topics_count = crawler.db.cursor.fetchone()[0]

        if topics_count == 0:
            return {
                "message": "è¯¥ç¾¤ç»„æ²¡æœ‰è¯é¢˜æ•°æ®",
                "deleted_count": 0
            }

        # åˆ é™¤ç›¸å…³æ•°æ®ï¼ˆæŒ‰ç…§å¤–é”®ä¾èµ–é¡ºåºï¼‰
        tables_to_clean = [
            ('user_liked_emojis', 'topic_id'),
            ('like_emojis', 'topic_id'),
            ('likes', 'topic_id'),
            ('images', 'topic_id'),
            ('comments', 'topic_id'),
            ('answers', 'topic_id'),
            ('questions', 'topic_id'),
            ('articles', 'topic_id'),
            ('talks', 'topic_id'),
            ('topic_files', 'topic_id'),  # æ·»åŠ è¯é¢˜æ–‡ä»¶è¡¨
            ('topic_tags', 'topic_id'),   # æ·»åŠ è¯é¢˜æ ‡ç­¾å…³è”è¡¨
            ('topics', 'group_id')
        ]

        deleted_counts = {}

        for table, id_column in tables_to_clean:
            if id_column == 'group_id':
                # ç›´æ¥æŒ‰group_idåˆ é™¤
                crawler.db.cursor.execute(f'DELETE FROM {table} WHERE {id_column} = ?', (group_id,))
            else:
                # æŒ‰topic_idåˆ é™¤ï¼Œéœ€è¦å…ˆæ‰¾åˆ°è¯¥ç¾¤ç»„çš„æ‰€æœ‰topic_id
                crawler.db.cursor.execute(f'''
                    DELETE FROM {table}
                    WHERE {id_column} IN (
                        SELECT topic_id FROM topics WHERE group_id = ?
                    )
                ''', (group_id,))

            deleted_counts[table] = crawler.db.cursor.rowcount

        # æäº¤äº‹åŠ¡
        crawler.db.conn.commit()

        return {
            "message": f"æˆåŠŸåˆ é™¤ç¾¤ç»„ {group_id} çš„æ‰€æœ‰è¯é¢˜æ•°æ®",
            "deleted_topics_count": topics_count,
            "deleted_details": deleted_counts
        }

    except Exception as e:
        # å›æ»šäº‹åŠ¡
        crawler.db.conn.rollback()
        raise HTTPException(status_code=500, detail=f"åˆ é™¤è¯é¢˜æ•°æ®å¤±è´¥: {str(e)}")

@app.get("/api/tasks/{task_id}/logs")
async def get_task_logs(task_id: str):
    """è·å–ä»»åŠ¡æ—¥å¿—"""
    if task_id not in task_logs:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    return {
        "task_id": task_id,
        "logs": task_logs[task_id]
    }

@app.get("/api/tasks/{task_id}/stream")
async def stream_task_logs(task_id: str):
    """SSEæµå¼ä¼ è¾“ä»»åŠ¡æ—¥å¿—"""
    async def event_stream():
        # åˆå§‹åŒ–è¿æ¥
        if task_id not in sse_connections:
            sse_connections[task_id] = []

        # å‘é€å†å²æ—¥å¿—
        if task_id in task_logs:
            for log in task_logs[task_id]:
                yield f"data: {json.dumps({'type': 'log', 'message': log})}\n\n"

        # å‘é€ä»»åŠ¡çŠ¶æ€
        if task_id in current_tasks:
            task = current_tasks[task_id]
            yield f"data: {json.dumps({'type': 'status', 'status': task['status'], 'message': task['message']})}\n\n"

        # è®°å½•å½“å‰æ—¥å¿—æ•°é‡ï¼Œç”¨äºæ£€æµ‹æ–°æ—¥å¿—
        last_log_count = len(task_logs.get(task_id, []))

        # ä¿æŒè¿æ¥æ´»è·ƒ
        try:
            while True:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ—¥å¿—
                current_log_count = len(task_logs.get(task_id, []))
                if current_log_count > last_log_count:
                    # å‘é€æ–°æ—¥å¿—
                    new_logs = task_logs[task_id][last_log_count:]
                    for log in new_logs:
                        yield f"data: {json.dumps({'type': 'log', 'message': log})}\n\n"
                    last_log_count = current_log_count

                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å˜åŒ–
                if task_id in current_tasks:
                    task = current_tasks[task_id]
                    yield f"data: {json.dumps({'type': 'status', 'status': task['status'], 'message': task['message']})}\n\n"

                    if task['status'] in ['completed', 'failed', 'cancelled']:
                        break

                # å‘é€å¿ƒè·³
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                await asyncio.sleep(0.5)  # æ›´é¢‘ç¹çš„æ£€æŸ¥

        except asyncio.CancelledError:
            # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
            pass

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*",
        }
    )

# å›¾ç‰‡ä»£ç†API
@app.get("/api/proxy/image")
async def proxy_image(url: str):
    """å›¾ç‰‡ä»£ç†ï¼Œè§£å†³ç›—é“¾é—®é¢˜"""
    import requests
    from fastapi.responses import Response

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://wx.zsxq.com/',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        return Response(
            content=response.content,
            media_type=response.headers.get('content-type', 'image/jpeg'),
            headers={
                'Cache-Control': 'public, max-age=3600',
                'Access-Control-Allow-Origin': '*'
            }
        )
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"å›¾ç‰‡åŠ è½½å¤±è´¥: {str(e)}")

# è®¾ç½®ç›¸å…³APIè·¯ç”±
@app.get("/api/settings/crawler")
async def get_crawler_settings():
    """è·å–çˆ¬è™«è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            return {
                "min_delay": 2.0,
                "max_delay": 5.0,
                "long_delay_interval": 15,
                "timestamp_offset_ms": 1,
                "debug_mode": False
            }

        return {
            "min_delay": crawler.min_delay,
            "max_delay": crawler.max_delay,
            "long_delay_interval": crawler.long_delay_interval,
            "timestamp_offset_ms": crawler.timestamp_offset_ms,
            "debug_mode": crawler.debug_mode
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çˆ¬è™«è®¾ç½®å¤±è´¥: {str(e)}")

class CrawlerSettingsRequest(BaseModel):
    min_delay: float = Field(default=2.0, ge=0.5, le=10.0)
    max_delay: float = Field(default=5.0, ge=1.0, le=20.0)
    long_delay_interval: int = Field(default=15, ge=5, le=100)
    timestamp_offset_ms: int = Field(default=1, ge=0, le=1000)
    debug_mode: bool = Field(default=False)

@app.post("/api/settings/crawler")
async def update_crawler_settings(request: CrawlerSettingsRequest):
    """æ›´æ–°çˆ¬è™«è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            raise HTTPException(status_code=404, detail="çˆ¬è™«æœªåˆå§‹åŒ–")

        # éªŒè¯è®¾ç½®
        if request.min_delay >= request.max_delay:
            raise HTTPException(status_code=400, detail="æœ€å°å»¶è¿Ÿå¿…é¡»å°äºæœ€å¤§å»¶è¿Ÿ")

        # æ›´æ–°è®¾ç½®
        crawler.min_delay = request.min_delay
        crawler.max_delay = request.max_delay
        crawler.long_delay_interval = request.long_delay_interval
        crawler.timestamp_offset_ms = request.timestamp_offset_ms
        crawler.debug_mode = request.debug_mode

        return {
            "message": "çˆ¬è™«è®¾ç½®å·²æ›´æ–°",
            "settings": {
                "min_delay": crawler.min_delay,
                "max_delay": crawler.max_delay,
                "long_delay_interval": crawler.long_delay_interval,
                "timestamp_offset_ms": crawler.timestamp_offset_ms,
                "debug_mode": crawler.debug_mode
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°çˆ¬è™«è®¾ç½®å¤±è´¥: {str(e)}")

@app.get("/api/settings/downloader")
async def get_downloader_settings():
    """è·å–æ–‡ä»¶ä¸‹è½½å™¨è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            return {
                "download_interval_min": 30,
                "download_interval_max": 60,
                "long_delay_interval": 10,
                "long_delay_min": 300,
                "long_delay_max": 600
            }

        downloader = crawler.get_file_downloader()
        return {
            "download_interval_min": downloader.download_interval_min,
            "download_interval_max": downloader.download_interval_max,
            "long_delay_interval": downloader.long_delay_interval,
            "long_delay_min": downloader.long_delay_min,
            "long_delay_max": downloader.long_delay_max
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¸‹è½½å™¨è®¾ç½®å¤±è´¥: {str(e)}")

class DownloaderSettingsRequest(BaseModel):
    download_interval_min: int = Field(default=30, ge=1, le=300)
    download_interval_max: int = Field(default=60, ge=5, le=600)
    long_delay_interval: int = Field(default=10, ge=1, le=100)
    long_delay_min: int = Field(default=300, ge=60, le=1800)
    long_delay_max: int = Field(default=600, ge=120, le=3600)

@app.post("/api/settings/downloader")
async def update_downloader_settings(request: DownloaderSettingsRequest):
    """æ›´æ–°æ–‡ä»¶ä¸‹è½½å™¨è®¾ç½®"""
    try:
        crawler = get_crawler_safe()
        if not crawler:
            raise HTTPException(status_code=404, detail="çˆ¬è™«æœªåˆå§‹åŒ–")

        # éªŒè¯è®¾ç½®
        if request.download_interval_min >= request.download_interval_max:
            raise HTTPException(status_code=400, detail="æœ€å°ä¸‹è½½é—´éš”å¿…é¡»å°äºæœ€å¤§ä¸‹è½½é—´éš”")

        if request.long_delay_min >= request.long_delay_max:
            raise HTTPException(status_code=400, detail="æœ€å°é•¿ä¼‘çœ æ—¶é—´å¿…é¡»å°äºæœ€å¤§é•¿ä¼‘çœ æ—¶é—´")

        downloader = crawler.get_file_downloader()

        # æ›´æ–°è®¾ç½®
        downloader.download_interval_min = request.download_interval_min
        downloader.download_interval_max = request.download_interval_max
        downloader.long_delay_interval = request.long_delay_interval
        downloader.long_delay_min = request.long_delay_min
        downloader.long_delay_max = request.long_delay_max

        return {
            "message": "ä¸‹è½½å™¨è®¾ç½®å·²æ›´æ–°",
            "settings": {
                "download_interval_min": downloader.download_interval_min,
                "download_interval_max": downloader.download_interval_max,
                "long_delay_interval": downloader.long_delay_interval,
                "long_delay_min": downloader.long_delay_min,
                "long_delay_max": downloader.long_delay_max
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä¸‹è½½å™¨è®¾ç½®å¤±è´¥: {str(e)}")

# =========================
# è‡ªåŠ¨è´¦å·åŒ¹é…ç¼“å­˜ä¸è¾…åŠ©å‡½æ•°
# =========================
ACCOUNT_DETECT_TTL_SECONDS = 300
_account_detect_cache: Dict[str, Any] = {
    "built_at": 0,
    "group_to_account": {},
    "cookie_by_account": {}
}

def clear_account_detect_cache():
    """æ¸…é™¤è´¦å·ç¾¤ç»„æ£€æµ‹ç¼“å­˜ï¼Œä½¿æ–°è´¦å·/åˆ é™¤è´¦å·åç¾¤ç»„ç«‹å³åˆ·æ–°"""
    _account_detect_cache["built_at"] = 0

def _get_all_account_sources() -> List[Dict[str, Any]]:
    """è·å–æ‰€æœ‰è´¦å·æ¥æº"""
    sources: List[Dict[str, Any]] = []
    try:
        sql_mgr = get_accounts_sql_manager()
        accounts = sql_mgr.get_accounts(mask_cookie=False)
        if accounts:
            sources.extend(accounts)
    except Exception:
        pass
    return sources

def build_account_group_detection(force_refresh: bool = False) -> Dict[str, Dict[str, Any]]:
    """
    æ„å»ºè‡ªåŠ¨åŒ¹é…æ˜ å°„ï¼šgroup_id -> è´¦å·æ‘˜è¦
    éå†æ‰€æœ‰è´¦å·æ¥æºï¼Œè°ƒç”¨å®˜æ–¹ /v2/groups è·å–å…¶å¯è®¿é—®ç¾¤ç»„è¿›è¡Œæ¯”å¯¹ã€‚
    ä½¿ç”¨å†…å­˜ç¼“å­˜å‡å°‘é¢‘ç¹è¯·æ±‚ã€‚
    """
    now = time.time()
    cache = _account_detect_cache
    if (not force_refresh
        and cache.get("group_to_account")
        and now - cache.get("built_at", 0) < ACCOUNT_DETECT_TTL_SECONDS):
        return cache["group_to_account"]

    group_to_account: Dict[str, Dict[str, Any]] = {}
    cookie_by_account: Dict[str, str] = {}

    sources = _get_all_account_sources()
    for src in sources:
        cookie = src.get("cookie", "")
        acc_id = src.get("id")
        if not cookie or cookie == "your_cookie_here" or not acc_id:
            continue

        # è®°å½•è´¦å·å¯¹åº”cookie
        cookie_by_account[acc_id] = cookie

        try:
            groups = fetch_groups_from_api(cookie)
            for g in groups or []:
                gid = str(g.get("group_id"))
                if gid and gid not in group_to_account:
                    group_to_account[gid] = {
                        "id": acc_id,
                        "name": src.get("name") or acc_id,
                        "created_at": src.get("created_at"),
                        "cookie": "***"
                    }
        except Exception:
            # å¿½ç•¥å•ä¸ªè´¦å·å¤±è´¥
            continue

    cache["group_to_account"] = group_to_account
    cache["cookie_by_account"] = cookie_by_account
    cache["built_at"] = now
    return group_to_account

def get_cookie_for_group(group_id: str) -> str:
    """æ ¹æ®è‡ªåŠ¨åŒ¹é…ç»“æœé€‰æ‹©ç”¨äºè¯¥ç¾¤ç»„çš„Cookieï¼Œå¤±è´¥åˆ™å›é€€åˆ°config.toml"""
    mapping = build_account_group_detection(force_refresh=False)
    summary = mapping.get(str(group_id))
    cookie = None
    if summary:
        cookie = _account_detect_cache.get("cookie_by_account", {}).get(summary["id"])
    if not cookie:
        cfg = load_config()
        auth = cfg.get('auth', {}) if cfg else {}
        cookie = auth.get('cookie', '')
    return cookie

def get_account_summary_for_group_auto(group_id: str) -> Optional[Dict[str, Any]]:
    """è¿”å›è‡ªåŠ¨åŒ¹é…åˆ°çš„è´¦å·æ‘˜è¦"""
    mapping = build_account_group_detection(force_refresh=False)
    summary = mapping.get(str(group_id))
    if summary:
        return summary

    # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è´¦å·ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè´¦å·
    try:
        sql_mgr = get_accounts_sql_manager()
        first_acc = sql_mgr.get_first_account(mask_cookie=True)
        if first_acc:
            return {
                "id": first_acc["id"],
                "name": first_acc["name"],
                "created_at": first_acc["created_at"],
                "cookie": first_acc["cookie"]
            }
    except Exception:
        pass

    return None

# =========================
# æ–°å¢ï¼šæŒ‰æ—¶é—´åŒºé—´çˆ¬å–
# =========================

class CrawlTimeRangeRequest(BaseModel):
    startTime: Optional[str] = Field(default=None, description="å¼€å§‹æ—¶é—´ï¼Œæ”¯æŒ YYYY-MM-DD æˆ– ISO8601ï¼Œç¼ºçœåˆ™æŒ‰ lastDays æ¨å¯¼")
    endTime: Optional[str] = Field(default=None, description="ç»“æŸæ—¶é—´ï¼Œé»˜è®¤å½“å‰æ—¶é—´ï¼ˆæœ¬åœ°ä¸œå…«åŒºï¼‰")
    lastDays: Optional[int] = Field(default=None, ge=1, le=3650, description="æœ€è¿‘Nå¤©ï¼ˆä¸ startTime/endTime äº’æ–¥ä¼˜å…ˆï¼›å½“ startTime ç¼ºçœæ—¶å¯ç”¨ï¼‰")
    perPage: Optional[int] = Field(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
    # å¯é€‰çš„éšæœºé—´éš”è®¾ç½®ï¼ˆä¸å…¶ä»–çˆ¬å–æ¥å£ä¿æŒä¸€è‡´ï¼‰
    crawlIntervalMin: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å°å€¼(ç§’)")
    crawlIntervalMax: Optional[float] = Field(default=None, ge=1.0, le=60.0, description="çˆ¬å–é—´éš”æœ€å¤§å€¼(ç§’)")
    longSleepIntervalMin: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å°å€¼(ç§’)")
    longSleepIntervalMax: Optional[float] = Field(default=None, ge=60.0, le=3600.0, description="é•¿ä¼‘çœ é—´éš”æœ€å¤§å€¼(ç§’)")
    pagesPerBatch: Optional[int] = Field(default=None, ge=5, le=50, description="æ¯æ‰¹æ¬¡é¡µé¢æ•°")


def run_crawl_time_range_task(task_id: str, group_id: str, request: "CrawlTimeRangeRequest"):
    """åå°æ‰§è¡Œâ€œæŒ‰æ—¶é—´åŒºé—´çˆ¬å–â€ä»»åŠ¡ï¼šä»…å¯¼å…¥ä½äºåŒºé—´ [startTime, endTime] å†…çš„è¯é¢˜"""
    try:
        from datetime import datetime, timedelta, timezone

        # è§£æç”¨æˆ·è¾“å…¥æ—¶é—´
        def parse_user_time(s: Optional[str]) -> Optional[datetime]:
            if not s:
                return None
            t = s.strip()
            try:
                # ä»…æ—¥æœŸï¼šYYYY-MM-DD -> å½“å¤©00:00:00ï¼ˆä¸œå…«åŒºï¼‰
                if len(t) == 10 and t[4] == '-' and t[7] == '-':
                    dt = datetime.strptime(t, '%Y-%m-%d')
                    return dt.replace(tzinfo=timezone(timedelta(hours=8)))
                # datetime-local (æ— ç§’)ï¼šYYYY-MM-DDTHH:MM
                if 'T' in t and len(t) == 16:
                    t = t + ':00'
                # å°¾éƒ¨Z -> +00:00
                if t.endswith('Z'):
                    t = t.replace('Z', '+00:00')
                # å…¼å®¹ +0800 -> +08:00
                if len(t) >= 24 and (t[-5] in ['+', '-']) and t[-3] != ':':
                    t = t[:-2] + ':' + t[-2:]
                dt = datetime.fromisoformat(t)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone(timedelta(hours=8)))
                return dt
            except Exception:
                return None

        bj_tz = timezone(timedelta(hours=8))
        now_bj = datetime.now(bj_tz)

        start_dt = parse_user_time(request.startTime)
        end_dt = parse_user_time(request.endTime) if request.endTime else None

        # è‹¥æŒ‡å®šäº†æœ€è¿‘Nå¤©ï¼Œä»¥ end_dtï¼ˆé»˜è®¤ç°åœ¨ï¼‰ä¸ºç»ˆç‚¹æ¨å¯¼ start_dt
        if request.lastDays and request.lastDays > 0:
            if end_dt is None:
                end_dt = now_bj
            start_dt = end_dt - timedelta(days=request.lastDays)

        # é»˜è®¤ end_dt = ç°åœ¨
        if end_dt is None:
            end_dt = now_bj
        # é»˜è®¤ start_dt = end_dt - 30å¤©
        if start_dt is None:
            start_dt = end_dt - timedelta(days=30)

        # ä¿è¯æ—¶é—´é¡ºåº
        if start_dt > end_dt:
            start_dt, end_dt = end_dt, start_dt

        update_task(task_id, "running", "å¼€å§‹æŒ‰æ—¶é—´åŒºé—´çˆ¬å–...")
        add_task_log(task_id, f"ğŸ—“ï¸ æ—¶é—´èŒƒå›´: {start_dt.isoformat()} ~ {end_dt.isoformat()}")

        # åœæ­¢æ£€æŸ¥
        def stop_check():
            return is_task_stopped(task_id)

        # çˆ¬è™«å®ä¾‹ï¼ˆç»‘å®šè¯¥ç¾¤ç»„ï¼‰
        def log_callback(message: str):
            add_task_log(task_id, message)

        cookie = get_cookie_for_group(group_id)
        path_manager = get_db_path_manager()
        db_path = path_manager.get_topics_db_path(group_id)

        crawler = ZSXQInteractiveCrawler(cookie, group_id, db_path, log_callback)
        crawler.stop_check_func = stop_check

        # å¯é€‰ï¼šåº”ç”¨è‡ªå®šä¹‰é—´éš”è®¾ç½®
        if any([
            request.crawlIntervalMin, request.crawlIntervalMax,
            request.longSleepIntervalMin, request.longSleepIntervalMax,
            request.pagesPerBatch
        ]):
            crawler.set_custom_intervals(
                crawl_interval_min=request.crawlIntervalMin,
                crawl_interval_max=request.crawlIntervalMax,
                long_sleep_interval_min=request.longSleepIntervalMin,
                long_sleep_interval_max=request.longSleepIntervalMax,
                pages_per_batch=request.pagesPerBatch
            )

        per_page = request.perPage or 20
        total_stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}
        end_time_param = None  # ä»æœ€æ–°å¼€å§‹
        max_retries_per_page = 10

        while True:
            if is_task_stopped(task_id):
                add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
                break

            retry = 0
            page_processed = False
            last_time_dt_in_page = None

            while retry < max_retries_per_page:
                if is_task_stopped(task_id):
                    break

                data = crawler.fetch_topics_safe(
                    scope="all",
                    count=per_page,
                    end_time=end_time_param,
                    is_historical=True if end_time_param else False
                )

                # ä¼šå‘˜è¿‡æœŸ
                if data and isinstance(data, dict) and data.get('expired'):
                    add_task_log(task_id, f"âŒ ä¼šå‘˜å·²è¿‡æœŸ: {data.get('message')}")
                    update_task(task_id, "failed", "ä¼šå‘˜å·²è¿‡æœŸ", data)
                    return

                if not data:
                    retry += 1
                    total_stats['errors'] += 1
                    add_task_log(task_id, f"âŒ é¡µé¢è·å–å¤±è´¥ (é‡è¯•{retry}/{max_retries_per_page})")
                    continue

                topics = (data.get('resp_data', {}) or {}).get('topics', []) or []
                if not topics:
                    add_task_log(task_id, "ğŸ“­ æ— æ›´å¤šæ•°æ®ï¼Œä»»åŠ¡ç»“æŸ")
                    page_processed = True
                    break

                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                from datetime import datetime
                filtered = []
                for t in topics:
                    ts = t.get('create_time')
                    dt = None
                    try:
                        if ts:
                            ts_fixed = ts.replace('+0800', '+08:00') if ts.endswith('+0800') else ts
                            dt = datetime.fromisoformat(ts_fixed)
                    except Exception:
                        dt = None

                    if dt:
                        last_time_dt_in_page = dt  # è¯¥é¡µæ•°æ®æŒ‰æ—¶é—´é™åºï¼›å¾ªç¯ç»“æŸåæŒæœ‰æœ€åï¼ˆæœ€è€ï¼‰æ—¶é—´
                        if start_dt <= dt <= end_dt:
                            filtered.append(t)

                # ä»…å¯¼å…¥æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
                if filtered:
                    filtered_data = {'succeeded': True, 'resp_data': {'topics': filtered}}
                    page_stats = crawler.store_batch_data(filtered_data)
                    total_stats['new_topics'] += page_stats.get('new_topics', 0)
                    total_stats['updated_topics'] += page_stats.get('updated_topics', 0)
                    total_stats['errors'] += page_stats.get('errors', 0)

                total_stats['pages'] += 1
                page_processed = True

                # è®¡ç®—ä¸‹ä¸€é¡µçš„ end_timeï¼ˆä½¿ç”¨è¯¥é¡µæœ€è€è¯é¢˜æ—¶é—´ - åç§»æ¯«ç§’ï¼‰
                oldest_in_page = topics[-1].get('create_time')
                try:
                    dt_oldest = datetime.fromisoformat(oldest_in_page.replace('+0800', '+08:00'))
                    dt_oldest = dt_oldest - timedelta(milliseconds=crawler.timestamp_offset_ms)
                    end_time_param = dt_oldest.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                except Exception:
                    end_time_param = oldest_in_page

                # è‹¥è¯¥é¡µæœ€è€æ—¶é—´å·²æ—©äº start_dtï¼Œåˆ™åç»­æ›´è€æ•°æ®å‡ä¸åœ¨èŒƒå›´å†…ï¼Œç»“æŸ
                if last_time_dt_in_page and last_time_dt_in_page < start_dt:
                    add_task_log(task_id, "âœ… å·²åˆ°è¾¾èµ·å§‹æ—¶é—´ä¹‹å‰ï¼Œä»»åŠ¡ç»“æŸ")
                    break

                # æˆåŠŸå¤„ç†åè¿›è¡Œé•¿ä¼‘çœ æ£€æŸ¥
                crawler.check_page_long_delay()
                break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯

            if not page_processed:
                add_task_log(task_id, "ğŸš« å½“å‰é¡µé¢è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»ˆæ­¢ä»»åŠ¡")
                break

            # ç»“æŸæ¡ä»¶ï¼šæ²¡æœ‰ä¸‹ä¸€é¡µæ—¶é—´æˆ–å·²è¶Šè¿‡èµ·å§‹è¾¹ç•Œ
            if not end_time_param or (last_time_dt_in_page and last_time_dt_in_page < start_dt):
                break

        update_task(task_id, "completed", "æ—¶é—´åŒºé—´çˆ¬å–å®Œæˆ", total_stats)
    except Exception as e:
        if not is_task_stopped(task_id):
            add_task_log(task_id, f"âŒ æ—¶é—´åŒºé—´çˆ¬å–å¤±è´¥: {str(e)}")
            update_task(task_id, "failed", f"æ—¶é—´åŒºé—´çˆ¬å–å¤±è´¥: {str(e)}")


@app.post("/api/crawl/range/{group_id}")
async def crawl_by_time_range(group_id: str, request: CrawlTimeRangeRequest, background_tasks: BackgroundTasks):
    """æŒ‰æ—¶é—´åŒºé—´çˆ¬å–è¯é¢˜ï¼ˆæ”¯æŒæœ€è¿‘Nå¤©æˆ–è‡ªå®šä¹‰å¼€å§‹/ç»“æŸæ—¶é—´ï¼‰"""
    try:
        task_id = create_task("crawl_time_range", f"æŒ‰æ—¶é—´åŒºé—´çˆ¬å– (ç¾¤ç»„: {group_id})")
        background_tasks.add_task(run_crawl_time_range_task, task_id, group_id, request)
        return {"task_id": task_id, "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨åå°æ‰§è¡Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºæ—¶é—´åŒºé—´çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")
@app.delete("/api/groups/{group_id}")
async def delete_group_local(group_id: str):
    """
    åˆ é™¤æŒ‡å®šç¤¾ç¾¤çš„æœ¬åœ°æ•°æ®ï¼ˆæ•°æ®åº“ã€ä¸‹è½½æ–‡ä»¶ã€å›¾ç‰‡ç¼“å­˜ï¼‰ï¼Œä¸å½±å“è´¦å·å¯¹è¯¥ç¤¾ç¾¤çš„è®¿é—®æƒé™
    """
    try:
        details = {
            "topics_db_removed": False,
            "files_db_removed": False,
            "downloads_dir_removed": False,
            "images_cache_removed": False,
            "group_dir_removed": False,
        }

        # å°è¯•å…³é—­æ•°æ®åº“è¿æ¥ï¼Œé¿å…æ–‡ä»¶å ç”¨
        try:
            crawler = get_crawler_for_group(group_id)
            try:
                if hasattr(crawler, "file_downloader") and crawler.file_downloader:
                    if hasattr(crawler.file_downloader, "file_db") and crawler.file_downloader.file_db:
                        crawler.file_downloader.file_db.close()
                        print(f"âœ… å·²å…³é—­æ–‡ä»¶æ•°æ®åº“è¿æ¥ï¼ˆç¾¤ {group_id}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ å…³é—­æ–‡ä»¶æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
            try:
                if hasattr(crawler, "db") and crawler.db:
                    crawler.db.close()
                    print(f"âœ… å·²å…³é—­è¯é¢˜æ•°æ®åº“è¿æ¥ï¼ˆç¾¤ {group_id}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ å…³é—­è¯é¢˜æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")
        except Exception as e:
            print(f"âš ï¸ è·å–çˆ¬è™«å®ä¾‹ä»¥å…³é—­è¿æ¥å¤±è´¥: {e}")

        # åƒåœ¾å›æ”¶ + ç­‰å¾…ç‰‡åˆ»ï¼Œç¡®ä¿å¥æŸ„é‡Šæ”¾
        import gc, time, shutil
        gc.collect()
        time.sleep(0.3)

        path_manager = get_db_path_manager()
        group_dir = path_manager.get_group_dir(group_id)
        topics_db = path_manager.get_topics_db_path(group_id)
        files_db = path_manager.get_files_db_path(group_id)

        # åˆ é™¤è¯é¢˜æ•°æ®åº“
        try:
            if os.path.exists(topics_db):
                os.remove(topics_db)
                details["topics_db_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤è¯é¢˜æ•°æ®åº“: {topics_db}")
        except PermissionError as pe:
            raise HTTPException(status_code=500, detail=f"è¯é¢˜æ•°æ®åº“è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤è¯é¢˜æ•°æ®åº“å¤±è´¥: {e}")

        # åˆ é™¤æ–‡ä»¶æ•°æ®åº“
        try:
            if os.path.exists(files_db):
                os.remove(files_db)
                details["files_db_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ–‡ä»¶æ•°æ®åº“: {files_db}")
        except PermissionError as pe:
            raise HTTPException(status_code=500, detail=f"æ–‡ä»¶æ•°æ®åº“è¢«å ç”¨ï¼Œæ— æ³•åˆ é™¤: {pe}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ–‡ä»¶æ•°æ®åº“å¤±è´¥: {e}")

        # åˆ é™¤ä¸‹è½½ç›®å½•
        downloads_dir = os.path.join(group_dir, "downloads")
        if os.path.exists(downloads_dir):
            try:
                shutil.rmtree(downloads_dir, ignore_errors=False)
                details["downloads_dir_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸‹è½½ç›®å½•: {downloads_dir}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ä¸‹è½½ç›®å½•å¤±è´¥: {e}")

        # æ¸…ç©ºå¹¶åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•ï¼ŒåŒæ—¶é‡Šæ”¾ç¼“å­˜ç®¡ç†å™¨
        try:
            from image_cache_manager import get_image_cache_manager, clear_group_cache_manager
            cache_manager = get_image_cache_manager(group_id)
            ok, msg = cache_manager.clear_cache()
            if ok:
                details["images_cache_removed"] = True
                print(f"ğŸ—‘ï¸ å›¾ç‰‡ç¼“å­˜æ¸…ç©º: {msg}")
            images_dir = os.path.join(group_dir, "images")
            if os.path.exists(images_dir):
                try:
                    shutil.rmtree(images_dir, ignore_errors=True)
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•: {images_dir}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤å›¾ç‰‡ç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            clear_group_cache_manager(group_id)
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å›¾ç‰‡ç¼“å­˜å¤±è´¥: {e}")

        # è‹¥ç¾¤ç»„ç›®å½•å·²ç©ºï¼Œåˆ™åˆ é™¤è¯¥ç›®å½•
        try:
            if os.path.exists(group_dir) and len(os.listdir(group_dir)) == 0:
                os.rmdir(group_dir)
                details["group_dir_removed"] = True
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºç¾¤ç»„ç›®å½•: {group_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤ç¾¤ç»„ç›®å½•å¤±è´¥: {e}")

        # æ›´æ–°æœ¬åœ°ç¾¤ç¼“å­˜ï¼ˆä»ç¼“å­˜é›†åˆç§»é™¤ï¼‰
        try:
            gid_int = int(group_id)
            if gid_int in _local_groups_cache.get("ids", set()):
                _local_groups_cache["ids"].discard(gid_int)
                _local_groups_cache["scanned_at"] = time.time()
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°æœ¬åœ°ç¾¤ç¼“å­˜å¤±è´¥: {e}")

        any_removed = any(details.values())
        return {
            "success": True,
            "message": f"ç¾¤ç»„ {group_id} æœ¬åœ°æ•°æ®" + ("å·²åˆ é™¤" if any_removed else "ä¸å­˜åœ¨"),
            "details": details,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ç¾¤ç»„æœ¬åœ°æ•°æ®å¤±è´¥: {str(e)}")


# =========================
# ä¸“æ ç›¸å…³ API
# =========================

def get_columns_db(group_id: str) -> ZSXQColumnsDatabase:
    """è·å–æŒ‡å®šç¾¤ç»„çš„ä¸“æ æ•°æ®åº“å®ä¾‹"""
    path_manager = get_db_path_manager()
    db_path = path_manager.get_columns_db_path(group_id)
    return ZSXQColumnsDatabase(db_path)


@app.get("/api/groups/{group_id}/columns")
async def get_group_columns(group_id: str):
    """è·å–ç¾¤ç»„çš„ä¸“æ ç›®å½•åˆ—è¡¨ï¼ˆä»æœ¬åœ°æ•°æ®åº“ï¼‰"""
    try:
        db = get_columns_db(group_id)
        columns = db.get_columns(int(group_id))
        stats = db.get_stats(int(group_id))
        db.close()
        return {
            "columns": columns,
            "stats": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¸“æ ç›®å½•å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/columns/{column_id}/topics")
async def get_column_topics(group_id: str, column_id: int):
    """è·å–ä¸“æ ä¸‹çš„æ–‡ç« åˆ—è¡¨ï¼ˆä»æœ¬åœ°æ•°æ®åº“ï¼‰"""
    try:
        db = get_columns_db(group_id)
        topics = db.get_column_topics(column_id)
        column = db.get_column(column_id)
        db.close()
        return {
            "column": column,
            "topics": topics
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/columns/topics/{topic_id}")
async def get_column_topic_detail(group_id: str, topic_id: int):
    """è·å–ä¸“æ æ–‡ç« è¯¦æƒ…ï¼ˆä»æœ¬åœ°æ•°æ®åº“ï¼‰"""
    try:
        db = get_columns_db(group_id)
        detail = db.get_topic_detail(topic_id)
        db.close()

        if not detail:
            raise HTTPException(status_code=404, detail="æ–‡ç« è¯¦æƒ…ä¸å­˜åœ¨")

        # è§£æ raw_json è·å– Q&A ç±»å‹å†…å®¹
        if detail.get('raw_json'):
            try:
                raw_data = json.loads(detail['raw_json'])
                topic_type = raw_data.get('type', '')

                # Q&A ç±»å‹ï¼šæå– question å’Œ answer
                if topic_type == 'q&a':
                    question = raw_data.get('question', {})
                    answer = raw_data.get('answer', {})

                    detail['question'] = {
                        'text': question.get('text', ''),
                        'owner': question.get('owner'),
                        'images': question.get('images', []),
                    }
                    detail['answer'] = {
                        'text': answer.get('text', ''),
                        'owner': answer.get('owner'),
                        'images': answer.get('images', []),
                    }
                    # å¦‚æœ full_text ä¸ºç©ºï¼Œä½¿ç”¨ answer.text
                    if not detail.get('full_text') and answer.get('text'):
                        detail['full_text'] = answer.get('text', '')

                # talk ç±»å‹ï¼šå¦‚æœ full_text ä¸ºç©ºï¼Œä» talk æå–
                elif topic_type == 'talk':
                    talk = raw_data.get('talk', {})
                    if not detail.get('full_text') and talk.get('text'):
                        detail['full_text'] = talk.get('text', '')

            except (json.JSONDecodeError, TypeError):
                pass

        return detail
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}")


@app.post("/api/groups/{group_id}/columns/fetch")
async def fetch_group_columns(group_id: str, request: ColumnsSettingsRequest, background_tasks: BackgroundTasks):
    """é‡‡é›†ç¾¤ç»„çš„æ‰€æœ‰ä¸“æ å†…å®¹ï¼ˆåå°ä»»åŠ¡ï¼‰"""
    global task_counter
    
    try:
        task_counter += 1
        task_id = f"columns_{group_id}_{task_counter}"
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        current_tasks[task_id] = {
            "task_id": task_id,
            "type": "columns_fetch",
            "group_id": group_id,
            "status": "running",
            "message": "æ­£åœ¨é‡‡é›†ä¸“æ å†…å®¹...",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "result": None
        }
        task_logs[task_id] = []
        task_stop_flags[task_id] = False
        
        # æ·»åŠ åˆ°åå°ä»»åŠ¡
        background_tasks.add_task(
            _fetch_columns_task,
            task_id,
            group_id,
            request
        )
        
        return {
            "success": True,
            "task_id": task_id,
            "message": "ä¸“æ é‡‡é›†ä»»åŠ¡å·²å¯åŠ¨"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä¸“æ é‡‡é›†å¤±è´¥: {str(e)}")


async def _fetch_columns_task(task_id: str, group_id: str, settings: ColumnsSettingsRequest):
    """ä¸“æ é‡‡é›†åå°ä»»åŠ¡"""
    log_id = None
    db = None
    
    try:
        # è·å–é…ç½®å‚æ•°
        crawl_interval_min = settings.crawlIntervalMin or 2.0
        crawl_interval_max = settings.crawlIntervalMax or 5.0
        long_sleep_min = settings.longSleepIntervalMin or 30.0
        long_sleep_max = settings.longSleepIntervalMax or 60.0
        items_per_batch = settings.itemsPerBatch or 10
        download_files = settings.downloadFiles if settings.downloadFiles is not None else True
        download_videos = settings.downloadVideos if settings.downloadVideos is not None else True
        cache_images = settings.cacheImages if settings.cacheImages is not None else True
        incremental_mode = settings.incrementalMode if settings.incrementalMode is not None else False
        
        add_task_log(task_id, f"ğŸ“š å¼€å§‹é‡‡é›†ç¾¤ç»„ {group_id} çš„ä¸“æ å†…å®¹")
        add_task_log(task_id, "=" * 50)
        add_task_log(task_id, "âš™ï¸ é‡‡é›†é…ç½®:")
        add_task_log(task_id, f"   â±ï¸ è¯·æ±‚é—´éš”: {crawl_interval_min}~{crawl_interval_max} ç§’")
        add_task_log(task_id, f"   ğŸ˜´ é•¿ä¼‘çœ é—´éš”: {long_sleep_min}~{long_sleep_max} ç§’")
        add_task_log(task_id, f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {items_per_batch} ä¸ªè¯·æ±‚")
        add_task_log(task_id, f"   ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {'æ˜¯' if download_files else 'å¦'}")
        add_task_log(task_id, f"   ğŸ¬ ä¸‹è½½è§†é¢‘: {'æ˜¯' if download_videos else 'å¦'}")
        add_task_log(task_id, f"   ğŸ–¼ï¸ ç¼“å­˜å›¾ç‰‡: {'æ˜¯' if cache_images else 'å¦'}")
        add_task_log(task_id, f"   ğŸ”„ å¢é‡æ¨¡å¼: {'æ˜¯ï¼ˆè·³è¿‡å·²å­˜åœ¨ï¼‰' if incremental_mode else 'å¦ï¼ˆå…¨é‡é‡‡é›†ï¼‰'}")
        add_task_log(task_id, "=" * 50)
        
        cookie = get_cookie_for_group(group_id)
        if not cookie:
            raise Exception("æœªæ‰¾åˆ°å¯ç”¨Cookieï¼Œè¯·å…ˆé…ç½®è´¦å·")
        
        headers = build_stealth_headers(cookie)
        db = get_columns_db(group_id)
        log_id = db.start_crawl_log(int(group_id), 'full_fetch')
        
        columns_count = 0
        topics_count = 0
        details_count = 0
        files_count = 0
        images_count = 0
        videos_count = 0
        skipped_count = 0  # å¢é‡æ¨¡å¼è·³è¿‡çš„æ–‡ç« æ•°
        files_skipped = 0  # è·³è¿‡çš„æ–‡ä»¶æ•°ï¼ˆå·²å­˜åœ¨ï¼‰
        videos_skipped = 0  # è·³è¿‡çš„è§†é¢‘æ•°ï¼ˆå·²å­˜åœ¨ï¼‰
        request_count = 0  # è¯·æ±‚è®¡æ•°å™¨ï¼Œç”¨äºè§¦å‘é•¿ä¼‘çœ 
        
        # 1. è·å–ä¸“æ ç›®å½•åˆ—è¡¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        add_task_log(task_id, "ğŸ“‚ è·å–ä¸“æ ç›®å½•åˆ—è¡¨...")
        columns_url = f"https://api.zsxq.com/v2/groups/{group_id}/columns"
        max_retries = 10
        columns = None
        
        for retry in range(max_retries):
            if is_task_stopped(task_id):
                break
            
            try:
                resp = requests.get(columns_url, headers=headers, timeout=30)
                request_count += 1
            except Exception as req_err:
                log_exception(f"è·å–ä¸“æ ç›®å½•è¯·æ±‚å¼‚å¸¸: group_id={group_id}, url={columns_url}")
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    add_task_log(task_id, f"   âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                    await asyncio.sleep(wait_time)
                    continue
                raise Exception(f"è·å–ä¸“æ ç›®å½•è¯·æ±‚å¼‚å¸¸: {req_err}")
            
            if resp.status_code != 200:
                log_error(f"è·å–ä¸“æ ç›®å½•å¤±è´¥: group_id={group_id}, HTTP {resp.status_code}, response={resp.text[:500] if resp.text else 'empty'}")
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    add_task_log(task_id, f"   âš ï¸ HTTP {resp.status_code}ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                    await asyncio.sleep(wait_time)
                    continue
                raise Exception(f"è·å–ä¸“æ ç›®å½•å¤±è´¥: HTTP {resp.status_code}")
            
            try:
                data = resp.json()
            except Exception as json_err:
                log_exception(f"è§£æä¸“æ ç›®å½•JSONå¤±è´¥: group_id={group_id}, response={resp.text[:500] if resp.text else 'empty'}")
                raise Exception(f"è§£æä¸“æ ç›®å½•å¤±è´¥: {json_err}")
                
            if not data.get('succeeded'):
                error_code = data.get('code')
                error_msg = data.get('error_message', 'æœªçŸ¥é”™è¯¯')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸ
                if 'expired' in error_msg.lower() or data.get('resp_data', {}).get('expired'):
                    raise Exception(f"ä¼šå‘˜å·²è¿‡æœŸ: {error_msg}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
                if error_code == 1059:
                    if retry < max_retries - 1:
                        wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                        add_task_log(task_id, f"   âš ï¸ é‡åˆ°åçˆ¬æœºåˆ¶ (é”™è¯¯ç 1059)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        log_error(f"è·å–ä¸“æ ç›®å½•é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: group_id={group_id}, code={error_code}")
                        raise Exception(f"è·å–ä¸“æ ç›®å½•å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é‡åˆ°åçˆ¬é™åˆ¶")
                else:
                    log_error(f"è·å–ä¸“æ ç›®å½•APIå¤±è´¥: group_id={group_id}, code={error_code}, message={error_msg}, response={json.dumps(data, ensure_ascii=False)[:500]}")
                    raise Exception(f"APIè¿”å›å¤±è´¥: {error_msg} (code={error_code})")
            else:
                # æˆåŠŸè·å–
                columns = data.get('resp_data', {}).get('columns', [])
                if retry > 0:
                    add_task_log(task_id, f"   âœ… é‡è¯•æˆåŠŸ (ç¬¬{retry+1}æ¬¡å°è¯•)")
                break
        
        if columns is None:
            raise Exception("è·å–ä¸“æ ç›®å½•å¤±è´¥")
        add_task_log(task_id, f"âœ… è·å–åˆ° {len(columns)} ä¸ªä¸“æ ç›®å½•")
        
        if len(columns) == 0:
            add_task_log(task_id, "â„¹ï¸ è¯¥ç¾¤ç»„æ²¡æœ‰ä¸“æ å†…å®¹")
            update_task(task_id, "completed", "è¯¥ç¾¤ç»„æ²¡æœ‰ä¸“æ å†…å®¹")
            db.close()
            return
        
        # 2. éå†æ¯ä¸ªä¸“æ 
        for col_idx, column in enumerate(columns, 1):
            if is_task_stopped(task_id):
                add_task_log(task_id, "ğŸ›‘ ä»»åŠ¡å·²è¢«ç”¨æˆ·åœæ­¢")
                break
            
            column_id = column.get('column_id')
            column_name = column.get('name', 'æœªå‘½å')
            column_topics_count = column.get('statistics', {}).get('topics_count', 0)
            db.insert_column(int(group_id), column)
            columns_count += 1
            
            add_task_log(task_id, "")
            add_task_log(task_id, f"ğŸ“ [{col_idx}/{len(columns)}] ä¸“æ : {column_name}")
            add_task_log(task_id, f"   ğŸ“Š é¢„è®¡æ–‡ç« æ•°: {column_topics_count}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é•¿ä¼‘çœ 
            if request_count > 0 and request_count % items_per_batch == 0:
                sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                add_task_log(task_id, f"   ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                await asyncio.sleep(sleep_time)
            
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(crawl_interval_min, crawl_interval_max)
            add_task_log(task_id, f"   â³ ç­‰å¾… {delay:.1f} ç§’åè·å–æ–‡ç« åˆ—è¡¨...")
            await asyncio.sleep(delay)
            
            # è·å–ä¸“æ æ–‡ç« åˆ—è¡¨
            topics_url = f"https://api.zsxq.com/v2/groups/{group_id}/columns/{column_id}/topics?count=100&sort=default&direction=desc"
            try:
                topics_resp = requests.get(topics_url, headers=headers, timeout=30)
                request_count += 1
            except Exception as req_err:
                log_exception(f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨è¯·æ±‚å¼‚å¸¸: column_id={column_id}, url={topics_url}")
                add_task_log(task_id, f"   âš ï¸ è·å–æ–‡ç« åˆ—è¡¨è¯·æ±‚å¼‚å¸¸: {req_err}")
                continue
            
            if topics_resp.status_code != 200:
                log_error(f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨å¤±è´¥: column_id={column_id}, HTTP {topics_resp.status_code}, response={topics_resp.text[:500] if topics_resp.text else 'empty'}")
                add_task_log(task_id, f"   âš ï¸ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: HTTP {topics_resp.status_code}")
                continue
            
            try:
                topics_data = topics_resp.json()
            except Exception as json_err:
                log_exception(f"è§£æä¸“æ æ–‡ç« åˆ—è¡¨JSONå¤±è´¥: column_id={column_id}, response={topics_resp.text[:500] if topics_resp.text else 'empty'}")
                add_task_log(task_id, f"   âš ï¸ è§£ææ–‡ç« åˆ—è¡¨å¤±è´¥: {json_err}")
                continue
                
            if not topics_data.get('succeeded'):
                error_code = topics_data.get('code', 'unknown')
                error_message = topics_data.get('error_message', 'æœªçŸ¥é”™è¯¯')
                log_error(f"è·å–ä¸“æ æ–‡ç« åˆ—è¡¨å¤±è´¥: column_id={column_id}, code={error_code}, message={error_message}")
                add_task_log(task_id, f"   âš ï¸ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {error_message} (code={error_code})")
                continue
            
            topics_list = topics_data.get('resp_data', {}).get('topics', [])
            add_task_log(task_id, f"   ğŸ“ è·å–åˆ° {len(topics_list)} ç¯‡æ–‡ç« ")
            
            # 3. éå†æ¯ç¯‡æ–‡ç« 
            for topic_idx, topic in enumerate(topics_list, 1):
                if is_task_stopped(task_id):
                    break
                
                topic_id = topic.get('topic_id')
                topic_title = topic.get('title', 'æ— æ ‡é¢˜')[:30]
                db.insert_column_topic(column_id, int(group_id), topic)
                topics_count += 1
                
                # å¢é‡æ¨¡å¼ï¼šæ£€æŸ¥æ–‡ç« è¯¦æƒ…æ˜¯å¦å·²å­˜åœ¨
                if incremental_mode and db.topic_detail_exists(topic_id):
                    add_task_log(task_id, f"   ğŸ“„ [{topic_idx}/{len(topics_list)}] {topic_title}... â­ï¸ è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰")
                    skipped_count += 1
                    continue
                
                add_task_log(task_id, f"   ğŸ“„ [{topic_idx}/{len(topics_list)}] {topic_title}...")
                
                # è·å–æ–‡ç« è¯¦æƒ…ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                max_retries = 10
                topic_detail = None
                
                for retry in range(max_retries):
                    if is_task_stopped(task_id):
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é•¿ä¼‘çœ 
                    if request_count > 0 and request_count % items_per_batch == 0:
                        sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                        add_task_log(task_id, f"      ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                        await asyncio.sleep(sleep_time)
                    
                    # éšæœºå»¶è¿Ÿ
                    delay = random.uniform(crawl_interval_min, crawl_interval_max)
                    await asyncio.sleep(delay)
                    
                    # è·å–æ–‡ç« è¯¦æƒ…
                    detail_url = f"https://api.zsxq.com/v2/topics/{topic_id}/info"
                    try:
                        detail_resp = requests.get(detail_url, headers=headers, timeout=30)
                        request_count += 1
                    except Exception as req_err:
                        log_exception(f"è·å–æ–‡ç« è¯¦æƒ…è¯·æ±‚å¼‚å¸¸: topic_id={topic_id}, url={detail_url}")
                        add_task_log(task_id, f"      âš ï¸ è·å–è¯¦æƒ…è¯·æ±‚å¼‚å¸¸: {req_err}")
                        continue
                    
                    if detail_resp.status_code != 200:
                        log_error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: topic_id={topic_id}, HTTP {detail_resp.status_code}, response={detail_resp.text[:500] if detail_resp.text else 'empty'}")
                        add_task_log(task_id, f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: HTTP {detail_resp.status_code}")
                        continue
                    
                    try:
                        detail_data = detail_resp.json()
                    except Exception as json_err:
                        log_exception(f"è§£ææ–‡ç« è¯¦æƒ…JSONå¤±è´¥: topic_id={topic_id}, response={detail_resp.text[:500] if detail_resp.text else 'empty'}")
                        add_task_log(task_id, f"      âš ï¸ è§£æè¯¦æƒ…å¤±è´¥: {json_err}")
                        continue
                        
                    if not detail_data.get('succeeded'):
                        error_code = detail_data.get('code')
                        error_message = detail_data.get('error_message', 'æœªçŸ¥é”™è¯¯')
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
                        if error_code == 1059:
                            if retry < max_retries - 1:
                                # æ™ºèƒ½ç­‰å¾…æ—¶é—´ç­–ç•¥ï¼šå‰å‡ æ¬¡çŸ­ç­‰å¾…ï¼Œåé¢é€æ¸å¢åŠ 
                                if retry < 3:
                                    wait_time = 2  # å‰3æ¬¡ç­‰å¾…2ç§’
                                elif retry < 6:
                                    wait_time = 5  # ç¬¬4-6æ¬¡ç­‰å¾…5ç§’
                                else:
                                    wait_time = 10  # ç¬¬7-10æ¬¡ç­‰å¾…10ç§’
                                
                                add_task_log(task_id, f"      âš ï¸ é‡åˆ°åçˆ¬æœºåˆ¶ (é”™è¯¯ç 1059)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• ({retry+1}/{max_retries})")
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                log_error(f"è·å–æ–‡ç« è¯¦æƒ…é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: topic_id={topic_id}, code={error_code}, message={error_message}")
                                add_task_log(task_id, f"      âŒ é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {error_message} (code={error_code})")
                                break
                        else:
                            log_error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: topic_id={topic_id}, code={error_code}, message={error_message}, full_response={json.dumps(detail_data, ensure_ascii=False)[:500]}")
                            add_task_log(task_id, f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: {error_message} (code={error_code})")
                            break
                    else:
                        # æˆåŠŸè·å–è¯¦æƒ…
                        topic_detail = detail_data.get('resp_data', {}).get('topic', {})
                        if retry > 0:
                            add_task_log(task_id, f"      âœ… é‡è¯•æˆåŠŸ (ç¬¬{retry+1}æ¬¡å°è¯•)")
                        break
                
                # å¦‚æœæ²¡æœ‰è·å–åˆ°è¯¦æƒ…ï¼Œè·³è¿‡åç»­å¤„ç†
                if not topic_detail:
                    continue
                db.insert_topic_detail(int(group_id), topic_detail, json.dumps(topic_detail, ensure_ascii=False))
                details_count += 1
                
                # å¤„ç†æ–‡ä»¶ä¸‹è½½
                if download_files:
                    talk = topic_detail.get('talk', {})
                    topic_files = talk.get('files', [])
                    content_voice = topic_detail.get('content_voice')
                    
                    all_files = topic_files.copy()
                    if content_voice:
                        all_files.append(content_voice)
                    
                    for file_info in all_files:
                        if is_task_stopped(task_id):
                            break
                        
                        file_id = file_info.get('file_id')
                        file_name = file_info.get('name', '')
                        file_size = file_info.get('size', 0)
                        
                        if file_id:
                            add_task_log(task_id, f"      ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {file_name[:40]}...")
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦é•¿ä¼‘çœ 
                            if request_count > 0 and request_count % items_per_batch == 0:
                                sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                                add_task_log(task_id, f"      ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                                await asyncio.sleep(sleep_time)
                            
                            delay = random.uniform(crawl_interval_min, crawl_interval_max)
                            await asyncio.sleep(delay)
                            
                            try:
                                result = await _download_column_file(
                                    group_id, file_id, file_name, file_size,
                                    topic_id, db, headers, task_id
                                )
                                if result == "downloaded":
                                    files_count += 1
                                    request_count += 1
                                    add_task_log(task_id, f"         âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ")
                                elif result == "skipped":
                                    files_skipped += 1
                                # "skipped" æ—¶æ—¥å¿—å·²åœ¨å‡½æ•°å†…è¾“å‡º
                            except Exception as fe:
                                log_exception(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: file_id={file_id}, file_name={file_name}, topic_id={topic_id}")
                                add_task_log(task_id, f"         âš ï¸ æ–‡ä»¶ä¸‹è½½å¤±è´¥: {fe}")
                
                # ç¼“å­˜å›¾ç‰‡
                if cache_images:
                    talk = topic_detail.get('talk', {}) if 'talk' in topic_detail else {}
                    topic_images = talk.get('images', [])
                    
                    for image in topic_images:
                        if is_task_stopped(task_id):
                            break
                        
                        original_url = image.get('original', {}).get('url')
                        image_id = image.get('image_id')
                        
                        if original_url and image_id:
                            try:
                                cache_manager = get_image_cache_manager(group_id)
                                success, local_path, error_msg = cache_manager.download_and_cache(original_url)
                                if success and local_path:
                                    db.update_image_local_path(image_id, str(local_path))
                                    images_count += 1
                                elif error_msg:
                                    add_task_log(task_id, f"      âš ï¸ å›¾ç‰‡ç¼“å­˜å¤±è´¥: {error_msg}")
                            except Exception as ie:
                                log_exception(f"å›¾ç‰‡ç¼“å­˜å¤±è´¥: image_id={image_id}, url={original_url}")
                                add_task_log(task_id, f"      âš ï¸ å›¾ç‰‡ç¼“å­˜å¤±è´¥: {ie}")
                
                # å¤„ç†è§†é¢‘
                talk_for_video = topic_detail.get('talk', {}) if 'talk' in topic_detail else {}
                video = talk_for_video.get('video')
                
                if video and video.get('video_id'):
                    video_id = video.get('video_id')
                    video_size = video.get('size', 0)
                    video_duration = video.get('duration', 0)
                    cover = video.get('cover', {})
                    cover_url = cover.get('url')
                    
                    add_task_log(task_id, f"      ğŸ¬ å‘ç°è§†é¢‘: ID={video_id}, å¤§å°={video_size/(1024*1024):.1f}MB, æ—¶é•¿={video_duration}ç§’")
                    
                    # ç¼“å­˜è§†é¢‘å°é¢ï¼ˆè·Ÿéšå›¾ç‰‡ç¼“å­˜é€‰é¡¹ï¼‰
                    if cache_images and cover_url:
                        try:
                            cache_manager = get_image_cache_manager(group_id)
                            success, cover_local, error_msg = cache_manager.download_and_cache(cover_url)
                            if success and cover_local:
                                db.update_video_cover_path(video_id, str(cover_local))
                                add_task_log(task_id, f"      âœ… è§†é¢‘å°é¢ç¼“å­˜æˆåŠŸ")
                            elif error_msg:
                                log_warning(f"è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: video_id={video_id}, url={cover_url}, error={error_msg}")
                                add_task_log(task_id, f"      âš ï¸ è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: {error_msg}")
                        except Exception as ve:
                            log_exception(f"è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: video_id={video_id}, url={cover_url}")
                            add_task_log(task_id, f"      âš ï¸ è§†é¢‘å°é¢ç¼“å­˜å¤±è´¥: {ve}")
                    
                    # ä¸‹è½½è§†é¢‘ï¼ˆå•ç‹¬æ§åˆ¶ï¼‰
                    if download_videos:
                        if request_count > 0 and request_count % items_per_batch == 0:
                            sleep_time = random.uniform(long_sleep_min, long_sleep_max)
                            add_task_log(task_id, f"      ğŸ˜´ å·²å®Œæˆ {request_count} æ¬¡è¯·æ±‚ï¼Œä¼‘çœ  {sleep_time:.1f} ç§’...")
                            await asyncio.sleep(sleep_time)
                        
                        delay = random.uniform(crawl_interval_min, crawl_interval_max)
                        await asyncio.sleep(delay)
                        
                        try:
                            result = await _download_column_video(
                                group_id, video_id, video_size, video_duration,
                                topic_id, db, headers, task_id
                            )
                            if result == "downloaded":
                                videos_count += 1
                                request_count += 1
                            elif result == "skipped":
                                videos_skipped += 1
                            # "skipped" æ—¶æ—¥å¿—å·²åœ¨å‡½æ•°å†…è¾“å‡º
                        except Exception as ve:
                            log_exception(f"è§†é¢‘ä¸‹è½½å¤±è´¥: video_id={video_id}, topic_id={topic_id}, size={video_size}")
                            add_task_log(task_id, f"      âš ï¸ è§†é¢‘ä¸‹è½½å¤±è´¥: {ve}")
                    else:
                        add_task_log(task_id, f"      â­ï¸ è·³è¿‡è§†é¢‘ä¸‹è½½ï¼ˆå·²ç¦ç”¨ï¼‰")
                
                # æ›´æ–°è¿›åº¦
                update_task(task_id, "running", f"è¿›åº¦: {details_count} ç¯‡æ–‡ç« , {files_count} ä¸ªæ–‡ä»¶, {videos_count} ä¸ªè§†é¢‘, {images_count} å¼ å›¾ç‰‡")
        
        # å®Œæˆ
        add_task_log(task_id, "")
        add_task_log(task_id, "=" * 50)
        add_task_log(task_id, "ğŸ‰ ä¸“æ é‡‡é›†å®Œæˆï¼")
        add_task_log(task_id, f"ğŸ“Š ç»Ÿè®¡:")
        add_task_log(task_id, f"   ğŸ“ ä¸“æ ç›®å½•: {columns_count} ä¸ª")
        add_task_log(task_id, f"   ğŸ“ æ–‡ç« åˆ—è¡¨: {topics_count} ç¯‡")
        add_task_log(task_id, f"   ğŸ“„ æ–‡ç« è¯¦æƒ…: {details_count} ç¯‡ï¼ˆæ–°å¢ï¼‰")
        if skipped_count > 0:
            add_task_log(task_id, f"   â­ï¸ è·³è¿‡å·²å­˜åœ¨æ–‡ç« : {skipped_count} ç¯‡")
        add_task_log(task_id, f"   ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {files_count} ä¸ª" + (f" (è·³è¿‡ {files_skipped} ä¸ªå·²å­˜åœ¨)" if files_skipped > 0 else ""))
        add_task_log(task_id, f"   ğŸ¬ ä¸‹è½½è§†é¢‘: {videos_count} ä¸ª" + (f" (è·³è¿‡ {videos_skipped} ä¸ªå·²å­˜åœ¨)" if videos_skipped > 0 else ""))
        add_task_log(task_id, f"   ğŸ–¼ï¸ ç¼“å­˜å›¾ç‰‡: {images_count} å¼ ")
        add_task_log(task_id, f"   ğŸ“¡ æ€»è¯·æ±‚æ•°: {request_count} æ¬¡")
        add_task_log(task_id, "=" * 50)
        
        db.update_crawl_log(log_id, columns_count=columns_count, topics_count=topics_count,
                          details_count=details_count, files_count=files_count, status='completed')
        db.close()
        
        skipped_info = f", è·³è¿‡ {skipped_count} ç¯‡" if skipped_count > 0 else ""
        result_msg = f"é‡‡é›†å®Œæˆ: {columns_count} ä¸ªä¸“æ , {details_count} ç¯‡æ–°æ–‡ç« {skipped_info}, {files_count} ä¸ªæ–‡ä»¶, {videos_count} ä¸ªè§†é¢‘"
        update_task(task_id, "completed", result_msg)
        
    except Exception as e:
        error_msg = str(e)
        add_task_log(task_id, "")
        add_task_log(task_id, f"âŒ é‡‡é›†å¤±è´¥: {error_msg}")
        update_task(task_id, "failed", f"é‡‡é›†å¤±è´¥: {error_msg}")
        
        try:
            if db and log_id:
                db.update_crawl_log(log_id, status='failed', error_message=error_msg)
                db.close()
        except:
            pass


async def _download_column_file(group_id: str, file_id: int, file_name: str, file_size: int,
                                topic_id: int, db: ZSXQColumnsDatabase, headers: dict, task_id: str = None) -> str:
    """ä¸‹è½½ä¸“æ æ–‡ä»¶
    
    Returns:
        str: "downloaded" è¡¨ç¤ºæ–°ä¸‹è½½, "skipped" è¡¨ç¤ºå·²å­˜åœ¨è·³è¿‡, æˆ–æŠ›å‡ºå¼‚å¸¸
    """
    # å…ˆæ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    path_manager = get_db_path_manager()
    group_dir = path_manager.get_group_dir(group_id)
    downloads_dir = os.path.join(group_dir, "column_downloads")
    local_path = os.path.join(downloads_dir, file_name)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”å¤§å°åŒ¹é…ï¼Œè·³è¿‡ä¸‹è½½
    if os.path.exists(local_path):
        existing_size = os.path.getsize(local_path)
        if existing_size == file_size or (file_size == 0 and existing_size > 0):
            db.update_file_download_status(file_id, 'completed', local_path)
            if task_id:
                add_task_log(task_id, f"         â­ï¸ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ ({existing_size/(1024*1024):.2f}MB)")
            return "skipped"
    
    # è·å–ä¸‹è½½URLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    download_url = f"https://api.zsxq.com/v2/files/{file_id}/download_url"
    max_retries = 10
    real_url = None
    
    for retry in range(max_retries):
        try:
            resp = requests.get(download_url, headers=headers, timeout=30)
        except Exception as req_err:
            if retry < max_retries - 1:
                wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                await asyncio.sleep(wait_time)
                continue
            log_exception(f"è·å–ä¸‹è½½é“¾æ¥è¯·æ±‚å¼‚å¸¸: file_id={file_id}")
            raise Exception(f"è·å–ä¸‹è½½é“¾æ¥è¯·æ±‚å¼‚å¸¸: {req_err}")
        
        if resp.status_code != 200:
            if retry < max_retries - 1:
                wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                await asyncio.sleep(wait_time)
                continue
            error_msg = f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: HTTP {resp.status_code}, URL={download_url}, Response={resp.text[:500] if resp.text else 'empty'}"
            log_error(error_msg)
            raise Exception(error_msg)
        
        data = resp.json()
        if not data.get('succeeded'):
            error_code = data.get('code')
            error_message = data.get('error_message', 'æœªçŸ¥é”™è¯¯')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
            if error_code == 1059:
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    log_error(f"è·å–ä¸‹è½½é“¾æ¥é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: file_id={file_id}, code={error_code}")
                    raise Exception(f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é‡åˆ°åçˆ¬é™åˆ¶")
            else:
                error_msg = f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: code={error_code}, message={error_message}, file_id={file_id}, file_name={file_name}"
                log_error(error_msg)
                raise Exception(f"è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {error_message} (code={error_code})")
        else:
            real_url = data.get('resp_data', {}).get('download_url')
            break
    
    if not real_url:
        raise Exception("ä¸‹è½½é“¾æ¥ä¸ºç©º")
    
    # åˆ›å»ºä¸‹è½½ç›®å½•ï¼ˆdownloads_dir å’Œ local_path åœ¨å‡½æ•°å¼€å¤´å·²å®šä¹‰ï¼‰
    os.makedirs(downloads_dir, exist_ok=True)
    
    # ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œå¤„ç† SSL é”™è¯¯ç­‰ç½‘ç»œé—®é¢˜ï¼‰
    download_retries = 3
    last_error = None
    
    for download_attempt in range(download_retries):
        try:
            file_resp = requests.get(real_url, headers=headers, stream=True, timeout=300)
            if file_resp.status_code == 200:
                with open(local_path, 'wb') as f:
                    for chunk in file_resp.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                db.update_file_download_status(file_id, 'completed', local_path)
                return "downloaded"
            else:
                last_error = f"HTTP {file_resp.status_code}"
                if download_attempt < download_retries - 1:
                    log_warning(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥ (å°è¯• {download_attempt + 1}/{download_retries}): {last_error}, file_id={file_id}")
                    await asyncio.sleep(2 * (download_attempt + 1))  # é€’å¢ç­‰å¾…
                    continue
        except requests.exceptions.SSLError as ssl_err:
            last_error = f"SSLé”™è¯¯: {ssl_err}"
            if download_attempt < download_retries - 1:
                log_warning(f"æ–‡ä»¶ä¸‹è½½SSLé”™è¯¯ (å°è¯• {download_attempt + 1}/{download_retries}): file_id={file_id}, error={ssl_err}")
                await asyncio.sleep(3 * (download_attempt + 1))  # SSLé”™è¯¯ç­‰å¾…æ›´ä¹…
                continue
        except requests.exceptions.RequestException as req_err:
            last_error = f"ç½‘ç»œé”™è¯¯: {req_err}"
            if download_attempt < download_retries - 1:
                log_warning(f"æ–‡ä»¶ä¸‹è½½ç½‘ç»œé”™è¯¯ (å°è¯• {download_attempt + 1}/{download_retries}): file_id={file_id}, error={req_err}")
                await asyncio.sleep(2 * (download_attempt + 1))
                continue
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    db.update_file_download_status(file_id, 'failed')
    raise Exception(f"ä¸‹è½½å¤±è´¥ (é‡è¯•{download_retries}æ¬¡): {last_error}")


async def _download_column_video(group_id: str, video_id: int, video_size: int, video_duration: int,
                                 topic_id: int, db: ZSXQColumnsDatabase, headers: dict, task_id: str = None) -> str:
    """ä¸‹è½½ä¸“æ è§†é¢‘ï¼ˆm3u8æ ¼å¼ï¼‰
    
    Returns:
        str: "downloaded" è¡¨ç¤ºæ–°ä¸‹è½½, "skipped" è¡¨ç¤ºå·²å­˜åœ¨è·³è¿‡, æˆ–æŠ›å‡ºå¼‚å¸¸
    """
    import subprocess
    import re
    
    # å…ˆæ£€æŸ¥æœ¬åœ°è§†é¢‘æ˜¯å¦å·²å­˜åœ¨
    path_manager = get_db_path_manager()
    group_dir = path_manager.get_group_dir(group_id)
    videos_dir = os.path.join(group_dir, "column_videos")
    video_filename = f"video_{video_id}.mp4"
    local_path = os.path.join(videos_dir, video_filename)
    
    # å¦‚æœè§†é¢‘å·²å­˜åœ¨ä¸”å¤§å°>0ï¼Œè·³è¿‡ä¸‹è½½
    if os.path.exists(local_path):
        existing_size = os.path.getsize(local_path)
        if existing_size > 0:
            db.update_video_download_status(video_id, 'completed', '', local_path)
            if task_id:
                add_task_log(task_id, f"         â­ï¸ è§†é¢‘å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ ({existing_size/(1024*1024):.1f}MB)")
            return "skipped"
    
    # è·å–è§†é¢‘URLï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    video_url_api = f"https://api.zsxq.com/v2/videos/{video_id}/url"
    max_retries = 10
    m3u8_url = None
    
    for retry in range(max_retries):
        try:
            resp = requests.get(video_url_api, headers=headers, timeout=30)
        except Exception as req_err:
            if retry < max_retries - 1:
                wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                await asyncio.sleep(wait_time)
                continue
            log_exception(f"è·å–è§†é¢‘é“¾æ¥è¯·æ±‚å¼‚å¸¸: video_id={video_id}")
            raise Exception(f"è·å–è§†é¢‘é“¾æ¥è¯·æ±‚å¼‚å¸¸: {req_err}")
        
        if resp.status_code != 200:
            if retry < max_retries - 1:
                wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                await asyncio.sleep(wait_time)
                continue
            error_msg = f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥: HTTP {resp.status_code}, URL={video_url_api}, Response={resp.text[:500] if resp.text else 'empty'}"
            log_error(error_msg)
            raise Exception(error_msg)
        
        data = resp.json()
        if not data.get('succeeded'):
            error_code = data.get('code')
            error_message = data.get('error_message', 'æœªçŸ¥é”™è¯¯')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç  1059ï¼Œéœ€è¦é‡è¯•
            if error_code == 1059:
                if retry < max_retries - 1:
                    wait_time = 2 if retry < 3 else (5 if retry < 6 else 10)
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    log_error(f"è·å–è§†é¢‘é“¾æ¥é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: video_id={video_id}, code={error_code}")
                    raise Exception(f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»é‡åˆ°åçˆ¬é™åˆ¶")
            else:
                error_msg = f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥: code={error_code}, message={error_message}, video_id={video_id}, topic_id={topic_id}"
                log_error(error_msg)
                raise Exception(f"è·å–è§†é¢‘é“¾æ¥å¤±è´¥: {error_message} (code={error_code})")
        else:
            m3u8_url = data.get('resp_data', {}).get('url')
            break
    
    if not m3u8_url:
        raise Exception("è§†é¢‘é“¾æ¥ä¸ºç©º")
    
    # åˆ›å»ºè§†é¢‘ä¸‹è½½ç›®å½•ï¼ˆvideos_dir å’Œ local_path åœ¨å‡½æ•°å¼€å¤´å·²å®šä¹‰ï¼‰
    os.makedirs(videos_dir, exist_ok=True)
    
    # æ›´æ–°çŠ¶æ€ä¸ºä¸‹è½½ä¸­
    db.update_video_download_status(video_id, 'downloading', m3u8_url)
    
    # ä½¿ç”¨ffmpegä¸‹è½½m3u8è§†é¢‘
    try:
        # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
        ffmpeg_check = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)
        if ffmpeg_check.returncode != 0:
            raise Exception("ffmpeg not available")
        
        # æ„å»º HTTP headers å­—ç¬¦ä¸²ç»™ ffmpeg
        # ffmpeg éœ€è¦çš„æ ¼å¼æ˜¯ "Header1: Value1\r\nHeader2: Value2\r\n"
        ffmpeg_headers = ""
        if headers.get('Cookie'):
            ffmpeg_headers += f"Cookie: {headers['Cookie']}\r\n"
        if headers.get('cookie'):
            ffmpeg_headers += f"Cookie: {headers['cookie']}\r\n"
        ffmpeg_headers += "Referer: https://wx.zsxq.com/\r\n"
        ffmpeg_headers += "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\r\n"
        ffmpeg_headers += "Origin: https://wx.zsxq.com\r\n"
        
        # ä½¿ç”¨ffmpegä¸‹è½½ï¼ˆå¸¦è¯·æ±‚å¤´å’Œè¿›åº¦æ˜¾ç¤ºï¼‰
        cmd = [
            'ffmpeg', '-y',
            '-headers', ffmpeg_headers,
            '-i', m3u8_url,
            '-c', 'copy',
            '-bsf:a', 'aac_adtstoasc',
            '-progress', 'pipe:1',  # è¾“å‡ºè¿›åº¦ä¿¡æ¯åˆ° stdout
            local_path
        ]
        
        log_info(f"å¼€å§‹ä¸‹è½½è§†é¢‘: video_id={video_id}, url={m3u8_url[:100]}...")
        if task_id:
            add_task_log(task_id, f"         ğŸ¬ å¼€å§‹ä¸‹è½½è§†é¢‘ (é¢„è®¡æ—¶é•¿: {video_duration}ç§’, å¤§å°: {video_size/(1024*1024):.1f}MB)")
        
        # ä½¿ç”¨ Popen å®æ—¶è¯»å–è¿›åº¦
        # åœ¨ Windows ä¸Šéœ€è¦ç‰¹æ®Šå¤„ç†ç®¡é“ç¼“å†²
        import threading
        import queue
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1
        )
        
        stderr_output = []
        stdout_queue = queue.Queue()
        
        # ä½¿ç”¨çº¿ç¨‹è¯»å– stdoutï¼Œé¿å…é˜»å¡
        def read_stdout():
            try:
                for line in iter(process.stdout.readline, ''):
                    if line:
                        stdout_queue.put(line)
                    if process.poll() is not None:
                        break
            except:
                pass
        
        # ä½¿ç”¨çº¿ç¨‹è¯»å– stderr
        def read_stderr():
            try:
                for line in iter(process.stderr.readline, ''):
                    if line:
                        stderr_output.append(line)
            except:
                pass
        
        stdout_thread = threading.Thread(target=read_stdout, daemon=True)
        stderr_thread = threading.Thread(target=read_stderr, daemon=True)
        stdout_thread.start()
        stderr_thread.start()
        
        last_log_time = time.time()
        start_time = time.time()
        
        # è¯»å–è¿›åº¦ä¿¡æ¯
        try:
            while process.poll() is None:
                # éé˜»å¡æ–¹å¼è·å–è¿›åº¦
                try:
                    line = stdout_queue.get(timeout=1)
                    
                    # è§£æ ffmpeg è¿›åº¦ä¿¡æ¯
                    # æ ¼å¼: out_time_ms=123456789
                    if line.startswith('out_time_ms='):
                        try:
                            time_ms = int(line.split('=')[1].strip())
                            current_seconds = time_ms / 1000000
                            
                            # æ¯ 3 ç§’æ›´æ–°ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…åˆ·å±
                            now = time.time()
                            if task_id and (now - last_log_time) >= 3:
                                if video_duration > 0:
                                    progress_pct = min(100, (current_seconds / video_duration) * 100)
                                    # ç”Ÿæˆè¿›åº¦æ¡
                                    bar_length = 20
                                    filled = int(bar_length * progress_pct / 100)
                                    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                                    add_task_log(task_id, f"         ğŸ“Š ä¸‹è½½è¿›åº¦: [{bar}] {progress_pct:.1f}% ({current_seconds:.0f}s/{video_duration}s)")
                                else:
                                    add_task_log(task_id, f"         ğŸ“Š ä¸‹è½½è¿›åº¦: {current_seconds:.0f}ç§’")
                                last_log_time = now
                        except:
                            pass
                except queue.Empty:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºç­‰å¾…ä¸­çš„è¿›åº¦
                    now = time.time()
                    elapsed = now - start_time
                    if task_id and (now - last_log_time) >= 5:
                        add_task_log(task_id, f"         â³ ä¸‹è½½ä¸­... (å·²ç”¨æ—¶ {elapsed:.0f}ç§’)")
                        last_log_time = now
                    continue
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            stdout_thread.join(timeout=5)
            stderr_thread.join(timeout=5)
                
        except Exception as e:
            process.kill()
            raise Exception(f"è§†é¢‘ä¸‹è½½å¼‚å¸¸: {e}")
        
        returncode = process.returncode
        stderr_text = ''.join(stderr_output)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸä¸‹è½½ï¼ˆffmpeg å¯èƒ½è¿”å›é 0 ä½†æ–‡ä»¶å·²æˆåŠŸä¸‹è½½ï¼‰
        if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
            db.update_video_download_status(video_id, 'completed', m3u8_url, local_path)
            final_size = os.path.getsize(local_path)
            log_info(f"è§†é¢‘ä¸‹è½½æˆåŠŸ: video_id={video_id}, path={local_path}, size={final_size}")
            if task_id:
                add_task_log(task_id, f"         âœ… è§†é¢‘ä¸‹è½½å®Œæˆ ({final_size/(1024*1024):.1f}MB)")
            return "downloaded"
        else:
            db.update_video_download_status(video_id, 'failed', m3u8_url)
            # ä» stderr ä¸­æå–çœŸæ­£çš„é”™è¯¯ä¿¡æ¯ï¼ˆè·³è¿‡ç‰ˆæœ¬ä¿¡æ¯ç­‰ï¼‰
            stderr_lines = stderr_text.strip().split('\n')
            # æŸ¥æ‰¾åŒ…å« "error" æˆ– "failed" çš„è¡Œ
            error_lines = [line for line in stderr_lines if 'error' in line.lower() or 'failed' in line.lower() or 'invalid' in line.lower()]
            if error_lines:
                error_msg = '; '.join(error_lines[-3:])  # å–æœ€å 3 æ¡é”™è¯¯
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°æ˜ç¡®é”™è¯¯ï¼Œå–æœ€åå‡ è¡Œ
                error_msg = '; '.join(stderr_lines[-3:]) if stderr_lines else 'unknown error'
            log_error(f"ffmpegä¸‹è½½å¤±è´¥: video_id={video_id}, returncode={returncode}, error={error_msg}")
            raise Exception(f"ffmpegä¸‹è½½å¤±è´¥: {error_msg[:300]}")
            
    except FileNotFoundError:
        # ffmpegä¸å¯ç”¨ï¼Œä¿å­˜m3u8é“¾æ¥ä¾›æ‰‹åŠ¨ä¸‹è½½
        db.update_video_download_status(video_id, 'pending_manual', m3u8_url)
        # ä¿å­˜m3u8é“¾æ¥åˆ°æ–‡ä»¶
        m3u8_link_file = os.path.join(videos_dir, f"video_{video_id}.m3u8.txt")
        with open(m3u8_link_file, 'w', encoding='utf-8') as f:
            f.write(f"Video ID: {video_id}\n")
            f.write(f"Duration: {video_duration} seconds\n")
            f.write(f"Size: {video_size} bytes\n")
            f.write(f"M3U8 URL: {m3u8_url}\n")
        raise Exception("ffmpegæœªå®‰è£…ï¼Œå·²ä¿å­˜m3u8é“¾æ¥åˆ°æ–‡ä»¶ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½")
    except subprocess.TimeoutExpired:
        db.update_video_download_status(video_id, 'failed', m3u8_url)
        raise Exception("è§†é¢‘ä¸‹è½½è¶…æ—¶")


@app.get("/api/groups/{group_id}/columns/stats")
async def get_columns_stats(group_id: str):
    """è·å–ä¸“æ ç»Ÿè®¡ä¿¡æ¯"""
    try:
        db = get_columns_db(group_id)
        stats = db.get_stats(int(group_id))
        db.close()
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä¸“æ ç»Ÿè®¡å¤±è´¥: {str(e)}")


@app.delete("/api/groups/{group_id}/columns/all")
async def delete_all_columns(group_id: str):
    """åˆ é™¤ç¾¤ç»„çš„æ‰€æœ‰ä¸“æ æ•°æ®"""
    try:
        db = get_columns_db(group_id)
        stats = db.clear_all_data(int(group_id))
        db.close()
        return {
            "success": True,
            "message": f"å·²æ¸…ç©ºä¸“æ æ•°æ®",
            "deleted": stats
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ä¸“æ æ•°æ®å¤±è´¥: {str(e)}")


@app.get("/api/groups/{group_id}/columns/topics/{topic_id}/comments")
async def get_column_topic_full_comments(group_id: str, topic_id: int):
    """è·å–ä¸“æ æ–‡ç« çš„å®Œæ•´è¯„è®ºåˆ—è¡¨ï¼ˆä»APIå®æ—¶è·å–å¹¶æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼‰"""
    try:
        # è·å–è¯¥ç¾¤ç»„ä½¿ç”¨çš„è´¦å·
        manager = get_accounts_sql_manager()
        account = manager.get_account_for_group(group_id, mask_cookie=False)
        if not account or not account.get('cookie'):
            raise HTTPException(status_code=400, detail="No valid account found for this group")

        cookie = account['cookie']
        # ä½¿ç”¨ä¸ä¸“æ é‡‡é›†ç›¸åŒçš„è¯·æ±‚å¤´æ„å»ºæ–¹å¼
        headers = build_stealth_headers(cookie)

        # è·å–å®Œæ•´è¯„è®ºï¼ˆå‚æ•°ä¸å®˜ç½‘ä¸€è‡´ï¼‰
        comments_url = f"https://api.zsxq.com/v2/topics/{topic_id}/comments?sort=asc&count=30&with_sticky=true"
        log_info(f"Fetching comments from: {comments_url}")
        resp = requests.get(comments_url, headers=headers, timeout=30)

        if resp.status_code != 200:
            log_error(f"Failed to fetch comments: HTTP {resp.status_code}, response={resp.text[:500] if resp.text else 'empty'}")
            raise HTTPException(status_code=resp.status_code, detail=f"Failed to fetch comments: HTTP {resp.status_code}")

        data = resp.json()
        log_debug(f"Comments API response: succeeded={data.get('succeeded')}, resp_data keys={list(data.get('resp_data', {}).keys()) if data.get('resp_data') else 'None'}")

        if not data.get('succeeded'):
            # å°è¯•å¤šç§é”™è¯¯æ¶ˆæ¯æ ¼å¼
            resp_data = data.get('resp_data', {})
            error_msg = resp_data.get('message') or resp_data.get('error_msg') or data.get('error_msg') or data.get('message')
            error_code = resp_data.get('code') or resp_data.get('error_code') or data.get('code')
            log_error(f"Comments API failed: code={error_code}, message={error_msg}, full_response={json.dumps(data, ensure_ascii=False)[:500]}")
            raise HTTPException(status_code=400, detail=f"API error: {error_msg or 'Request failed'} (code: {error_code})")

        comments = data.get('resp_data', {}).get('comments', [])

        # å¤„ç†è¯„è®ºæ•°æ®ï¼ŒåŒ…æ‹¬ replied_comments
        processed_comments = []
        for comment in comments:
            processed = {
                'comment_id': comment.get('comment_id'),
                'parent_comment_id': comment.get('parent_comment_id'),
                'text': comment.get('text', ''),
                'create_time': comment.get('create_time'),
                'likes_count': comment.get('likes_count', 0),
                'rewards_count': comment.get('rewards_count', 0),
                'replies_count': comment.get('replies_count', 0),
                'sticky': comment.get('sticky', False),
                'owner': comment.get('owner'),
                'repliee': comment.get('repliee'),
                'images': comment.get('images', []),
            }

            # å¤„ç†åµŒå¥—çš„ replied_comments
            replied_comments = comment.get('replied_comments', [])
            if replied_comments:
                processed['replied_comments'] = [
                    {
                        'comment_id': rc.get('comment_id'),
                        'parent_comment_id': rc.get('parent_comment_id'),
                        'text': rc.get('text', ''),
                        'create_time': rc.get('create_time'),
                        'likes_count': rc.get('likes_count', 0),
                        'owner': rc.get('owner'),
                        'repliee': rc.get('repliee'),
                        'images': rc.get('images', []),
                    }
                    for rc in replied_comments
                ]

            processed_comments.append(processed)

        # æŒä¹…åŒ–è¯„è®ºåˆ°æ•°æ®åº“
        try:
            db = get_columns_db(group_id)
            saved_count = db.import_comments(topic_id, processed_comments)
            db.close()
            log_info(f"Saved {saved_count} comments to database for topic {topic_id}")
        except Exception as e:
            log_error(f"Failed to save comments to database: {e}")
            # ä¸é˜»æ–­æµç¨‹ï¼Œè¯„è®ºä»ç„¶è¿”å›ç»™å‰ç«¯

        # è®¡ç®—æ€»è¯„è®ºæ•°ï¼ˆåŒ…æ‹¬åµŒå¥—å›å¤ï¼‰
        total_count = sum(1 + len(c.get('replied_comments', [])) for c in processed_comments)

        return {
            'success': True,
            'comments': processed_comments,
            'total': total_count
        }

    except HTTPException:
        raise
    except Exception as e:
        log_exception(f"è·å–ä¸“æ å®Œæ•´è¯„è®ºå¤±è´¥: topic_id={topic_id}")
        raise HTTPException(status_code=500, detail=f"è·å–å®Œæ•´è¯„è®ºå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    import sys
    port = 8208  # é»˜è®¤ç«¯å£
    if len(sys.argv) > 2 and sys.argv[1] == "--port":
        try:
            port = int(sys.argv[2])
        except ValueError:
            port = 8208
    uvicorn.run(app, host="0.0.0.0", port=port)
