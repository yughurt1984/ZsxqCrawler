#!/usr/bin/env python3
"""
çŸ¥è¯†æ˜Ÿçƒäº¤äº’å¼æ•°æ®é‡‡é›†å™¨
æ”¯æŒå¤šç§çˆ¬å–æ¨¡å¼ï¼Œå¢å¼ºåæ£€æµ‹æœºåˆ¶
"""

import requests
import time
import random
import json
from typing import Dict, Any, Optional, List
from zsxq_database import ZSXQDatabase
from zsxq_file_downloader import ZSXQFileDownloader
from db_path_manager import get_db_path_manager
import os
import argparse
try:
    import tomllib
except ImportError:
    try:
        import tomli as tomllib
    except ImportError:
        print("âš ï¸ éœ€è¦å®‰è£…tomliåº“æ¥è§£æTOMLé…ç½®æ–‡ä»¶")
        print("ğŸ’¡ è¯·è¿è¡Œ: pip install tomli")
        tomllib = None


class ZSXQInteractiveCrawler:
    """çŸ¥è¯†æ˜Ÿçƒäº¤äº’å¼æ•°æ®é‡‡é›†å™¨"""
    def __init__(self, cookie: str, group_id: str, db_path: str = None, 
             log_callback=None, wecom_webhook_url: str = None, 
             wecom_enabled: bool = True):
        self.cookie = self.clean_cookie(cookie)
        self.group_id = group_id
        self.log_callback = log_callback  # æ—¥å¿—å›è°ƒå‡½æ•°
        self.stop_flag = False  # åœæ­¢æ ‡å¿—
        self.stop_check_func = None  # åœæ­¢æ£€æŸ¥å‡½æ•°

        # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æ•°æ®åº“è·¯å¾„
        path_manager = get_db_path_manager()
        if db_path is None:
            db_path = path_manager.get_topics_db_path(group_id)

        self.db_path = db_path  # ä¿å­˜æ•°æ®åº“è·¯å¾„
        self.db = ZSXQDatabase(db_path)
        self.session = requests.Session()

        # æ–‡ä»¶ä¸‹è½½å™¨ï¼ˆæ‡’åŠ è½½ï¼‰
        self.file_downloader = None
        
        # åˆå§‹åŒ–ä¼ä¸šå¾®ä¿¡webhook
        self.wecom_webhook = None
        if wecom_webhook_url:
            try:
                from wecom_webhook import WeComWebhook
                self.wecom_webhook = WeComWebhook(wecom_webhook_url, enabled=wecom_enabled)
                self.log("ğŸ“± ä¼ä¸šå¾®ä¿¡Webhookå·²å¯ç”¨")
            except ImportError:
                self.log("âš ï¸ æœªæ‰¾åˆ°wecom_webhookæ¨¡å—ï¼Œwebhookæ¨é€åŠŸèƒ½ä¸å¯ç”¨")
            except Exception as e:
                self.log(f"âš ï¸ ä¼ä¸šå¾®ä¿¡Webhookåˆå§‹åŒ–å¤±è´¥: {e}")

        # åŸºç¡€APIé…ç½®
        self.base_url = "https://api.zsxq.com"
        self.api_endpoint = f"/v2/groups/{group_id}/topics"

        # åæ£€æµ‹é…ç½®
        self.request_count = 0
        self.page_count = 0  # æˆåŠŸå¤„ç†çš„é¡µé¢æ•°
        self.last_request_time = 0
        self.min_delay = 2.0  # æœ€å°å»¶è¿Ÿ
        self.max_delay = 5.0  # æœ€å¤§å»¶è¿Ÿ
        self.long_delay_interval = 15  # æ¯15ä¸ªé¡µé¢è¿›è¡Œé•¿å»¶è¿Ÿ
        self.debug_mode = False  # è°ƒè¯•æ¨¡å¼
        self.timestamp_offset_ms = 1  # æ—¶é—´æˆ³å‡å»çš„æ¯«ç§’æ•°

        # å¯é…ç½®çš„é—´éš”å‚æ•°ï¼ˆç”¨äºAPIè°ƒç”¨æ—¶è¦†ç›–é»˜è®¤å€¼ï¼‰
        self.use_custom_intervals = False
        self.custom_min_delay = None
        self.custom_max_delay = None
        self.custom_long_delay_min = None
        self.custom_long_delay_max = None
        self.custom_pages_per_batch = None

        self.log(f"ğŸš€ çŸ¥è¯†æ˜Ÿçƒäº¤äº’å¼é‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆ")
        self.log(f"ğŸ“Š ç›®æ ‡ç¾¤ç»„: {group_id}")
        self.log(f"ğŸ’¾ æ•°æ®åº“: {db_path}")

        # æ˜¾ç¤ºå½“å‰æ•°æ®åº“çŠ¶æ€
        self.show_database_status()

    def set_custom_intervals(self, crawl_interval_min=None, crawl_interval_max=None,
                           long_sleep_interval_min=None, long_sleep_interval_max=None,
                           pages_per_batch=None):
        """è®¾ç½®è‡ªå®šä¹‰é—´éš”å‚æ•°"""
        if any([crawl_interval_min, crawl_interval_max, long_sleep_interval_min,
                long_sleep_interval_max, pages_per_batch]):
            self.use_custom_intervals = True
            self.custom_min_delay = crawl_interval_min
            self.custom_max_delay = crawl_interval_max
            self.custom_long_delay_min = long_sleep_interval_min
            self.custom_long_delay_max = long_sleep_interval_max
            self.custom_pages_per_batch = pages_per_batch

            self.log(f"ğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰é—´éš”è®¾ç½®:")
            if crawl_interval_min and crawl_interval_max:
                self.log(f"   é¡µé¢é—´éš”: {crawl_interval_min}-{crawl_interval_max}ç§’")
            if long_sleep_interval_min and long_sleep_interval_max:
                self.log(f"   é•¿ä¼‘çœ : {long_sleep_interval_min}-{long_sleep_interval_max}ç§’")
            if pages_per_batch:
                self.log(f"   æ‰¹æ¬¡å¤§å°: {pages_per_batch}é¡µ")
        else:
            self.use_custom_intervals = False
            self.log(f"ğŸ”§ ä½¿ç”¨é»˜è®¤é—´éš”è®¾ç½®")

    def log(self, message: str):
        """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºæ–¹æ³•"""
        print(message)  # ä»ç„¶è¾“å‡ºåˆ°æ§åˆ¶å°
        if self.log_callback:
            self.log_callback(message)  # åŒæ—¶æ¨é€åˆ°å‰ç«¯

    def set_stop_flag(self):
        """è®¾ç½®åœæ­¢æ ‡å¿—"""
        self.stop_flag = True
        self.log("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä»»åŠ¡å°†åœ¨ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹åœæ­¢")

    def is_stopped(self):
        """æ£€æŸ¥æ˜¯å¦è¢«åœæ­¢"""
        # é¦–å…ˆæ£€æŸ¥æœ¬åœ°åœæ­¢æ ‡å¿—
        if self.stop_flag:
            return True
        # ç„¶åæ£€æŸ¥å¤–éƒ¨åœæ­¢æ£€æŸ¥å‡½æ•°
        if self.stop_check_func and self.stop_check_func():
            self.stop_flag = True  # åŒæ­¥æœ¬åœ°æ ‡å¿—
            return True
        return False

    def _interruptible_sleep(self, duration: float):
        """å¯ä¸­æ–­çš„ç¡çœ ï¼Œæ¯0.1ç§’æ£€æŸ¥ä¸€æ¬¡åœæ­¢æ ‡å¿—"""
        start_time = time.time()
        while time.time() - start_time < duration:
            if self.is_stopped():
                return
            time.sleep(0.1)  # çŸ­æš‚ç¡çœ ï¼Œå…è®¸å¿«é€Ÿå“åº”åœæ­¢ä¿¡å·
    
    def clean_cookie(self, cookie: str) -> str:
        """æ¸…ç†Cookieå­—ç¬¦ä¸²ï¼Œå»é™¤ä¸åˆæ³•å­—ç¬¦
        
        Args:
            cookie (str): åŸå§‹Cookieå­—ç¬¦ä¸²
        
        Returns:
            str: æ¸…ç†åçš„Cookieå­—ç¬¦ä¸²
        """
        try:
            # å¦‚æœæ˜¯bytesç±»å‹ï¼Œå…ˆè§£ç 
            if isinstance(cookie, bytes):
                cookie = cookie.decode('utf-8')
            
            # å»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œç¬¦
            cookie = cookie.strip()
            
            # å¦‚æœæœ‰å¤šè¡Œï¼Œåªå–ç¬¬ä¸€è¡Œ
            if '\n' in cookie:
                cookie = cookie.split('\n')[0]
            
            # å»é™¤æœ«å°¾çš„åæ–œæ 
            cookie = cookie.rstrip('\\')
            
            # å»é™¤å¯èƒ½çš„å‰ç¼€bå’Œå¼•å·
            if cookie.startswith("b'") and cookie.endswith("'"):
                cookie = cookie[2:-1]
            elif cookie.startswith('b"') and cookie.endswith('"'):
                cookie = cookie[2:-1]
            elif cookie.startswith("'") and cookie.endswith("'"):
                cookie = cookie[1:-1]
            elif cookie.startswith('"') and cookie.endswith('"'):
                cookie = cookie[1:-1]
            
            # å¤„ç†è½¬ä¹‰å­—ç¬¦
            cookie = cookie.replace('\\n', '')
            cookie = cookie.replace('\\"', '"')
            cookie = cookie.replace("\\'", "'")
            
            # ç¡®ä¿åˆ†å·åæœ‰ç©ºæ ¼
            cookie = '; '.join(part.strip() for part in cookie.split(';'))
            
            return cookie
        except Exception as e:
            print(f"Cookieæ¸…ç†å¤±è´¥: {e}")
            return cookie  # è¿”å›åŸå§‹å€¼
    
    def get_file_downloader(self):
        """è·å–æ–‡ä»¶ä¸‹è½½å™¨ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self.file_downloader is None:
            # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨è·å–æ–‡ä»¶æ•°æ®åº“è·¯å¾„
            path_manager = get_db_path_manager()
            files_db_path = path_manager.get_files_db_path(self.group_id)
            self.file_downloader = ZSXQFileDownloader(self.cookie, self.group_id, files_db_path, wecom_webhook=self.wecom_webhook,log_callback=self.log)
        
        # ğŸ†• æ–°å¢ï¼šä¼ é€’webhookåˆ°æ–‡ä»¶ä¸‹è½½å™¨
        if self.wecom_webhook:
            self.file_downloader.wecom_webhook = self.wecom_webhook
            self.log("ğŸ“± æ–‡ä»¶ä¸‹è½½å™¨å·²é›†æˆä¼ä¸šå¾®ä¿¡æ¨é€")
        return self.file_downloader
    
    def show_database_status(self):
        """æ˜¾ç¤ºæ•°æ®åº“å½“å‰çŠ¶æ€"""
        stats = self.db.get_database_stats()
        total_topics = stats.get('topics', 0)
        total_users = stats.get('users', 0)
        total_comments = stats.get('comments', 0)
        
        print(f"\nğŸ“Š å½“å‰æ•°æ®åº“çŠ¶æ€:")
        print(f"   è¯é¢˜: {total_topics}, ç”¨æˆ·: {total_users}, è¯„è®º: {total_comments}")
        
        # æ˜¾ç¤ºæ—¶é—´æˆ³èŒƒå›´ä¿¡æ¯
        if total_topics > 0:
            timestamp_info = self.db.get_timestamp_range_info()
            if timestamp_info['has_data']:
                print(f"   æ—¶é—´èŒƒå›´: {timestamp_info['oldest_timestamp']} ~ {timestamp_info['newest_timestamp']}")
            else:
                print(f"   âš ï¸ æ—¶é—´æˆ³æ•°æ®ä¸å®Œæ•´")
    
    def get_stealth_headers(self) -> Dict[str, str]:
        """è·å–éšè”½æ€§æ›´å¼ºçš„è¯·æ±‚å¤´"""
        # æ›´å¤šæ ·åŒ–çš„User-Agentæ± 
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        ]
        
        # åŸºç¡€å¤´éƒ¨
        headers = {
            "Accept": "application/json, text/plain, */*",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7",
            "Cache-Control": "no-cache",
            "Cookie": self.cookie,
            "Origin": "https://wx.zsxq.com",
            "Pragma": "no-cache",
            "Priority": "u=1, i",
            "Referer": "https://wx.zsxq.com/",
            "Sec-Ch-Ua": '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"Windows"',
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-site",
            "User-Agent": random.choice(user_agents),
            "X-Aduid": "a3be07cd6-dd67-3912-0093-862d844e7fe",
            "X-Request-Id": f"dcc5cb6ab-1bc3-8273-cc26-{random.randint(100000000000, 999999999999)}",
            "X-Signature": "733fd672ddf6d4e367730d9622cdd1e28a4b6203",
            "X-Timestamp": str(int(time.time())),
            "X-Version": "2.77.0"
        }
        
        # éšæœºæ·»åŠ å¯é€‰å¤´éƒ¨
        optional_headers = {
            "X-Requested-With": "XMLHttpRequest",
            "Sec-GPC": "1",
            "Upgrade-Insecure-Requests": "1"
        }
        
        for key, value in optional_headers.items():
            if random.random() > 0.4:  # 60%æ¦‚ç‡æ·»åŠ 
                headers[key] = value
        
        return headers
    
    def smart_delay(self, is_historical: bool = False):
        """æ™ºèƒ½å»¶è¿Ÿæœºåˆ¶ - æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼ˆä»…åŸºç¡€å»¶è¿Ÿï¼‰"""
        self.request_count += 1

        # åŸºç¡€å»¶è¿Ÿæ—¶é—´
        if self.use_custom_intervals and self.custom_min_delay and self.custom_max_delay:
            # ä½¿ç”¨è‡ªå®šä¹‰é—´éš”
            min_delay = self.custom_min_delay
            max_delay = self.custom_max_delay
            if is_historical:
                delay = random.uniform(min_delay, max_delay + 1.0)  # å†å²çˆ¬å–ç¨é•¿
            else:
                delay = random.uniform(min_delay, max_delay)
            self.log(f"â±ï¸ é¡µé¢é—´éš”: {delay:.2f}ç§’ [è‡ªå®šä¹‰èŒƒå›´: {min_delay}-{max_delay}ç§’]")
        else:
            # ä½¿ç”¨é»˜è®¤é—´éš”
            if is_historical:
                delay = random.uniform(self.min_delay + 1.0, self.max_delay + 2.0)  # å†å²çˆ¬å–ç¨é•¿
            else:
                delay = random.uniform(self.min_delay, self.max_delay)
            if self.debug_mode:
                self.log(f"   â±ï¸ å»¶è¿Ÿ: {delay:.2f}ç§’ (è¯·æ±‚#{self.request_count})")

        # å¯ä¸­æ–­çš„å»¶è¿Ÿ
        self._interruptible_sleep(delay)
        self.last_request_time = time.time()
    
    def check_page_long_delay(self):
        """æ£€æŸ¥é¡µé¢çº§é•¿ä¼‘çœ ï¼šæ ¹æ®é…ç½®è¿›è¡Œé•¿ä¼‘çœ """
        self.page_count += 1

        # ç¡®å®šé•¿ä¼‘çœ é—´éš”
        if self.use_custom_intervals and self.custom_pages_per_batch:
            interval = self.custom_pages_per_batch
        else:
            interval = self.long_delay_interval

        if self.page_count % interval == 0:
            import datetime

            # ç¡®å®šé•¿ä¼‘çœ æ—¶é—´
            if self.use_custom_intervals and self.custom_long_delay_min and self.custom_long_delay_max:
                long_delay = random.uniform(self.custom_long_delay_min, self.custom_long_delay_max)
                self.log(f"ğŸ›Œ é•¿ä¼‘çœ å¼€å§‹: {long_delay:.1f}ç§’ ({long_delay/60:.1f}åˆ†é’Ÿ) [è‡ªå®šä¹‰èŒƒå›´: {self.custom_long_delay_min/60:.1f}-{self.custom_long_delay_max/60:.1f}åˆ†é’Ÿ]")
            else:
                long_delay = random.uniform(180, 300)  # 3-5åˆ†é’Ÿé•¿ä¼‘çœ 
                self.log(f"ğŸ›Œ é•¿ä¼‘çœ å¼€å§‹: {long_delay:.1f}ç§’ ({long_delay/60:.1f}åˆ†é’Ÿ) [é»˜è®¤èŒƒå›´: 3-5åˆ†é’Ÿ]")

            start_time = datetime.datetime.now()
            end_time = start_time + datetime.timedelta(seconds=long_delay)

            self.log(f"   å·²å®Œæˆ {self.page_count} ä¸ªé¡µé¢ï¼Œè¿›å…¥é•¿ä¼‘çœ æ¨¡å¼...")
            self.log(f"   â° å¼€å§‹æ—¶é—´: {start_time.strftime('%H:%M:%S')}")
            self.log(f"   ğŸ• é¢„è®¡æ¢å¤: {end_time.strftime('%H:%M:%S')}")

            # å¯ä¸­æ–­çš„é•¿å»¶è¿Ÿ
            self._interruptible_sleep(long_delay)

            actual_end_time = datetime.datetime.now()
            self.log(f"ğŸ˜´ é•¿ä¼‘çœ ç»“æŸï¼Œç»§ç»­çˆ¬å–...")
            self.log(f"   ğŸ• å®é™…ç»“æŸ: {actual_end_time.strftime('%H:%M:%S')}")

            # è°ƒè¯•ä¿¡æ¯
            if self.debug_mode:
                actual_duration = (actual_end_time - start_time).total_seconds()
                print(f"   ğŸ’¤ é•¿ä¼‘çœ å®Œæˆ: é¢„è®¡{long_delay:.1f}ç§’ï¼Œå®é™…{actual_duration:.1f}ç§’ (é¡µé¢#{self.page_count})")

    def fetch_comments_safe(self, topic_id: int, begin_time: str = None, count: int = 30, max_retries: int = 10) -> Optional[Dict[str, Any]]:
        """å®‰å…¨è·å–è¯é¢˜è¯„è®ºï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å¤„ç†åçˆ¬"""
        for retry in range(max_retries):
            try:
                # æ„å»ºè¯„è®ºAPI URL
                url = f"https://api.zsxq.com/v2/topics/{topic_id}/comments"
                params = {
                    'sort': 'asc',
                    'count': count,
                    'with_sticky': 'true'
                }

                if begin_time:
                    params['begin_time'] = begin_time

                # ä½¿ç”¨ä¸ä¸»è¦APIç›¸åŒçš„éšè”½æ€§è¯·æ±‚å¤´ï¼ŒåŒ…å«å®Œæ•´çš„è®¤è¯ä¿¡æ¯
                headers = self.get_stealth_headers()

                # è°ƒè¯•æ¨¡å¼è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                if self.debug_mode and retry == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶è¾“å‡º
                    from urllib.parse import urlencode
                    full_url = f"{url}?{urlencode(params)}"
                    print(f"ğŸ” è¯„è®ºAPIè°ƒè¯•ä¿¡æ¯:")
                    print(f"   ğŸ”— å®Œæ•´URL: {full_url}")
                    print(f"   ğŸ“Š å‚æ•°: {params}")
                    print(f"   ğŸ”§ å…³é”®è®¤è¯å¤´:")
                    print(f"      X-Signature: {headers.get('X-Signature', 'N/A')}")
                    print(f"      X-Timestamp: {headers.get('X-Timestamp', 'N/A')}")
                    print(f"      X-Request-Id: {headers.get('X-Request-Id', 'N/A')}")
                    print(f"      X-Aduid: {headers.get('X-Aduid', 'N/A')}")

                # å‘é€è¯·æ±‚
                response = self.session.get(url, params=params, headers=headers, timeout=30)

                if response.status_code == 200:
                    data = response.json()
                    if data.get('succeeded'):
                        if retry > 0:
                            self.log(f"âœ… è¯„è®ºAPIé‡è¯•æˆåŠŸ (ç¬¬{retry+1}æ¬¡å°è¯•)")
                        return data
                    else:
                        error_code = data.get('code')
                        error_msg = data.get('error', 'æœªçŸ¥é”™è¯¯')

                        # æ£€æŸ¥æ˜¯å¦æ˜¯åçˆ¬é”™è¯¯ç 1059
                        if error_code == 1059:
                            if retry < max_retries - 1:
                                # æ™ºèƒ½ç­‰å¾…æ—¶é—´ç­–ç•¥ï¼šå‰å‡ æ¬¡çŸ­ç­‰å¾…ï¼Œåé¢é€æ¸å¢åŠ 
                                if retry < 3:
                                    wait_time = 2  # å‰3æ¬¡ç­‰å¾…2ç§’
                                elif retry < 6:
                                    wait_time = 5  # ç¬¬4-6æ¬¡ç­‰å¾…5ç§’
                                else:
                                    wait_time = 10  # ç¬¬7-10æ¬¡ç­‰å¾…10ç§’

                                self.log(f"âš ï¸ é‡åˆ°åçˆ¬æœºåˆ¶ (é”™è¯¯ç 1059)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• (ç¬¬{retry+1}/{max_retries}æ¬¡)")
                                time.sleep(wait_time)
                                continue
                            else:
                                self.log(f"âŒ è¯„è®ºAPIé‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: é”™è¯¯ç {error_code} - {error_msg}")
                                return None
                        else:
                            self.log(f"âŒ è¯„è®ºAPIè¿”å›å¤±è´¥: é”™è¯¯ç {error_code} - {error_msg}")
                            return None
                else:
                    # è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
                    self.log(f"âŒ è¯„è®ºAPIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    self.log(f"ğŸ”— è¯·æ±‚URL: {response.url}")
                    self.log(f"ğŸ“‹ å“åº”å†…å®¹: {response.text[:500]}...")
                    return None

            except Exception as e:
                if retry < max_retries - 1:
                    # ä½¿ç”¨ä¸1059é”™è¯¯ç›¸åŒçš„ç­‰å¾…ç­–ç•¥
                    if retry < 3:
                        wait_time = 2
                    elif retry < 6:
                        wait_time = 5
                    else:
                        wait_time = 10

                    self.log(f"âŒ è·å–è¯„è®ºå¼‚å¸¸: {str(e)}ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• (ç¬¬{retry+1}/{max_retries}æ¬¡)")
                    time.sleep(wait_time)
                    continue
                else:
                    self.log(f"âŒ è·å–è¯„è®ºå¼‚å¸¸ï¼Œé‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {str(e)}")
                    return None

        return None

    def fetch_all_comments(self, topic_id: int, comments_count: int) -> List[Dict[str, Any]]:
        """è·å–è¯é¢˜çš„æ‰€æœ‰è¯„è®ºï¼ˆå¦‚æœè¯„è®ºæ•°é‡å¤§äº8ï¼‰"""
        if comments_count <= 8:
            return []  # ä¸éœ€è¦é¢å¤–è·å–

        self.log(f"ğŸ“ è¯é¢˜ {topic_id} æœ‰ {comments_count} æ¡è¯„è®ºï¼Œå¼€å§‹è·å–å®Œæ•´è¯„è®ºåˆ—è¡¨...")

        all_comments = []
        begin_time = None
        page = 1

        while True:
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                self.log("ğŸ›‘ è¯„è®ºè·å–å·²åœæ­¢")
                break

            self.log(f"   ğŸ“„ è·å–ç¬¬ {page} é¡µè¯„è®º...")

            # è·å–å½“å‰é¡µè¯„è®º
            data = self.fetch_comments_safe(topic_id, begin_time, count=30)
            if not data:
                self.log(f"   âŒ ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œå¯èƒ½æ˜¯æƒé™é—®é¢˜ï¼Œè·³è¿‡æ­¤è¯é¢˜")
                break

            comments = data.get('resp_data', {}).get('comments', [])
            if not comments:
                self.log(f"   ğŸ“­ ç¬¬ {page} é¡µæ— è¯„è®ºï¼Œåœæ­¢è·å–")
                break

            self.log(f"   âœ… ç¬¬ {page} é¡µè·å–åˆ° {len(comments)} æ¡è¯„è®º")

            # å¤„ç†è¯„è®ºæ•°æ®ï¼ŒåŒ…æ‹¬å›å¤è¯„è®º
            for comment in comments:
                all_comments.append(comment)

                # å¤„ç†å›å¤è¯„è®º
                if 'replied_comments' in comment and comment['replied_comments']:
                    for reply in comment['replied_comments']:
                        all_comments.append(reply)

            # å¦‚æœè¿”å›çš„è¯„è®ºæ•°é‡å°‘äº30ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
            if len(comments) < 30:
                self.log(f"   ğŸ å·²è·å–å®Œæ‰€æœ‰è¯„è®ºï¼Œå…± {len(all_comments)} æ¡")
                break

            # å‡†å¤‡ä¸‹ä¸€é¡µçš„ begin_timeï¼ˆæœ€åä¸€æ¡è¯„è®ºçš„æ—¶é—´ + 1æ¯«ç§’ï¼‰
            last_comment = comments[-1]
            last_time = last_comment.get('create_time')
            if last_time:
                begin_time = self._increment_time(last_time)
                self.log(f"   â­ï¸ ä¸‹ä¸€é¡µèµ·å§‹æ—¶é—´: {begin_time}")
            else:
                self.log("   âŒ æ— æ³•è·å–æœ€åè¯„è®ºæ—¶é—´ï¼Œåœæ­¢è·å–")
                break

            page += 1

            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)

        return all_comments

    def _increment_time(self, time_str: str) -> str:
        """å°†æ—¶é—´å­—ç¬¦ä¸²å¢åŠ 1æ¯«ç§’"""
        try:
            from datetime import datetime, timedelta
            import re

            # è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚: "2025-07-03T12:54:05.849+0800"
            # æå–æ¯«ç§’éƒ¨åˆ†
            match = re.match(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})\.(\d{3})(\+\d{4})', time_str)
            if match:
                base_time = match.group(1)
                milliseconds = int(match.group(2))
                timezone = match.group(3)

                # å¢åŠ 1æ¯«ç§’
                milliseconds += 1
                if milliseconds >= 1000:
                    # éœ€è¦è¿›ä½åˆ°ç§’
                    dt = datetime.strptime(base_time, '%Y-%m-%dT%H:%M:%S')
                    dt += timedelta(seconds=1)
                    base_time = dt.strftime('%Y-%m-%dT%H:%M:%S')
                    milliseconds = 0

                return f"{base_time}.{milliseconds:03d}{timezone}"
            else:
                # å¦‚æœæ ¼å¼ä¸åŒ¹é…ï¼Œç›´æ¥è¿”å›åŸæ—¶é—´
                return time_str

        except Exception as e:
            self.log(f"âŒ æ—¶é—´å¢é‡å¤±è´¥: {e}")
            return time_str

    def fetch_topics_safe(self, scope: str = "all", count: int = 20,
                         end_time: Optional[str] = None, is_historical: bool = False) -> Optional[Dict[str, Any]]:
        """å®‰å…¨çš„è¯é¢˜è·å–æ–¹æ³•"""
        
        # æ™ºèƒ½å»¶è¿Ÿ
        self.smart_delay(is_historical)
        
        url = f"{self.base_url}{self.api_endpoint}"
        headers = self.get_stealth_headers()
        
        # æ„å»ºå‚æ•°
        params = {
            "scope": scope,
            "count": str(count)
        }
        
        if end_time:
            params["end_time"] = end_time
        
        # ä¸æ·»åŠ é¢å¤–å‚æ•°ï¼Œä¿æŒä¸å®˜ç½‘è¯·æ±‚ä¸€è‡´
        # random_params = {
        #     "_t": str(int(time.time() * 1000)),
        #     "v": "1.0",
        #     "_r": str(random.randint(1000, 9999))
        # }
        # 
        # for key, value in random_params.items():
        #     if random.random() > 0.3:  # 70%æ¦‚ç‡æ·»åŠ 
        #         params[key] = value
        
        # æ„é€ å®Œæ•´URLç”¨äºæ˜¾ç¤º
        from urllib.parse import urlencode
        full_url = f"{url}?{urlencode(params)}"
        
        self.log(f"ğŸŒ å®‰å…¨è¯·æ±‚ #{self.request_count}")
        self.log(f"   ğŸ¯ å‚æ•°: scope={scope}, count={count}")
        if end_time:
            self.log(f"   ğŸ“… æ—¶é—´: {end_time}")
        self.log(f"   ğŸ”— å®Œæ•´é“¾æ¥: {full_url}")
        
        # è°ƒè¯•æ¨¡å¼è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        if self.debug_mode:
            print(f"   ğŸ” è°ƒè¯•æ¨¡å¼:")
            print(f"   ğŸ“ åŸºç¡€URL: {url}")
            print(f"   ğŸ“Š æ‰€æœ‰å‚æ•°: {params}")
            print(f"   ğŸ”§ è¯·æ±‚å¤´: {json.dumps(headers, ensure_ascii=False, indent=4)}")
            print(f"   ğŸª Cookieé•¿åº¦: {len(self.cookie)}å­—ç¬¦")
            print(f"   â° å½“å‰æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åœ¨å‘èµ·è¯·æ±‚å‰æ£€æŸ¥åœæ­¢æ ‡å¿—
        if self.is_stopped():
            # åœæ­¢æ—¶ä¸å†æ‰“å°æ—¥å¿—ï¼Œç›´æ¥è¿”å›
            return None

        try:
            response = self.session.get(
                url,
                headers=headers,
                params=params,
                timeout=10,  # é™ä½è¶…æ—¶æ—¶é—´ä»¥ä¾¿å¿«é€Ÿå“åº”åœæ­¢ä¿¡å·
                allow_redirects=True
            )
            
            self.log(f"   ğŸ“Š çŠ¶æ€: {response.status_code}, å¤§å°: {len(response.content)}B")

            # è¯·æ±‚å®Œæˆåç«‹å³æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                return None

            if response.status_code == 200:
                try:
                    # åœ¨å¤„ç†å“åº”å‰æ£€æŸ¥åœæ­¢æ ‡å¿—
                    if self.is_stopped():
                        self.log("ğŸ›‘ å“åº”å¤„ç†å‰æ£€æµ‹åˆ°åœæ­¢ä¿¡å·")
                        return None

                    data = response.json()
                    if data.get('succeeded'):
                        topics = data.get('resp_data', {}).get('topics', [])
                        self.log(f"   âœ… è·å–æˆåŠŸ: {len(topics)}ä¸ªè¯é¢˜")
                        return data
                    else:
                        error_code = data.get('code')
                        error_message = data.get('error', data.get('message', 'æœªçŸ¥é”™è¯¯'))

                        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                        if error_code == 14210:
                            print(f"   âŒ ä¼šå‘˜å·²è¿‡æœŸ: {error_message}")
                            print(f"   ğŸ“‹ å®Œæ•´å“åº”: {json.dumps(data, ensure_ascii=False, indent=2)}")
                            # è®¾ç½®è¿‡æœŸæ ‡å¿—ï¼Œè®©è°ƒç”¨æ–¹çŸ¥é“è¿™æ˜¯è¿‡æœŸé”™è¯¯
                            return {"expired": True, "code": error_code, "message": error_message}
                        else:
                            print(f"   âŒ APIå¤±è´¥: {error_message}")
                            print(f"   ğŸ“‹ å®Œæ•´å“åº”: {json.dumps(data, ensure_ascii=False, indent=2)}")
                            return None
                except json.JSONDecodeError as e:
                    print(f"   âŒ JSONè§£æå¤±è´¥: {e}")
                    print(f"   ğŸ“„ å“åº”å†…å®¹: {response.text[:500]}...")
                    print(f"   ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
                    return None
            else:
                print(f"   âŒ HTTPé”™è¯¯: {response.status_code}")
                print(f"   ğŸ“„ å“åº”å†…å®¹: {response.text}")
                print(f"   ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
                if response.status_code == 429:
                    print("   ğŸš¨ è§¦å‘é¢‘ç‡é™åˆ¶ï¼Œå»ºè®®å¢åŠ å»¶è¿Ÿæ—¶é—´")
                elif response.status_code == 403:
                    print("   ğŸš¨ è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookieæˆ–åæ£€æµ‹ç­–ç•¥")
                elif response.status_code == 401:
                    print("   ğŸš¨ è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥Cookieæ˜¯å¦è¿‡æœŸ")
                return None
                
        except requests.exceptions.Timeout as e:
            print(f"   âŒ è¯·æ±‚è¶…æ—¶: {e}")
            print(f"   ğŸ”§ å»ºè®®: å¢åŠ è¶…æ—¶æ—¶é—´æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return None
        except requests.exceptions.ConnectionError as e:
            print(f"   âŒ è¿æ¥é”™è¯¯: {e}")
            print(f"   ğŸ”§ å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–DNSè®¾ç½®")
            return None
        except requests.exceptions.HTTPError as e:
            print(f"   âŒ HTTPåè®®é”™è¯¯: {e}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            print(f"   ğŸ”§ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            return None
    
    def store_batch_data(self, data: Dict[str, Any]) -> Dict[str, int]:
        """æ‰¹é‡å­˜å‚¨æ•°æ®åˆ°æ•°æ®åº“"""
        # åœ¨æ•°æ®å­˜å‚¨å‰æ£€æŸ¥åœæ­¢æ ‡å¿—
        if self.is_stopped():
            self.log("ğŸ›‘ æ•°æ®å­˜å‚¨å‰æ£€æµ‹åˆ°åœæ­¢ä¿¡å·")
            return {'new_topics': 0, 'updated_topics': 0, 'errors': 0}

        if not data or not data.get('succeeded'):
            return {'new_topics': 0, 'updated_topics': 0, 'errors': 0}

        topics = data.get('resp_data', {}).get('topics', [])
        if not topics:
            return {'new_topics': 0, 'updated_topics': 0, 'errors': 0}

        stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0}

        for topic_data in topics:
            # åœ¨å¤„ç†æ¯ä¸ªè¯é¢˜å‰æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                self.log("ğŸ›‘ è¯é¢˜å¤„ç†è¿‡ç¨‹ä¸­æ£€æµ‹åˆ°åœæ­¢ä¿¡å·")
                break

            try:
                topic_id = topic_data.get('topic_id')

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                self.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
                exists = self.db.cursor.fetchone()

                # å¯¼å…¥æ•°æ®
                self.db.import_topic_data(topic_data)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–æ›´å¤šè¯„è®º
                comments_count = topic_data.get('comments_count', 0)
                if comments_count > 8:
                    self.log(f"ğŸ“ è¯é¢˜ {topic_id} æœ‰ {comments_count} æ¡è¯„è®ºï¼Œå°è¯•è·å–å®Œæ•´è¯„è®ºåˆ—è¡¨...")
                    try:
                        additional_comments = self.fetch_all_comments(topic_id, comments_count)
                        if additional_comments:
                            self.db.import_additional_comments(topic_id, additional_comments)
                            self.log(f"âœ… æˆåŠŸè·å–å¹¶å¯¼å…¥ {len(additional_comments)} æ¡é¢å¤–è¯„è®º")
                        else:
                            self.log(f"â„¹ï¸ è¯é¢˜ {topic_id} æ— æ³•è·å–æ›´å¤šè¯„è®ºï¼Œå¯èƒ½æ˜¯æƒé™é™åˆ¶")
                    except Exception as e:
                        self.log(f"âš ï¸ è¯é¢˜ {topic_id} è·å–è¯„è®ºæ—¶å‡ºé”™: {e}")
                        # ä¸å½±å“è¯é¢˜æœ¬èº«çš„å¯¼å…¥

                if exists:
                    stats['updated_topics'] += 1
                else:
                    stats['new_topics'] += 1

            except Exception as e:
                stats['errors'] += 1
                print(f"   âš ï¸ è¯é¢˜å¯¼å…¥å¤±è´¥: {e}")
        
        # æäº¤äº‹åŠ¡
        self.db.conn.commit()
        return stats
    
    def crawl_latest(self, count: int = 20) -> Dict[str, int]:
        """çˆ¬å–æœ€æ–°è¯é¢˜"""
        print(f"\nğŸ†• çˆ¬å–æœ€æ–° {count} ä¸ªè¯é¢˜...")
        
        data = self.fetch_topics_safe(scope="all", count=count)
        if data:
            # wx_pushæ–°å¢åŠŸèƒ½ï¼Œåœ¨å­˜å…¥æ•°æ®åº“å‰åˆ¤æ–­æ–°å¢å†…å®¹
            topics = data.get('resp_data', {}).get('topics', [])
            
            # âœ… åœ¨å­˜å‚¨å‰è¯†åˆ«æ–°å¢è¯é¢˜
            new_topics = []
            for topic in topics:
                topic_id = topic.get('topic_id')
                self.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
                if not self.db.cursor.fetchone():
                    new_topics.append(topic)
            
            # å­˜å‚¨æ•°æ®
            stats = self.store_batch_data(data)
            self.log(f"ğŸ’¾ å­˜å‚¨ç»“æœ: æ–°å¢{stats['new_topics']}, æ›´æ–°{stats['updated_topics']}")
            
            # ä¼ä¸šå¾®ä¿¡æ¨é€
            if self.wecom_webhook and new_topics:  # âœ… ä½¿ç”¨new_topicsåˆ—è¡¨åˆ¤æ–­
                self.log(f"ğŸ“± å‡†å¤‡æ¨é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥ï¼Œå…±{len(new_topics)}ä¸ªæ–°è¯é¢˜...")
                success = self.wecom_webhook.send_new_topics_notification(new_topics, stats)
                if success:
                    self.log("âœ… ä¼ä¸šå¾®ä¿¡æ¨é€æˆåŠŸ")
                else:
                    self.log("âš ï¸ ä¼ä¸šå¾®ä¿¡æ¨é€å¤±è´¥")
                return stats
            else:
                print("âŒ è·å–å¤±è´¥")
                return {'new_topics': 0, 'updated_topics': 0, 'errors': 1}
    
    def crawl_historical(self, pages: int = 10, per_page: int = 20) -> Dict[str, int]:
        """çˆ¬å–å†å²æ•°æ®"""
        print(f"\nğŸ“š çˆ¬å–å†å²æ•°æ®: {pages}é¡µ x {per_page}æ¡/é¡µ")
        
        total_stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}
        end_time = None
        completed_pages = 0
        max_retries_per_page = 10  # æ¯é¡µæœ€å¤§é‡è¯•æ¬¡æ•°
        
        while completed_pages < pages:
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                self.log("ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
                break

            current_page = completed_pages + 1
            self.log(f"\nğŸ“„ é¡µé¢ {current_page}/{pages}")
            retry_count = 0
            
            # é‡è¯•å½“å‰é¡µç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            while retry_count < max_retries_per_page:
                # åœ¨é‡è¯•å¾ªç¯ä¸­ä¹Ÿæ£€æŸ¥åœæ­¢æ ‡å¿—
                if self.is_stopped():
                    return total_stats

                if retry_count > 0:
                    self.log(f"   ğŸ”„ ç¬¬{retry_count}æ¬¡é‡è¯•")
                
                # è·å–æ•°æ®
                if current_page == 1:
                    data = self.fetch_topics_safe(scope="all", count=per_page, is_historical=True)
                else:
                    data = self.fetch_topics_safe(scope="all", count=per_page, 
                                                end_time=end_time, is_historical=True)
                
                if data:
                    # æˆåŠŸè·å–æ•°æ®
                    topics = data.get('resp_data', {}).get('topics', [])
                    if not topics:
                        print(f"   ğŸ“­ æ— æ›´å¤šæ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                        return total_stats
                    
                    # å­˜å‚¨æ•°æ®
                    page_stats = self.store_batch_data(data)
                    self.log(f"   ğŸ’¾ é¡µé¢å­˜å‚¨: æ–°å¢{page_stats['new_topics']}, æ›´æ–°{page_stats['updated_topics']}")
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    total_stats['new_topics'] += page_stats['new_topics']
                    total_stats['updated_topics'] += page_stats['updated_topics']
                    total_stats['errors'] += page_stats['errors']
                    total_stats['pages'] += 1
                    completed_pages += 1
                    
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºæ‰€æœ‰è¯é¢˜çš„æ—¶é—´æˆ³ï¼ˆåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼‰
                    if self.debug_mode:
                        self.log(f"   ğŸ” è°ƒè¯•ä¿¡æ¯:")
                        self.log(f"   ğŸ“Š æœ¬é¡µè·å–åˆ° {len(topics)} ä¸ªè¯é¢˜")
                        for i, topic in enumerate(topics):
                            topic_time = topic.get('create_time', 'N/A')
                            topic_title = topic.get('title', 'æ— æ ‡é¢˜')[:30]
                            self.log(f"   {i+1:2d}. {topic_time} - {topic_title}")
                    
                    # å‡†å¤‡ä¸‹ä¸€é¡µçš„æ—¶é—´æˆ³
                    if topics:
                        original_time = topics[-1].get('create_time')
                        try:
                            from datetime import datetime, timedelta
                            dt = datetime.fromisoformat(original_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                            print(f"   ğŸ“… åŸå§‹æ—¶é—´æˆ³: {original_time}")
                            print(f"   â­ï¸ ä¸‹ä¸€é¡µæ—¶é—´æˆ³: {end_time} (å‡å»{self.timestamp_offset_ms}æ¯«ç§’)")
                        except Exception as e:
                            end_time = original_time
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                            print(f"   â­ï¸ ä¸‹ä¸€é¡µæ—¶é—´æˆ³: {end_time} (æœªè°ƒæ•´)")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å®Œ
                    if len(topics) < per_page:
                        print(f"   ğŸ“­ å·²çˆ¬å–å®Œæ¯• (è¿”å›{len(topics)}æ¡)")
                        return total_stats
                    
                    # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    self.check_page_long_delay()  # é¡µé¢æˆåŠŸå¤„ç†åè¿›è¡Œé•¿ä¼‘çœ æ£€æŸ¥
                    break
                else:
                    # å¤±è´¥ï¼Œå¢åŠ é‡è¯•è®¡æ•°å’Œé”™è¯¯è®¡æ•°
                    retry_count += 1
                    total_stats['errors'] += 1
                    print(f"   âŒ é¡µé¢ {current_page} è·å–å¤±è´¥ (é‡è¯•{retry_count}/{max_retries_per_page})")
                    
                    # è°ƒæ•´æ—¶é—´æˆ³ç”¨äºé‡è¯•
                    if end_time:
                        try:
                            from datetime import datetime, timedelta
                            dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                            print(f"   ğŸ”„ è°ƒæ•´æ—¶é—´æˆ³: {end_time} (å†æ¬¡å‡å»{self.timestamp_offset_ms}æ¯«ç§’)")
                        except Exception as e:
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
            
            # å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œä»ç„¶å¤±è´¥
            if retry_count >= max_retries_per_page:
                print(f"   ğŸš« é¡µé¢ {current_page} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤é¡µ")
                # å¦‚æœæœ‰æ—¶é—´æˆ³ï¼Œå°è¯•å¤§å¹…åº¦è°ƒæ•´è·³è¿‡é—®é¢˜åŒºåŸŸ
                if end_time:
                    try:
                        from datetime import datetime, timedelta
                        dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                        # å¤§å¹…åº¦è·³è¿‡ï¼Œå‡å»1å°æ—¶
                        dt = dt - timedelta(hours=1)
                        end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        print(f"   â° å¤§å¹…åº¦è·³è¿‡æ—¶é—´æ®µ: {end_time} (å‡å»1å°æ—¶)")
                    except Exception as e:
                        print(f"   âš ï¸ å¤§å¹…åº¦æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                completed_pages += 1  # è·³è¿‡è¿™ä¸€é¡µ
        
        print(f"\nğŸ å†å²çˆ¬å–å®Œæˆ:")
        print(f"   ğŸ“„ æˆåŠŸé¡µæ•°: {total_stats['pages']}")
        print(f"   âœ… æ–°å¢è¯é¢˜: {total_stats['new_topics']}")
        print(f"   ğŸ”„ æ›´æ–°è¯é¢˜: {total_stats['updated_topics']}")
        if total_stats['errors'] > 0:
            print(f"   âŒ æ€»é”™è¯¯æ•°: {total_stats['errors']}")
        
        return total_stats
    
    def crawl_all_historical(self, per_page: int = 20, auto_confirm: bool = False) -> Dict[str, int]:
        """è·å–æ‰€æœ‰å†å²æ•°æ®ï¼šæ— é™çˆ¬å–ç›´åˆ°æ²¡æœ‰æ•°æ®ï¼ˆä½¿ç”¨å¢é‡çˆ¬å–é€»è¾‘ï¼‰"""
        self.log(f"\nğŸŒŠ è·å–æ‰€æœ‰å†å²æ•°æ®æ¨¡å¼ (æ¯é¡µ{per_page}æ¡)")
        self.log(f"âš ï¸ è­¦å‘Šï¼šæ­¤æ¨¡å¼å°†æŒç»­çˆ¬å–ç›´åˆ°æ²¡æœ‰æ•°æ®ï¼Œå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´")
        
        # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€ï¼Œå¦‚æœæœ‰æ•°æ®åˆ™ä½¿ç”¨å¢é‡çˆ¬å–é€»è¾‘
        timestamp_info = self.db.get_timestamp_range_info()
        start_end_time = None
        
        if timestamp_info['has_data']:
            oldest_timestamp = timestamp_info['oldest_timestamp']
            total_existing = timestamp_info['total_topics']
            
            self.log(f"ğŸ“Š æ•°æ®åº“ç°çŠ¶:")
            self.log(f"   ç°æœ‰è¯é¢˜æ•°: {total_existing}")
            self.log(f"   æœ€è€æ—¶é—´æˆ³: {oldest_timestamp}")
            self.log(f"   æœ€æ–°æ—¶é—´æˆ³: {timestamp_info['newest_timestamp']}")
            self.log(f"ğŸ¯ å°†ä»æœ€è€æ—¶é—´æˆ³å¼€å§‹ç»§ç»­å‘å†å²çˆ¬å–ï¼ˆå¢é‡æ¨¡å¼ï¼‰...")
            
            # å‡†å¤‡å¢é‡çˆ¬å–çš„èµ·å§‹æ—¶é—´æˆ³
            try:
                from datetime import datetime, timedelta
                dt = datetime.fromisoformat(oldest_timestamp.replace('+0800', '+08:00'))
                dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                start_end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                print(f"ğŸš€ å¢é‡çˆ¬å–èµ·å§‹æ—¶é—´æˆ³: {start_end_time}")
            except Exception as e:
                print(f"âš ï¸ æ—¶é—´æˆ³å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸæ—¶é—´æˆ³: {e}")
                start_end_time = oldest_timestamp
        else:
            self.log(f"ğŸ“Š æ•°æ®åº“ä¸ºç©ºï¼Œå°†ä»æœ€æ–°æ•°æ®å¼€å§‹çˆ¬å–")

        # ç”¨æˆ·ç¡®è®¤ï¼ˆWeb APIè°ƒç”¨æ—¶è‡ªåŠ¨ç¡®è®¤ï¼‰
        if not auto_confirm:
            confirm = input("ç¡®è®¤å¼€å§‹æ— é™çˆ¬å–ï¼Ÿ(y/N): ").lower().strip()
            if confirm != 'y':
                self.log("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}

        self.log(f"ğŸš€ å¼€å§‹æ— é™å†å²çˆ¬å–...")
        
        total_stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}
        end_time = start_end_time  # ä½¿ç”¨å¢é‡çˆ¬å–çš„èµ·å§‹æ—¶é—´æˆ³
        current_page = 0
        max_retries_per_page = 10
        consecutive_empty_pages = 0  # è¿ç»­ç©ºé¡µé¢è®¡æ•°
        max_consecutive_empty = 3   # æœ€å¤§è¿ç»­ç©ºé¡µé¢æ•°
        
        while True:
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                self.log("ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
                break

            current_page += 1
            self.log(f"\nğŸ“„ é¡µé¢ {current_page}")
            retry_count = 0
            page_success = False

            # é‡è¯•å½“å‰é¡µç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            while retry_count < max_retries_per_page:
                # åœ¨é‡è¯•å¾ªç¯ä¸­ä¹Ÿæ£€æŸ¥åœæ­¢æ ‡å¿—
                if self.is_stopped():
                    return total_stats
                if retry_count > 0:
                    self.log(f"   ğŸ”„ ç¬¬{retry_count}æ¬¡é‡è¯•")
                
                # è·å–æ•°æ® - æ ¹æ®æ˜¯å¦æœ‰èµ·å§‹æ—¶é—´æˆ³å†³å®šè¯·æ±‚æ–¹å¼
                if current_page == 1 and start_end_time is None:
                    # æ•°æ®åº“ä¸ºç©ºï¼Œä»æœ€æ–°å¼€å§‹
                    data = self.fetch_topics_safe(scope="all", count=per_page, is_historical=True)
                else:
                    # æœ‰æ•°æ®æˆ–åç»­é¡µé¢ï¼Œä½¿ç”¨ end_time å‚æ•°
                    data = self.fetch_topics_safe(scope="all", count=per_page,
                                                end_time=end_time, is_historical=True)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                if data and data.get('expired'):
                    print(f"   âŒ ä¼šå‘˜å·²è¿‡æœŸï¼Œåœæ­¢çˆ¬å–")
                    return data  # ç›´æ¥è¿”å›è¿‡æœŸä¿¡æ¯

                if data:
                    # æˆåŠŸè·å–æ•°æ®
                    topics = data.get('resp_data', {}).get('topics', [])
                    
                    if not topics:
                        consecutive_empty_pages += 1
                        print(f"   ğŸ“­ ç¬¬{consecutive_empty_pages}ä¸ªç©ºé¡µé¢")
                        
                        if consecutive_empty_pages >= max_consecutive_empty:
                            print(f"   ğŸ è¿ç»­{max_consecutive_empty}ä¸ªç©ºé¡µé¢ï¼Œæ‰€æœ‰å†å²æ•°æ®çˆ¬å–å®Œæˆ")
                            print(f"\nğŸ‰ æ— é™çˆ¬å–å®Œæˆæ€»ç»“:")
                            print(f"   ğŸ“„ æ€»é¡µæ•°: {total_stats['pages']}")
                            print(f"   âœ… æ–°å¢è¯é¢˜: {total_stats['new_topics']}")
                            print(f"   ğŸ”„ æ›´æ–°è¯é¢˜: {total_stats['updated_topics']}")
                            if total_stats['errors'] > 0:
                                print(f"   âŒ æ€»é”™è¯¯æ•°: {total_stats['errors']}")
                            
                            # æ˜¾ç¤ºæœ€ç»ˆæ•°æ®åº“çŠ¶æ€
                            final_db_stats = self.db.get_timestamp_range_info()
                            if final_db_stats['has_data']:
                                print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®åº“çŠ¶æ€:")
                                print(f"   è¯é¢˜æ€»æ•°: {final_db_stats['total_topics']}")
                                if timestamp_info['has_data']:
                                    print(f"   æ–°å¢è¯é¢˜: {final_db_stats['total_topics'] - timestamp_info['total_topics']}")
                                print(f"   æ—¶é—´èŒƒå›´: {final_db_stats['oldest_timestamp']} ~ {final_db_stats['newest_timestamp']}")
                            
                            return total_stats
                        
                        # ç©ºé¡µé¢ä¹Ÿç®—æˆåŠŸï¼Œé¿å…æ— é™é‡è¯•
                        page_success = True
                        break
                    else:
                        consecutive_empty_pages = 0  # é‡ç½®è¿ç»­ç©ºé¡µé¢è®¡æ•°
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®ï¼ˆé¿å…é‡å¤çˆ¬å–å·²æœ‰æ•°æ®ï¼‰
                    new_topics_count = 0
                    for topic in topics:
                        topic_id = topic.get('topic_id')
                        self.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
                        if not self.db.cursor.fetchone():
                            new_topics_count += 1
                    
                    # å­˜å‚¨æ•°æ®
                    page_stats = self.store_batch_data(data)
                    print(f"   ğŸ’¾ é¡µé¢å­˜å‚¨: æ–°å¢{page_stats['new_topics']}, æ›´æ–°{page_stats['updated_topics']}")
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    total_stats['new_topics'] += page_stats['new_topics']
                    total_stats['updated_topics'] += page_stats['updated_topics']
                    total_stats['errors'] += page_stats['errors']
                    total_stats['pages'] += 1
                    
                    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                    print(f"   ğŸ“Š è·å–åˆ° {len(topics)} ä¸ªè¯é¢˜ï¼Œå…¶ä¸­ {new_topics_count} ä¸ªä¸ºæ–°è¯é¢˜")
                    print(f"   ğŸ“ˆ ç´¯è®¡: æ–°å¢{total_stats['new_topics']}, æ›´æ–°{total_stats['updated_topics']}, é¡µæ•°{total_stats['pages']}")
                    
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºæ—¶é—´æˆ³ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    if topics:
                        first_time = topics[0].get('create_time', 'N/A')
                        last_time = topics[-1].get('create_time', 'N/A')
                        print(f"   â° æ—¶é—´èŒƒå›´: {first_time} ~ {last_time}")
                    
                    # å‡†å¤‡ä¸‹ä¸€é¡µçš„æ—¶é—´æˆ³
                    if topics:
                        original_time = topics[-1].get('create_time')
                        try:
                            from datetime import datetime, timedelta
                            dt = datetime.fromisoformat(original_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        except Exception as e:
                            end_time = original_time
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¿”å›æ•°æ®é‡å°äºé¢„æœŸï¼ˆå¯èƒ½æ¥è¿‘åº•éƒ¨ï¼‰
                    if len(topics) < per_page:
                        print(f"   âš ï¸ è¿”å›æ•°æ®é‡({len(topics)})å°äºé¢„æœŸ({per_page})ï¼Œå¯èƒ½æ¥è¿‘å†å²åº•éƒ¨")
                    
                    # å¦‚æœæ²¡æœ‰æ–°è¯é¢˜ä¸”æ•°æ®é‡ä¸è¶³ï¼Œå¯èƒ½å·²è¾¾å†å²åº•éƒ¨
                    if new_topics_count == 0 and len(topics) < per_page:
                        print(f"   ğŸ“­ æ— æ–°è¯é¢˜ä¸”æ•°æ®é‡ä¸è¶³ï¼Œå¯èƒ½å·²è¾¾å†å²åº•éƒ¨")
                        return total_stats
                    
                    # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    page_success = True
                    break
                else:
                    # å¤±è´¥ï¼Œå¢åŠ é‡è¯•è®¡æ•°å’Œé”™è¯¯è®¡æ•°
                    retry_count += 1
                    total_stats['errors'] += 1
                    print(f"   âŒ é¡µé¢ {current_page} è·å–å¤±è´¥ (é‡è¯•{retry_count}/{max_retries_per_page})")
                    
                    # è°ƒæ•´æ—¶é—´æˆ³ç”¨äºé‡è¯•
                    if end_time:
                        try:
                            from datetime import datetime, timedelta
                            dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        except Exception as e:
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
            
            # å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œä»ç„¶å¤±è´¥
            if not page_success:
                print(f"   ğŸš« é¡µé¢ {current_page} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                # å¤§å¹…åº¦è·³è¿‡é—®é¢˜åŒºåŸŸ
                if end_time:
                    try:
                        from datetime import datetime, timedelta
                        dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                        dt = dt - timedelta(hours=1)
                        end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        print(f"   â° å¤§å¹…åº¦è·³è¿‡æ—¶é—´æ®µ: {end_time} (å‡å»1å°æ—¶)")
                    except Exception as e:
                        print(f"   âš ï¸ å¤§å¹…åº¦æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                completed_pages += 1  # è·³è¿‡è¿™ä¸€é¡µ
            else:
                # é¡µé¢æˆåŠŸå¤„ç†åè¿›è¡Œé•¿ä¼‘çœ æ£€æŸ¥ï¼ˆåŸºäºé¡µé¢æ•°è€Œéè¯·æ±‚æ•°ï¼‰
                self.check_page_long_delay()
            
            # æ¯50é¡µæ˜¾ç¤ºä¸€æ¬¡æ€»ä½“è¿›åº¦
            if current_page % 50 == 0:
                print(f"\nğŸ¯ è¿›åº¦æŠ¥å‘Š (ç¬¬{current_page}é¡µ):")
                print(f"   ğŸ“Š ç´¯è®¡æ–°å¢: {total_stats['new_topics']}")
                print(f"   ğŸ“Š ç´¯è®¡æ›´æ–°: {total_stats['updated_topics']}")
                print(f"   ğŸ“Š æˆåŠŸé¡µæ•°: {total_stats['pages']}")
                print(f"   ğŸ“Š é”™è¯¯æ¬¡æ•°: {total_stats['errors']}")
                
                # æ˜¾ç¤ºå½“å‰æ•°æ®åº“çŠ¶æ€
                current_db_stats = self.db.get_timestamp_range_info()
                if current_db_stats['has_data']:
                    print(f"   ğŸ“Š æ•°æ®åº“çŠ¶æ€: {current_db_stats['total_topics']}ä¸ªè¯é¢˜")
                    print(f"   ğŸ“Š æ—¶é—´èŒƒå›´: {current_db_stats['oldest_timestamp']} ~ {current_db_stats['newest_timestamp']}")
        
        # è¿™é‡Œç†è®ºä¸Šä¸ä¼šåˆ°è¾¾ï¼Œå› ä¸ºåœ¨å¾ªç¯å†…ä¼šreturn
        return total_stats
    
    def crawl_incremental(self, pages: int = 10, per_page: int = 20) -> Dict[str, int]:
        """å¢é‡çˆ¬å–ï¼šåŸºäºæ•°æ®åº“æœ€è€æ—¶é—´æˆ³ç»§ç»­å‘å†å²çˆ¬å–"""
        print(f"\nğŸ“ˆ å¢é‡çˆ¬å–æ¨¡å¼: {pages}é¡µ x {per_page}æ¡/é¡µ")
        
        # è·å–æ•°æ®åº“æ—¶é—´æˆ³èŒƒå›´ä¿¡æ¯
        timestamp_info = self.db.get_timestamp_range_info()
        
        if not timestamp_info['has_data']:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰è¯é¢˜æ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œå†å²çˆ¬å–")
            return {'new_topics': 0, 'updated_topics': 0, 'errors': 1}
        
        oldest_timestamp = timestamp_info['oldest_timestamp']
        total_existing = timestamp_info['total_topics']
        
        print(f"ğŸ“Š æ•°æ®åº“çŠ¶æ€:")
        print(f"   ç°æœ‰è¯é¢˜æ•°: {total_existing}")
        print(f"   æœ€è€æ—¶é—´æˆ³: {oldest_timestamp}")
        print(f"   æœ€æ–°æ—¶é—´æˆ³: {timestamp_info['newest_timestamp']}")
        print(f"ğŸ¯ å°†ä»æœ€è€æ—¶é—´æˆ³å¼€å§‹ç»§ç»­å‘å†å²çˆ¬å–...")
        
        # å‡†å¤‡å¢é‡çˆ¬å–çš„èµ·å§‹æ—¶é—´æˆ³ï¼ˆåœ¨æœ€è€æ—¶é—´æˆ³åŸºç¡€ä¸Šå‡å»åç§»é‡ï¼‰
        try:
            from datetime import datetime, timedelta
            dt = datetime.fromisoformat(oldest_timestamp.replace('+0800', '+08:00'))
            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
            start_end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
            print(f"ğŸš€ å¢é‡çˆ¬å–èµ·å§‹æ—¶é—´æˆ³: {start_end_time}")
        except Exception as e:
            print(f"âš ï¸ æ—¶é—´æˆ³å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸæ—¶é—´æˆ³: {e}")
            start_end_time = oldest_timestamp
        
        # æ‰§è¡Œå¢é‡çˆ¬å–
        total_stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}
        end_time = start_end_time
        completed_pages = 0
        max_retries_per_page = 10
        
        while completed_pages < pages:
            current_page = completed_pages + 1
            self.log(f"\nğŸ“„ å¢é‡é¡µé¢ {current_page}/{pages}")
            retry_count = 0
            
            # é‡è¯•å½“å‰é¡µç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            while retry_count < max_retries_per_page:
                if retry_count > 0:
                    print(f"   ğŸ”„ ç¬¬{retry_count}æ¬¡é‡è¯•")
                
                # è·å–æ•°æ® - æ€»æ˜¯ä½¿ç”¨ end_time å‚æ•°
                data = self.fetch_topics_safe(scope="all", count=per_page,
                                            end_time=end_time, is_historical=True)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼šå‘˜è¿‡æœŸé”™è¯¯
                if data and data.get('expired'):
                    print(f"   âŒ ä¼šå‘˜å·²è¿‡æœŸï¼Œåœæ­¢çˆ¬å–")
                    return data  # ç›´æ¥è¿”å›è¿‡æœŸä¿¡æ¯

                if data:
                    # æˆåŠŸè·å–æ•°æ®
                    topics = data.get('resp_data', {}).get('topics', [])
                    if not topics:
                        print(f"   ğŸ“­ æ— æ›´å¤šå†å²æ•°æ®ï¼Œå¢é‡çˆ¬å–å®Œæˆ")
                        return total_stats
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®ï¼ˆé¿å…é‡å¤çˆ¬å–å·²æœ‰æ•°æ®ï¼‰
                    new_topics_count = 0
                    for topic in topics:
                        topic_id = topic.get('topic_id')
                        self.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
                        if not self.db.cursor.fetchone():
                            new_topics_count += 1
                    
                    print(f"   ğŸ“Š è·å–åˆ° {len(topics)} ä¸ªè¯é¢˜ï¼Œå…¶ä¸­ {new_topics_count} ä¸ªä¸ºæ–°è¯é¢˜")
                    
                    # å¦‚æœæ²¡æœ‰æ–°è¯é¢˜ä¸”å½“å‰é¡µè¯é¢˜æ•°å°‘äºé¢„æœŸï¼Œå¯èƒ½å·²åˆ°è¾¾å†å²åº•éƒ¨
                    if new_topics_count == 0 and len(topics) < per_page:
                        print(f"   ğŸ“­ æ— æ–°è¯é¢˜ä¸”æ•°æ®é‡ä¸è¶³ï¼Œå¯èƒ½å·²è¾¾å†å²åº•éƒ¨")
                        return total_stats
                    
                    # å­˜å‚¨æ•°æ®
                    page_stats = self.store_batch_data(data)
                    print(f"   ğŸ’¾ é¡µé¢å­˜å‚¨: æ–°å¢{page_stats['new_topics']}, æ›´æ–°{page_stats['updated_topics']}")
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    total_stats['new_topics'] += page_stats['new_topics']
                    total_stats['updated_topics'] += page_stats['updated_topics']
                    total_stats['errors'] += page_stats['errors']
                    total_stats['pages'] += 1
                    completed_pages += 1
                    
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºè¯é¢˜æ—¶é—´æˆ³ä¿¡æ¯
                    if self.debug_mode:
                        print(f"   ğŸ” è°ƒè¯•ä¿¡æ¯:")
                        print(f"   ğŸ“Š æœ¬é¡µè·å–åˆ° {len(topics)} ä¸ªè¯é¢˜")
                        for i, topic in enumerate(topics):
                            topic_time = topic.get('create_time', 'N/A')
                            topic_title = topic.get('title', 'æ— æ ‡é¢˜')[:30]
                            print(f"   {i+1:2d}. {topic_time} - {topic_title}")
                    
                    # å‡†å¤‡ä¸‹ä¸€é¡µçš„æ—¶é—´æˆ³
                    if topics:
                        original_time = topics[-1].get('create_time')
                        try:
                            dt = datetime.fromisoformat(original_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                            print(f"   â­ï¸ ä¸‹ä¸€é¡µæ—¶é—´æˆ³: {end_time}")
                        except Exception as e:
                            end_time = original_time
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                    
                    # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    self.check_page_long_delay()  # é¡µé¢æˆåŠŸå¤„ç†åè¿›è¡Œé•¿ä¼‘çœ æ£€æŸ¥
                    break
                else:
                    # å¤±è´¥ï¼Œå¢åŠ é‡è¯•è®¡æ•°å’Œé”™è¯¯è®¡æ•°
                    retry_count += 1
                    total_stats['errors'] += 1

                    # å¦‚æœä»»åŠ¡å·²åœæ­¢ï¼Œä¸å†æ‰“å°é”™è¯¯ä¿¡æ¯å’Œè°ƒæ•´æ—¶é—´æˆ³
                    if self.is_stopped():
                        return total_stats

                    print(f"   âŒ é¡µé¢ {current_page} è·å–å¤±è´¥ (é‡è¯•{retry_count}/{max_retries_per_page})")

                    # è°ƒæ•´æ—¶é—´æˆ³ç”¨äºé‡è¯•
                    if end_time:
                        try:
                            dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                            print(f"   ğŸ”„ è°ƒæ•´æ—¶é—´æˆ³: {end_time}")
                        except Exception as e:
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
            
            # å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œä»ç„¶å¤±è´¥
            if retry_count >= max_retries_per_page:
                # å¦‚æœä»»åŠ¡å·²åœæ­¢ï¼Œä¸å†æ‰“å°ä¿¡æ¯
                if self.is_stopped():
                    return total_stats

                print(f"   ğŸš« é¡µé¢ {current_page} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤é¡µ")
                # å¤§å¹…åº¦è·³è¿‡é—®é¢˜åŒºåŸŸ
                if end_time:
                    try:
                        dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                        dt = dt - timedelta(hours=1)
                        end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        print(f"   â° å¤§å¹…åº¦è·³è¿‡æ—¶é—´æ®µ: {end_time} (å‡å»1å°æ—¶)")
                    except Exception as e:
                        print(f"   âš ï¸ å¤§å¹…åº¦æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                completed_pages += 1  # è·³è¿‡è¿™ä¸€é¡µ
        
        print(f"\nğŸ å¢é‡çˆ¬å–å®Œæˆ:")
        print(f"   ğŸ“„ æˆåŠŸé¡µæ•°: {total_stats['pages']}")
        print(f"   âœ… æ–°å¢è¯é¢˜: {total_stats['new_topics']}")
        print(f"   ğŸ”„ æ›´æ–°è¯é¢˜: {total_stats['updated_topics']}")
        if total_stats['errors'] > 0:
            print(f"   âŒ æ€»é”™è¯¯æ•°: {total_stats['errors']}")
        
        # æ˜¾ç¤ºæ›´æ–°åçš„æ•°æ®åº“çŠ¶æ€
        updated_info = self.db.get_timestamp_range_info()
        print(f"\nğŸ“Š æ›´æ–°åæ•°æ®åº“çŠ¶æ€:")
        print(f"   è¯é¢˜æ€»æ•°: {updated_info['total_topics']} (+{updated_info['total_topics'] - total_existing})")
        print(f"   æ—¶é—´èŒƒå›´: {updated_info['oldest_timestamp']} ~ {updated_info['newest_timestamp']}")
        
        return total_stats
    
    def crawl_latest_until_complete(self, per_page: int = 20) -> Dict[str, int]:
        """è·å–æœ€æ–°è®°å½•ï¼šæ™ºèƒ½å¢é‡æ›´æ–°ï¼Œçˆ¬å–åˆ°ä¸æ•°æ®åº“å®Œå…¨è¡”æ¥ä¸ºæ­¢"""
        print(f"\nğŸ”„ è·å–æœ€æ–°è®°å½•æ¨¡å¼ (æ¯é¡µ{per_page}æ¡)")
        print(f"ğŸ’¡ æ™ºèƒ½é€»è¾‘ï¼šæ£€æŸ¥æœ€æ–°è¯é¢˜ï¼Œå¦‚æœ‰æ–°å†…å®¹åˆ™å‘åçˆ¬å–ç›´åˆ°ä¸æ•°æ®åº“å®Œå…¨è¡”æ¥")
        
        # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
        timestamp_info = self.db.get_timestamp_range_info()
        if not timestamp_info['has_data']:
            self.log("ğŸ“Š æ•°æ®åº“ä¸ºç©ºï¼Œå°†ä»æœ€æ–°å¼€å§‹çˆ¬å–")
            # ç©ºåº“åœºæ™¯ï¼šç›´æ¥ä»æœ€æ–°å¼€å§‹å¢é‡ï¼Œç›´åˆ°ä¸å·²å­˜æ•°æ®è¡”æ¥æˆ–æ— æ›´å¤šæ•°æ®
        
        print(f"ğŸ“Š æ•°æ®åº“çŠ¶æ€:")
        print(f"   ç°æœ‰è¯é¢˜æ•°: {timestamp_info['total_topics']}")
        print(f"   æœ€æ–°æ—¶é—´æˆ³: {timestamp_info['newest_timestamp']}")
        
        total_stats = {'new_topics': 0, 'updated_topics': 0, 'errors': 0, 'pages': 0}
        end_time = None  # ä»æœ€æ–°å¼€å§‹
        current_page = 0
        max_retries_per_page = 10
        
        # âœ… æ·»åŠ ï¼šè®°å½•æ‰€æœ‰æ–°å¢è¯é¢˜
        all_new_topics = []
        
        while True:
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.is_stopped():
                break

            current_page += 1
            self.log(f"\nğŸ“„ æ£€æŸ¥é¡µé¢ {current_page}")
            retry_count = 0
            page_success = False

            # é‡è¯•å½“å‰é¡µç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            while retry_count < max_retries_per_page:
                # åœ¨é‡è¯•å¾ªç¯ä¸­ä¹Ÿæ£€æŸ¥åœæ­¢æ ‡å¿—
                if self.is_stopped():
                    return total_stats
                if retry_count > 0:
                    print(f"   ğŸ”„ ç¬¬{retry_count}æ¬¡é‡è¯•")
                
                # è·å–æ•°æ®
                if current_page == 1:
                    # ç¬¬ä¸€é¡µï¼šè·å–æœ€æ–°è¯é¢˜
                    data = self.fetch_topics_safe(scope="all", count=per_page, is_historical=False)
                else:
                    # åç»­é¡µé¢ï¼šä½¿ç”¨ end_time å‘å†å²çˆ¬å–
                    data = self.fetch_topics_safe(scope="all", count=per_page, 
                                                end_time=end_time, is_historical=True)
                
                if data:
                    # æˆåŠŸè·å–æ•°æ®
                    topics = data.get('resp_data', {}).get('topics', [])
                    
                    if not topics:
                        print(f"   ğŸ“­ æ— æ›´å¤šæ•°æ®ï¼Œè·å–å®Œæˆ")
                        break
                    
                    # æ£€æŸ¥è¿™ä¸€é¡µçš„è¯é¢˜æ˜¯å¦åœ¨æ•°æ®åº“ä¸­å…¨éƒ¨å­˜åœ¨
                    existing_count = 0
                    new_topics_list = []
                    
                    for topic in topics:
                        topic_id = topic.get('topic_id')
                        self.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
                        if self.db.cursor.fetchone():
                            existing_count += 1
                        else:
                            new_topics_list.append(topic)
                    
                    print(f"   ğŸ“Š é¡µé¢åˆ†æ: {len(topics)}ä¸ªè¯é¢˜ï¼Œ{existing_count}ä¸ªå·²å­˜åœ¨ï¼Œ{len(new_topics_list)}ä¸ªæ–°è¯é¢˜")
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦åœæ­¢
                    if existing_count == len(topics):
                        # æ•´é¡µè¯é¢˜å…¨éƒ¨å­˜åœ¨äºæ•°æ®åº“ä¸­
                        print(f"   âœ… æ•´é¡µè¯é¢˜å…¨éƒ¨å­˜åœ¨äºæ•°æ®åº“ï¼Œå¢é‡æ›´æ–°å®Œæˆ")
                        print(f"\nğŸ‰ è·å–æœ€æ–°è®°å½•å®Œæˆæ€»ç»“:")
                        print(f"   ğŸ“„ æ£€æŸ¥é¡µæ•°: {total_stats['pages']}")
                        print(f"   âœ… æ–°å¢è¯é¢˜: {total_stats['new_topics']}")
                        print(f"   ğŸ”„ æ›´æ–°è¯é¢˜: {total_stats['updated_topics']}")
                        if total_stats['errors'] > 0:
                            print(f"   âŒ æ€»é”™è¯¯æ•°: {total_stats['errors']}")
                        
                        # æ˜¾ç¤ºæ›´æ–°åçš„æ•°æ®åº“çŠ¶æ€
                        final_db_stats = self.db.get_timestamp_range_info()
                        if final_db_stats['has_data']:
                            print(f"\nğŸ“Š æ•°æ®åº“æœ€ç»ˆçŠ¶æ€:")
                            print(f"   è¯é¢˜æ€»æ•°: {final_db_stats['total_topics']} (+{final_db_stats['total_topics'] - timestamp_info['total_topics']})")
                            print(f"   æ—¶é—´èŒƒå›´: {final_db_stats['oldest_timestamp']} ~ {final_db_stats['newest_timestamp']}")
                        
                         # âœ… æ·»åŠ ï¼šä¼ä¸šå¾®ä¿¡æ¨é€ï¼ˆåœ¨è¿”å›å‰ï¼‰
                        if self.wecom_webhook and all_new_topics:
                            self.log(f"ğŸ“± å‡†å¤‡æ¨é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥ï¼Œå…±{len(all_new_topics)}ä¸ªæ–°è¯é¢˜...")
                            success = self.wecom_webhook.send_new_topics_notification(all_new_topics, total_stats)
                            if success:
                                self.log("âœ… ä¼ä¸šå¾®ä¿¡æ¨é€æˆåŠŸ")
                            else:
                                self.log("âš ï¸ ä¼ä¸šå¾®ä¿¡æ¨é€å¤±è´¥")
                                
                        return total_stats
                    
                    elif existing_count == 0:
                        # æ•´é¡µè¯é¢˜éƒ½æ˜¯æ–°çš„ï¼Œå…¨éƒ¨å­˜å‚¨
                        page_stats = self.store_batch_data(data)
                        print(f"   ğŸ’¾ æ•´é¡µå­˜å‚¨: æ–°å¢{page_stats['new_topics']}, æ›´æ–°{page_stats['updated_topics']}")

                         # âœ… æ·»åŠ ï¼šè®°å½•æ–°å¢è¯é¢˜ï¼ˆåœ¨å­˜å‚¨å‰è®°å½•ï¼‰
                        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é‡æ–°æŸ¥è¯¢ï¼Œå› ä¸ºstore_batch_dataå·²ç»å­˜å‚¨äº†
                        # æ›´å¥½çš„æ–¹å¼æ˜¯åœ¨store_batch_dataä¹‹å‰è®°å½•
                        # ä½†ä¸ºäº†ä¿æŒä»£ç ç»“æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨new_topics_list
                        all_new_topics.extend(new_topics_list)
                    
                    else:
                        # éƒ¨åˆ†è¯é¢˜æ˜¯æ–°çš„ï¼Œåªå­˜å‚¨æ–°è¯é¢˜
                        print(f"   ğŸ’¾ éƒ¨åˆ†å­˜å‚¨: åªå¤„ç†{len(new_topics_list)}ä¸ªæ–°è¯é¢˜")
                        new_topics_count = 0
                        updated_topics_count = 0
                        
                        for new_topic in new_topics_list:
                            try:
                                topic_id = new_topic.get('topic_id')
                                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŒé‡æ£€æŸ¥ï¼‰
                                self.db.cursor.execute('SELECT topic_id FROM topics WHERE topic_id = ?', (topic_id,))
                                exists = self.db.cursor.fetchone()
                                
                                # å¯¼å…¥æ•°æ®
                                self.db.import_topic_data(new_topic)
                                
                                if exists:
                                    updated_topics_count += 1
                                else:
                                    new_topics_count += 1
                                    
                            except Exception as e:
                                print(f"   âš ï¸ è¯é¢˜å¯¼å…¥å¤±è´¥: {e}")
                        
                        # æäº¤äº‹åŠ¡
                        self.db.conn.commit()
                        print(f"   ğŸ’¾ æ–°è¯é¢˜å­˜å‚¨: æ–°å¢{new_topics_count}, æ›´æ–°{updated_topics_count}")
                        
                        # âœ… æ·»åŠ ï¼šè®°å½•æ–°å¢è¯é¢˜
                        all_new_topics.extend(new_topics_list)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        total_stats['new_topics'] += new_topics_count
                        total_stats['updated_topics'] += updated_topics_count
                    
                    # ç´¯è®¡ç»Ÿè®¡ï¼ˆå¦‚æœæ˜¯æ•´é¡µå­˜å‚¨ï¼‰
                    if existing_count == 0:
                        total_stats['new_topics'] += page_stats['new_topics']
                        total_stats['updated_topics'] += page_stats['updated_topics']
                        total_stats['errors'] += page_stats['errors']
                    
                    total_stats['pages'] += 1
                    
                    # æ˜¾ç¤ºå½“å‰è¿›åº¦
                    print(f"   ğŸ“ˆ ç´¯è®¡: æ–°å¢{total_stats['new_topics']}, æ›´æ–°{total_stats['updated_topics']}, é¡µæ•°{total_stats['pages']}")
                    
                    # æ˜¾ç¤ºæ—¶é—´æˆ³ä¿¡æ¯
                    if topics:
                        first_time = topics[0].get('create_time', 'N/A')
                        last_time = topics[-1].get('create_time', 'N/A')
                        print(f"   â° æ—¶é—´èŒƒå›´: {first_time} ~ {last_time}")
                    
                    # å‡†å¤‡ä¸‹ä¸€é¡µçš„æ—¶é—´æˆ³
                    if topics:
                        original_time = topics[-1].get('create_time')
                        try:
                            from datetime import datetime, timedelta
                            dt = datetime.fromisoformat(original_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        except Exception as e:
                            end_time = original_time
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
                    
                    # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    page_success = True
                    break
                else:
                    # å¤±è´¥ï¼Œå¢åŠ é‡è¯•è®¡æ•°å’Œé”™è¯¯è®¡æ•°
                    retry_count += 1
                    total_stats['errors'] += 1

                    # å¦‚æœä»»åŠ¡å·²åœæ­¢ï¼Œä¸å†æ‰“å°é”™è¯¯ä¿¡æ¯å’Œè°ƒæ•´æ—¶é—´æˆ³
                    if self.is_stopped():
                        return total_stats

                    print(f"   âŒ é¡µé¢ {current_page} è·å–å¤±è´¥ (é‡è¯•{retry_count}/{max_retries_per_page})")

                    # è°ƒæ•´æ—¶é—´æˆ³ç”¨äºé‡è¯•
                    if end_time:
                        try:
                            from datetime import datetime, timedelta
                            dt = datetime.fromisoformat(end_time.replace('+0800', '+08:00'))
                            dt = dt - timedelta(milliseconds=self.timestamp_offset_ms)
                            end_time = dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + '+0800'
                        except Exception as e:
                            print(f"   âš ï¸ æ—¶é—´æˆ³è°ƒæ•´å¤±è´¥: {e}")
            
            # å¦‚æœé‡è¯•æ¬¡æ•°ç”¨å®Œä»ç„¶å¤±è´¥
            if not page_success:
                # å¦‚æœä»»åŠ¡å·²åœæ­¢ï¼Œä¸å†æ‰“å°ä¿¡æ¯
                if self.is_stopped():
                    break
                print(f"   ğŸš« é¡µé¢ {current_page} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåœæ­¢è·å–")
                break
            else:
                # é¡µé¢æˆåŠŸå¤„ç†åè¿›è¡Œé•¿ä¼‘çœ æ£€æŸ¥ï¼ˆåŸºäºé¡µé¢æ•°è€Œéè¯·æ±‚æ•°ï¼‰
                self.check_page_long_delay()
                
        return total_stats
    
    def show_menu(self):
        """æ˜¾ç¤ºäº¤äº’èœå•"""
        print(f"\n{'='*60}")
        print("ğŸ•·ï¸ çŸ¥è¯†æ˜Ÿçƒäº¤äº’å¼æ•°æ®é‡‡é›†å™¨")
        print("="*60)
        print("ğŸ“ è¯é¢˜é‡‡é›†åŠŸèƒ½:")
        print("1. è·å–æ‰€æœ‰å†å²æ•°æ® (æ— é™çˆ¬å–) - é€‚åˆï¼šå…¨é‡å½’æ¡£ï¼Œä»æœ€è€æ•°æ®æ— é™æŒ–æ˜")
        print("2. å¢é‡çˆ¬å–å†å² (åŸºäºæ•°æ®åº“æœ€è€æ—¶é—´æˆ³) - é€‚åˆï¼šç²¾ç¡®è¡¥å……å†å²ï¼Œæœ‰ç›®æ ‡çš„å›å¡«")
        print("3. è·å–æœ€æ–°è®°å½• (æ™ºèƒ½å¢é‡æ›´æ–°) - é€‚åˆï¼šæ—¥å¸¸ç»´æŠ¤ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶åªçˆ¬æ–°å†…å®¹")
        print("")
        print("ğŸ“¥ æ–‡ä»¶ä¸‹è½½åŠŸèƒ½:")
        print("4. å¢é‡æ”¶é›†æ–‡ä»¶åˆ—è¡¨ - é€‚åˆï¼šä»æ•°æ®åº“æœ€è€æ—¶é—´æˆ³ç»§ç»­æ”¶é›†æ›´æ—©æ–‡ä»¶")
        print("5. æŸ¥çœ‹æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡ - é€‚åˆï¼šæŸ¥çœ‹æ”¶é›†çš„æ–‡ä»¶ä¿¡æ¯å’Œä¸‹è½½çŠ¶æ€")
        print("6. æŒ‰ä¸‹è½½æ¬¡æ•°ä¸‹è½½æ–‡ä»¶ - é€‚åˆï¼šè‡ªåŠ¨æ”¶é›†çƒ­é—¨æ–‡ä»¶å¹¶æŒ‰ä¸‹è½½æ¬¡æ•°æ’åºä¸‹è½½")
        print("7. æŒ‰æ—¶é—´é¡ºåºä¸‹è½½æ–‡ä»¶ - é€‚åˆï¼šè‡ªåŠ¨æ”¶é›†æ–‡ä»¶åˆ—è¡¨å¹¶æŒ‰æ—¶é—´é¡ºåºä¸‹è½½")
        print("8. æ–‡ä»¶ä¸‹è½½è®¾ç½® - é€‚åˆï¼šè°ƒæ•´ä¸‹è½½é—´éš”å’Œä¼‘çœ å‚æ•°")
        print("")
        print("âš™ï¸ ç³»ç»ŸåŠŸèƒ½:")
        print("9. æŸ¥çœ‹è¯é¢˜æ•°æ®åº“ç»Ÿè®¡ - é€‚åˆï¼šç›‘æ§è¯é¢˜æ•°æ®çŠ¶æ€ï¼Œäº†è§£å½“å‰æ•°æ®èŒƒå›´")
        print("10. è°ƒæ•´åæ£€æµ‹è®¾ç½® - é€‚åˆï¼šä¼˜åŒ–çˆ¬å–é€Ÿåº¦ï¼Œåº”å¯¹ä¸åŒç½‘ç»œç¯å¢ƒ")
        print(f"11. æ—¶é—´æˆ³è®¾ç½® (å½“å‰: å‡å»{self.timestamp_offset_ms}æ¯«ç§’) - é€‚åˆï¼šè§£å†³æ—¶é—´ç‚¹å†²çªï¼Œç²¾ç¡®æ§åˆ¶åˆ†é¡µ")
        print(f"12. è°ƒè¯•æ¨¡å¼ (å½“å‰: {'å¼€å¯' if self.debug_mode else 'å…³é—­'}) - é€‚åˆï¼šæ’æŸ¥é—®é¢˜ï¼ŒæŸ¥çœ‹è¯¦ç»†è¯·æ±‚ä¿¡æ¯")
        print("13. é€€å‡ºç¨‹åº")
        print("="*60)
    
    def adjust_stealth_settings(self):
        """è°ƒæ•´åæ£€æµ‹è®¾ç½®"""
        print(f"\nğŸ”§ å½“å‰åæ£€æµ‹è®¾ç½®:")
        print(f"   æœ€å°å»¶è¿Ÿ: {self.min_delay}ç§’")
        print(f"   æœ€å¤§å»¶è¿Ÿ: {self.max_delay}ç§’")
        print(f"   é•¿å»¶è¿Ÿé—´éš”: æ¯{self.long_delay_interval}ä¸ªé¡µé¢")
        print(f"   é•¿ä¼‘çœ æ—¶é—´: 3-5åˆ†é’Ÿ (180-300ç§’)")
        print(f"ğŸ’¡ è¯´æ˜: é•¿ä¼‘çœ åŸºäºæˆåŠŸå¤„ç†çš„é¡µé¢æ•°ï¼Œè€Œéè¯·æ±‚æ•°ï¼Œæ›´åŠ åˆç†ç¨³å®š")
        
        try:
            new_min = float(input(f"æ–°çš„æœ€å°å»¶è¿Ÿ (å½“å‰{self.min_delay}): ") or self.min_delay)
            new_max = float(input(f"æ–°çš„æœ€å¤§å»¶è¿Ÿ (å½“å‰{self.max_delay}): ") or self.max_delay)
            new_interval = int(input(f"é•¿å»¶è¿Ÿé—´éš” (å½“å‰æ¯{self.long_delay_interval}é¡µ): ") or self.long_delay_interval)
            
            self.min_delay = max(new_min, 1.0)  # æœ€å°1ç§’
            self.max_delay = max(new_max, self.min_delay + 1.0)
            self.long_delay_interval = max(new_interval, 5)
            
            print(f"âœ… è®¾ç½®å·²æ›´æ–°")
            print(f"ğŸ’¡ é•¿ä¼‘çœ æ—¶é—´å›ºå®šä¸º3-5åˆ†é’Ÿï¼Œæœ‰åŠ©äºæ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»è¡Œä¸º")
            print(f"ğŸ¯ é•¿ä¼‘çœ è§¦å‘ï¼šæ¯æˆåŠŸå¤„ç†{self.long_delay_interval}ä¸ªé¡µé¢è¿›è¡Œä¸€æ¬¡é•¿ä¼‘çœ ")
            
        except ValueError:
            print("âŒ è¾“å…¥æ— æ•ˆï¼Œä¿æŒåŸè®¾ç½®")
    
    def adjust_timestamp_settings(self):
        """è°ƒæ•´æ—¶é—´æˆ³è®¾ç½®"""
        print(f"\nâ° å½“å‰æ—¶é—´æˆ³è®¾ç½®:")
        print(f"   å‡å»æ¯«ç§’æ•°: {self.timestamp_offset_ms}æ¯«ç§’")
        print(f"\nğŸ’¡ è¯´æ˜:")
        print(f"   - å‡å»1æ¯«ç§’: æ ‡å‡†è®¾ç½®ï¼Œä¸å®˜ç½‘ä¸€è‡´")
        print(f"   - å‡å»2-3æ¯«ç§’: å¯èƒ½é¿å¼€æŸäº›é—®é¢˜æ—¶é—´ç‚¹")
        print(f"   - å‡å»5-10æ¯«ç§’: æ›´å¤§çš„å®¹é”™èŒƒå›´")
        
        try:
            new_offset = int(input(f"æ–°çš„æ¯«ç§’åç§»é‡ (å½“å‰{self.timestamp_offset_ms}): ") or self.timestamp_offset_ms)
            
            if new_offset < 0:
                print("âŒ æ¯«ç§’åç§»é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
                return
            
            self.timestamp_offset_ms = new_offset
            print(f"âœ… æ—¶é—´æˆ³è®¾ç½®å·²æ›´æ–°: å‡å»{self.timestamp_offset_ms}æ¯«ç§’")
            
        except ValueError:
            print("âŒ è¾“å…¥æ— æ•ˆï¼Œä¿æŒåŸè®¾ç½®")
    
    def run_interactive(self):
        """è¿è¡Œäº¤äº’å¼ç•Œé¢"""
        try:
            while True:
                self.show_menu()
                choice = input("\nè¯·é€‰æ‹© (1-13): ").strip()
                
                if choice == "1":
                    per_page = int(input("æ¯é¡µæ•°é‡ (é»˜è®¤20): ") or "20")
                    self.crawl_all_historical(per_page)
                    
                elif choice == "2":
                    pages = int(input("çˆ¬å–é¡µæ•° (é»˜è®¤10): ") or "10")
                    per_page = int(input("æ¯é¡µæ•°é‡ (é»˜è®¤20): ") or "20")
                    self.crawl_incremental(pages, per_page)
                    
                elif choice == "3":
                    per_page = int(input("æ¯é¡µæ•°é‡ (é»˜è®¤20): ") or "20")
                    self.crawl_latest_until_complete(per_page)
                    
                elif choice == "4":
                    # å¢é‡æ”¶é›†æ–‡ä»¶åˆ—è¡¨
                    downloader = self.get_file_downloader()
                    downloader.collect_incremental_files()
                    
                elif choice == "5":
                    # æŸ¥çœ‹æ–‡ä»¶æ•°æ®åº“ç»Ÿè®¡
                    downloader = self.get_file_downloader()
                    downloader.show_database_stats()
                    
                elif choice == "6":
                    # æŒ‰ä¸‹è½½æ¬¡æ•°ä¸‹è½½æ–‡ä»¶ (é›†æˆæ”¶é›†å’Œä¸‹è½½)
                    downloader = self.get_file_downloader()
                    
                    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰æ–‡ä»¶æ•°æ®
                    stats = downloader.file_db.get_database_stats()
                    existing_files = stats.get('files', 0)
                    
                    if existing_files > 0:
                        print(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {existing_files} ä¸ªæ–‡ä»¶è®°å½•")
                        collect_confirm = input("æ˜¯å¦é‡æ–°æ”¶é›†æ–‡ä»¶åˆ—è¡¨? (y/n, é»˜è®¤nç›´æ¥ä¸‹è½½): ").strip().lower()
                        if collect_confirm != 'y':
                            print("âš¡ ç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®è¿›è¡Œä¸‹è½½...")
                        else:
                            print("ğŸ”„ æŒ‰ä¸‹è½½æ¬¡æ•°é‡æ–°æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")
                            downloader.collect_all_files_to_database()
                    else:
                        print("ğŸ”„ æŒ‰ä¸‹è½½æ¬¡æ•°æ”¶é›†çƒ­é—¨æ–‡ä»¶åˆ—è¡¨...")
                        downloader.collect_all_files_to_database()
                    
                    # è‡ªåŠ¨å¼€å§‹ä¸‹è½½
                    print("\nğŸš€ è‡ªåŠ¨å¼€å§‹ä¸‹è½½æ–‡ä»¶...")
                    user_input = input("æœ€å¤§ä¸‹è½½æ–‡ä»¶æ•° (é»˜è®¤æ— é™ï¼Œè¾“å…¥æ•°å­—é™åˆ¶): ").strip()
                    if user_input and user_input.isdigit():
                        max_files = int(user_input)
                    else:
                        max_files = None
                    
                    days_input = input("åªä¸‹è½½æœ€è¿‘Nå¤©çš„æ–‡ä»¶ (é»˜è®¤æ— é™ï¼Œè¾“å…¥æ•°å­—é™åˆ¶å¤©æ•°): ").strip()
                    if days_input and days_input.isdigit():
                        recent_days = int(days_input)
                    else:
                        recent_days = None
                    downloader.download_files_from_database(max_files=max_files, status_filter='pending', recent_days=recent_days, order_by="download_count DESC")
                    
                elif choice == "7":
                    # æŒ‰æ—¶é—´é¡ºåºä¸‹è½½æ–‡ä»¶ (é›†æˆæ”¶é›†å’Œä¸‹è½½)
                    downloader = self.get_file_downloader()
                    
                    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æœ‰æ–‡ä»¶æ•°æ®
                    stats = downloader.file_db.get_database_stats()
                    existing_files = stats.get('files', 0)
                    
                    if existing_files > 0:
                        print(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {existing_files} ä¸ªæ–‡ä»¶è®°å½•")
                        collect_confirm = input("æ˜¯å¦é‡æ–°æ”¶é›†æ–‡ä»¶åˆ—è¡¨? (y/n, é»˜è®¤nç›´æ¥ä¸‹è½½): ").strip().lower()
                        if collect_confirm != 'y':
                            print("âš¡ ç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®è¿›è¡Œä¸‹è½½...")
                        else:
                            print("ğŸ”„ æŒ‰æ—¶é—´æ’åºé‡æ–°æ”¶é›†æ–‡ä»¶åˆ—è¡¨...")
                            downloader.collect_files_by_time()
                    else:
                        print("ğŸ”„ æŒ‰æ—¶é—´æ’åºæ”¶é›†æ–‡ä»¶åˆ—è¡¨...")
                        downloader.collect_files_by_time()
                    
                    # è‡ªåŠ¨å¼€å§‹ä¸‹è½½
                    print("\nğŸš€ è‡ªåŠ¨å¼€å§‹ä¸‹è½½æ–‡ä»¶...")
                    user_input = input("æœ€å¤§ä¸‹è½½æ–‡ä»¶æ•° (é»˜è®¤æ— é™ï¼Œè¾“å…¥æ•°å­—é™åˆ¶): ").strip()
                    if user_input and user_input.isdigit():
                        max_files = int(user_input)
                    else:
                        max_files = None
                    
                    days_input = input("åªä¸‹è½½æœ€è¿‘Nå¤©çš„æ–‡ä»¶ (é»˜è®¤æ— é™ï¼Œè¾“å…¥æ•°å­—é™åˆ¶å¤©æ•°): ").strip()
                    if days_input and days_input.isdigit():
                        recent_days = int(days_input)
                    else:
                        recent_days = None
                    
                    downloader.download_files_from_database(max_files=max_files, status_filter='pending', recent_days=recent_days, order_by="create_time DESC")
                    
                elif choice == "8":
                    # æ–‡ä»¶ä¸‹è½½è®¾ç½®
                    downloader = self.get_file_downloader()
                    downloader.adjust_settings()
                    
                elif choice == "9":
                    # æŸ¥çœ‹è¯é¢˜æ•°æ®åº“ç»Ÿè®¡
                    self.show_database_status()
                    stats = self.db.get_database_stats()
                    print("\nğŸ“Š è¯¦ç»†ç»Ÿè®¡:")
                    for table, count in stats.items():
                        print(f"   {table}: {count}")
                    
                elif choice == "10":
                    self.adjust_stealth_settings()
                
                elif choice == "11":
                    self.adjust_timestamp_settings()
                    
                elif choice == "12":
                    self.debug_mode = not self.debug_mode
                    status = "å¼€å¯" if self.debug_mode else "å…³é—­"
                    print(f"ğŸ” è°ƒè¯•æ¨¡å¼å·²{status}")
                    if self.debug_mode:
                        print("âš ï¸ è°ƒè¯•æ¨¡å¼ä¼šè¾“å‡ºè¯¦ç»†çš„è¯·æ±‚ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®Œæ•´çš„å¤±è´¥å“åº”")
                    
                elif choice == "13":
                    print("ğŸ‘‹ é€€å‡ºç¨‹åº")
                    break
                    
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
                
                input("\næŒ‰å›è½¦é”®ç»§ç»­...")
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        finally:
            self.close()
    
    def close(self):
        """å…³é—­èµ„æº"""
        self.db.close()
        print("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")


def load_config():
    """åŠ è½½TOMLé…ç½®æ–‡ä»¶"""
    if tomllib is None:
        return None

    # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
    config_paths = [
        "config.toml",           # å½“å‰ç›®å½•
        "../config.toml",        # ä¸Šçº§ç›®å½•ï¼ˆä»backendç›®å½•è¿è¡Œæ—¶ï¼‰
        "../../config.toml"      # ä¸Šä¸Šçº§ç›®å½•
    ]

    config_file = None
    for path in config_paths:
        if os.path.exists(path):
            config_file = path
            break

    if config_file is None:
        print("âš ï¸ æœªæ‰¾åˆ°config.tomlé…ç½®æ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»ºå¹¶é…ç½®")
        print("ğŸ’¡ å¯ä»¥å¤åˆ¶config.toml.exampleä¸ºconfig.tomlå¹¶ä¿®æ”¹")
        return None
    
    try:
        with open(config_file, 'rb') as f:
            config = tomllib.load(f)
        
        print("âœ… å·²ä»config.tomlåŠ è½½é…ç½®")
        return config
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='çŸ¥è¯†æ˜Ÿçƒäº¤äº’å¼æ•°æ®é‡‡é›†å™¨')
    parser.add_argument('-d', '--auto-download', action='store_true',
                        help='è‡ªåŠ¨ä¸‹è½½æ¨¡å¼ï¼šæŒ‰æ—¶é—´æ’åºä¸‹è½½æœ€è¿‘3å¤©çš„æ–‡ä»¶ï¼Œæ— éœ€äº¤äº’')
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®ä¿¡æ¯
    config = load_config()
    if not config:
        return
    
    # ä»TOMLé…ç½®ä¸­è·å–å€¼
    auth_config = config.get('auth', {})
    db_config = config.get('database', {})
    # wx_webhook æ–°å¢
    wecom_config = config.get('wecom_webhook', {})
    
    COOKIE = auth_config.get('cookie', 'your_cookie_here')
    GROUP_ID = auth_config.get('group_id', 'your_group_id_here')
    # æ•°æ®åº“è·¯å¾„æ”¹ä¸ºå¯é€‰ï¼›å¦‚æœªé…ç½®åˆ™ç”±è·¯å¾„ç®¡ç†å™¨è‡ªåŠ¨ç®¡ç†
    DB_PATH = db_config.get('path') if isinstance(db_config, dict) else None
    
     # ä¼ä¸šå¾®ä¿¡webhooké…ç½®-æ–°å¢
    wecom_webhook_url = wecom_config.get('webhook_url') if isinstance(wecom_config, dict) else None
    wecom_enabled = wecom_config.get('enabled', True) if isinstance(wecom_config, dict) else True
    
    # æ£€æŸ¥é…ç½®æ˜¯å¦å·²ä¿®æ”¹
    if COOKIE == "your_cookie_here" or not COOKIE:
        print("âš ï¸ è¯·å…ˆåœ¨config.tomlä¸­é…ç½®æ‚¨çš„ cookie")
        return
    if GROUP_ID == "your_group_id_here" or not GROUP_ID:
        print("âš ï¸ äº¤äº’å¼å‘½ä»¤è¡Œæ¨¡å¼ä»éœ€æ‰‹åŠ¨æŒ‡å®šå•ä¸ªç¾¤ç»„IDï¼Œè¯·åœ¨ config.toml ä¸­æ·»åŠ  [auth].group_id")
        return
    
    # åˆ›å»ºäº¤äº’å¼çˆ¬è™«
    crawler = ZSXQInteractiveCrawler(COOKIE, GROUP_ID, DB_PATH,wecom_webhook_url=wecom_webhook_url,
        wecom_enabled=wecom_enabled)
    
    # å¦‚æœæ˜¯è‡ªåŠ¨ä¸‹è½½æ¨¡å¼
    if args.auto_download:
        print("ğŸ¤– è‡ªåŠ¨ä¸‹è½½æ¨¡å¼ï¼šæŒ‰æ—¶é—´æ’åºä¸‹è½½æœ€è¿‘3å¤©çš„æ–‡ä»¶")
        print("=" * 60)
        
        # è·å–æ–‡ä»¶ä¸‹è½½å™¨
        downloader = crawler.get_file_downloader()
        
        print("ğŸ”„ æŒ‰æ—¶é—´æ’åºæ”¶é›†æ–‡ä»¶åˆ—è¡¨...")
        downloader.collect_files_by_time()
        
        # è‡ªåŠ¨ä¸‹è½½æœ€è¿‘3å¤©çš„æ–‡ä»¶
        print("\nğŸš€ å¼€å§‹ä¸‹è½½æœ€è¿‘3å¤©çš„æ–‡ä»¶...")
        downloader.download_files_from_database(
            max_files=None,
            status_filter='pending',
            recent_days=3,
            order_by="create_time DESC"
        )
        
        print("\nâœ… è‡ªåŠ¨ä¸‹è½½ä»»åŠ¡å®Œæˆï¼")
    else:
        # è¿è¡Œäº¤äº’ç•Œé¢
        crawler.run_interactive()


if __name__ == "__main__":
    main()